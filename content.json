[{"title":"使用二进制包在CentOS7上部署Kubernetes1.9集群","date":"2018-05-24T07:42:37.000Z","path":"2018/05/24/使用二进制包在CentOS7上部署Kubernetes1-9集群.html","text":"本文续《Kubernetes是什么？》之后的搭建k8s集群篇，主要以实战演示为重心。如果你还不是太熟悉k8s的各个组件或应用，不妨先把它搭建出来， 再来慢慢学习他们的原理及关联关系！ 一、环境规划1.1、环境准备你需要准备一台或多台服务器（虚拟机也行），这里我准备了三台物理机，分别如下： OS：CentOS Linux release 7.2.1511 (Core) 软件版本： Kubernetes 1.9 Etcd 3.0 Docker Version: 18.03.0-ce 主机名 机器配置 IP 安装服务角色 k8s-master-90 32核/64G内存/200G磁盘 10.0.10.90 kube-apiserver、kube-controller-manager、kube-scheduler、etcd k8s-node-91 32核/64G内存/200G磁盘 10.0.10.91 kubelet、kube-proxy、docker、flannel、etcd k8s-node-92 32核/64G内存/200G磁盘 10.0.10.92 kubelet、kube-proxy、docker、flannel、etcd 1.2、禁用SElinux12345$ vim /etc/selinux/configSELINUX=disabled# 临时禁用selinuxsetenforce 0 1.3、关闭SWAP分区k8s集群不允许使用swap分区，否则会出问题，详细信息自行了解。12345[root@k8s-node-91 ~]# swapon -s文件名 类型 大小 已用 权限/dev/sda4 partition 8286204 0 -1[root@k8s-node-91 ~]# swapoff -a[root@k8s-node-91 ~]# swapon -s # 再次查看，没有分区信息即可。 1.4、配置host解析1234vim /etc/hosts10.0.10.90 k8s-master-9010.0.10.91 k8s-node-9110.0.10.92 k8s-node-92 1.5、配置主机密钥通讯信任做这个密钥认证是为了后面分发k8s集群私钥时方便。12345678910111213141516171819202122232425262728[root@k8s-master-90 ~]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:74:0d:7a:84:0d:7e:90:83:e0:5a:cc:5b:eb:18:a9:6e root@k8s-master-90The keys randomart image is:+--[ RSA 2048]----+| .. .o=o || + ..++.o || = . +.+ . || o + o + || . + . S || . + || . . . || .E || .. |+-----------------+[root@k8s-master-90 ~]# ls -lh /root/.ssh/总用量 8.0K-rw------- 1 root root 1.7K 4月 24 18:17 id_rsa-rw-r--r-- 1 root root 400 4月 24 18:17 id_rsa.pub[root@k8s-master-90 ~]# ssh-copy-id 10.0.10.91[root@k8s-master-90 ~]# ssh-copy-id 10.0.10.92[root@k8s-master-90 ~]# ssh 10.0.10.92 # 测试登录下 上面做的是k8s-master单向信任认证，如果需要做多向信任认证的话，把私钥cp到其他两台节点上即可。 1.6、创建K8S集群工作目录1[root@k8s-master-90 ~]# mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl&#125; # 创建k8s工作目录，今后有关于k8s所有东西都在这里 二、安装Docker其他版本安装请移步官网：https://docs.docker.com/install123456789101112131415161718192021222324252627yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engineyum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2# 官网的repo源在中国用不了，咱们还是乖乖使用马爸爸提供的源好了yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum makecache fastyum install docker-ce -ycat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"]&#125;EOFsystemctl enable docker &amp;&amp; systemctl start docker 三、自签TLS证书参考： Kubernetes安装之证书验证 创建CA证书和秘钥 组件 使用的证书 etcd ca.pem，server.pem，server-key.pem flannel ca.pem，server.pem，server-key.pem kube-apiserver ca.pem，server.pem，server-key.pem kubelet ca.pem，ca-key.pem kube-proxy ca.pem，kube-proxy.pem，kube-proxy-key.pem kubectl ca.pem，admin.pem，admin-key.pem 3.1、安装证书生成工具cfssl在Master节点生成即可12345678910[root@k8s-master-90 ~]# mkdir ssl[root@k8s-master-90 ~]# cd ssl/wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 3.2、生成证书将下面的内容写入一个脚本，批量生成vim certificate.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130[root@k8s-master-90 ssl]# pwd/root/ssl[root@k8s-master-90 ssl]# vim certificate.shcat &gt; ca-config.json &lt;&lt;EOF&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"expiry\": \"87600h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"Beijing\", \"ST\": \"Beijing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca -#-----------------------cat &gt; server-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.0.10.90\", # 如果你的服务器节点IP与我的不同，需要更改这里（把我的节点都换掉） \"10.0.10.91\", \"10.0.10.92\", \"10.10.10.1\", # 这个IP是k8s集群的IP地址，保留！ \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"BeiJing\", \"ST\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server#-----------------------cat &gt; admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"BeiJing\", \"ST\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin#-----------------------cat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"BeiJing\", \"ST\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy[root@k8s-master-90 ssl]# chmod +x certificate.sh # 给个执行权限 注意： 你如果使用上面内容创建证书的话，需要更改节点的IP字段，在上面我已有提示。 3.3、生成证书过程及证书文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@k8s-master-90 ssl]# sh certificate.sh2018/04/17 18:35:22 [INFO] generating a new CA key and certificate from CSR2018/04/17 18:35:22 [INFO] generate received request2018/04/17 18:35:22 [INFO] received CSR2018/04/17 18:35:22 [INFO] generating key: rsa-20482018/04/17 18:35:22 [INFO] encoded CSR2018/04/17 18:35:22 [INFO] signed certificate with serial number 3408360138081080563853025004802404084740900887302018/04/17 18:35:22 [INFO] generate received request2018/04/17 18:35:22 [INFO] received CSR2018/04/17 18:35:22 [INFO] generating key: rsa-20482018/04/17 18:35:22 [INFO] encoded CSR2018/04/17 18:35:22 [INFO] signed certificate with serial number 5681000870946528409275975781345572662675434932612018/04/17 18:35:22 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (\"Information Requirements\").2018/04/17 18:35:22 [INFO] generate received request2018/04/17 18:35:22 [INFO] received CSR2018/04/17 18:35:22 [INFO] generating key: rsa-20482018/04/17 18:35:23 [INFO] encoded CSR2018/04/17 18:35:23 [INFO] signed certificate with serial number 1543041540126216007575190989060451286202333902492018/04/17 18:35:23 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (\"Information Requirements\").2018/04/17 18:35:23 [INFO] generate received request2018/04/17 18:35:23 [INFO] received CSR2018/04/17 18:35:23 [INFO] generating key: rsa-20482018/04/17 18:35:23 [INFO] encoded CSR2018/04/17 18:35:23 [INFO] signed certificate with serial number 1251235192094462271455335934754243331339372301472018/04/17 18:35:23 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (\"Information Requirements\").[root@k8s-master-90 ssl]# ls -lh总用量 72K-rw-r--r-- 1 root root 1009 4月 17 18:35 admin.csr-rw-r--r-- 1 root root 229 4月 17 18:35 admin-csr.json-rw------- 1 root root 1.7K 4月 17 18:35 admin-key.pem-rw-r--r-- 1 root root 1.4K 4月 17 18:35 admin.pem-rw-r--r-- 1 root root 292 4月 17 18:35 ca-config.json-rw-r--r-- 1 root root 1001 4月 17 18:35 ca.csr-rw-r--r-- 1 root root 266 4月 17 18:35 ca-csr.json-rw------- 1 root root 1.7K 4月 17 18:35 ca-key.pem-rw-r--r-- 1 root root 1.4K 4月 17 18:35 ca.pem-rwxr-xr-x 1 root root 2.3K 4月 17 18:33 certificate.sh-rw-r--r-- 1 root root 1009 4月 17 18:35 kube-proxy.csr-rw-r--r-- 1 root root 230 4月 17 18:35 kube-proxy-csr.json-rw------- 1 root root 1.7K 4月 17 18:35 kube-proxy-key.pem-rw-r--r-- 1 root root 1.4K 4月 17 18:35 kube-proxy.pem-rw-r--r-- 1 root root 1.3K 4月 17 18:35 server.csr-rw-r--r-- 1 root root 559 4月 17 18:35 server-csr.json-rw------- 1 root root 1.7K 4月 17 18:35 server-key.pem-rw-r--r-- 1 root root 1.6K 4月 17 18:35 server.pem 四、部署高可用Etcd集群4.1、下载二进制包etcd官网下载地址：https://github.com/coreos/etcd/releases1234[root@k8s-master-90 ~]# wget -c https://github.com/coreos/etcd/releases/download/v3.2.17/etcd-v3.2.17-linux-amd64.tar.gz[root@k8s-master-90 ~]# tar zxf etcd-v3.2.17-linux-amd64.tar.gz[root@k8s-master-90 ~]# cd etcd-v3.2.17-linux-amd64[root@k8s-master-90 etcd-v3.2.17-linux-amd64]# cp etcd etcdctl /opt/kubernetes/bin/ #将所有k8s部署中的二进制执行文件都放到这里，便于管理。 4.2、部署Etcd的配置文件部署etcd，并配置systemd方式启动etcd进程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@k8s-master-90 ~]# cat etcd.sh #!/bin/bashETCD_NAME=$&#123;1:-\"etcd01\"&#125; #etcd当前节点的名称ETCD_IP=$&#123;2:-\"127.0.0.1\"&#125; # etcd当前节点的ipETCD_CLUSTER=$&#123;3:-\"etcd01=http://127.0.0.1:2379\"&#125; # 填写etcd集群的所有节点cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/etcd # 注意：这里需要与第一步面配置的路径一样，所以都是有关联的#[Member]ETCD_NAME=\"$&#123;ETCD_NAME&#125;\"ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"ETCD_LISTEN_PEER_URLS=\"https://$&#123;ETCD_IP&#125;:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://$&#123;ETCD_IP&#125;:2379\"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://$&#123;ETCD_IP&#125;:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"https://$&#123;ETCD_IP&#125;:2379\"ETCD_INITIAL_CLUSTER=\"$&#123;ETCD_CLUSTER&#125;\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=-/opt/kubernetes/cfg/etcdExecStart=/opt/kubernetes/bin/etcd \\\\--name=\\$&#123;ETCD_NAME&#125; \\\\--data-dir=\\$&#123;ETCD_DATA_DIR&#125; \\\\--listen-peer-urls=\\$&#123;ETCD_LISTEN_PEER_URLS&#125; \\\\--listen-client-urls=\\$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\\\--advertise-client-urls=\\$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\\\--initial-advertise-peer-urls=\\$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\\\--initial-cluster=\\$&#123;ETCD_INITIAL_CLUSTER&#125; \\\\--initial-cluster-token=\\$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\\\--initial-cluster-state=new \\\\--cert-file=/opt/kubernetes/ssl/server.pem \\\\--key-file=/opt/kubernetes/ssl/server-key.pem \\\\--peer-cert-file=/opt/kubernetes/ssl/server.pem \\\\--peer-key-file=/opt/kubernetes/ssl/server-key.pem \\\\--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable etcd 关于systemd的知识，自己补充哈。 4.3、启动Etcd节点12345[root@k8s-master-90 ~]# chmod +x etcd.sh[root@k8s-master-90 ~]# cp ssl/ca*pem ssl/server*pem /opt/kubernetes/ssl/[root@k8s-master-90 ~]# ls /opt/kubernetes/ssl/ca-key.pem ca.pem server-key.pem server.pem[root@k8s-master-90 ~]# ./etcd.sh etcd01 10.0.10.90 etcd01=https://10.0.10.90:2380,etcd02=https://10.0.10.91:2380,etcd03=https://10.0.10.92:2380 #因为etcd集群通过证书通讯，所以这里需要写成https通讯协议。 启动过程中，有错误去看/var/log/message日志，或者看以下两个文件是否配置正确1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1、配置文件[root@k8s-master-90 ~]# cat /opt/kubernetes/cfg/etcd#[Member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://10.0.10.90:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://10.0.10.90:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.0.10.90:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.0.10.90:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://10.0.10.90:2380,etcd02=https://10.0.10.91:2380,etcd03=https://10.0.10.92:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#2、启动文件[root@k8s-master-90 ~]# cat /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=-/opt/kubernetes/cfg/etcdExecStart=/opt/kubernetes/bin/etcd \\--name=$&#123;ETCD_NAME&#125; \\--data-dir=$&#123;ETCD_DATA_DIR&#125; \\--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \\--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \\--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\--initial-cluster-state=new \\--cert-file=/opt/kubernetes/ssl/server.pem \\--key-file=/opt/kubernetes/ssl/server-key.pem \\--peer-cert-file=/opt/kubernetes/ssl/server.pem \\--peer-key-file=/opt/kubernetes/ssl/server-key.pem \\--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 以上两个文件都是通过etcd.sh生成的。 4.4、Etcd节点启动状态目前启动可一个etcd节点，上面我们在执行./etcd.sh ** 这条命令时指定了etcd的其他节点，而其他两个节点目前并没有部署好，所以现在部署的这个节点状态就是失败的，一直超时，原因是在寻找其他两个节点，我们来看下当前状态：12345678910111213141516171819202122232425262728293031323334[root@k8s-master-90 ~]# systemctl status etcd.service # ● etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: activating (auto-restart) (Result: timeout) since 二 2018-04-24 18:08:20 CST; 98ms ago Process: 21474 ExecStart=/opt/kubernetes/bin/etcd --name=$&#123;ETCD_NAME&#125; --data-dir=$&#123;ETCD_DATA_DIR&#125; --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; --initial-cluster-state=new --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem (code=killed, signal=TERM)Main PID: 21474 (code=killed, signal=TERM)4月 24 18:08:20 k8s-master-90 systemd[1]: Failed to start Etcd Server. # 提示启动失败，忽略即可，其他节点起来后这个节点就会正常了。4月 24 18:08:20 k8s-master-90 systemd[1]: Unit etcd.service entered failed state.4月 24 18:08:20 k8s-master-90 systemd[1]: etcd.service failed.[root@k8s-master-90 ~]# systemctl status etcd.service● etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: activating (start) since 二 2018-04-24 18:09:50 CST; 28s agoMain PID: 21531 (etcd) Memory: 10.6M CGroup: /system.slice/etcd.service └─21531 /opt/kubernetes/bin/etcd --name=etcd01 --data-dir=/var/lib/etcd/default.etcd --li...4月 24 18:10:17 k8s-master-90 etcd[21531]: abd375109ec937d4 became candidate at term 580 # 又启动etcd进程尝试连接我们指定的集群4月 24 18:10:17 k8s-master-90 etcd[21531]: abd375109ec937d4 received MsgVoteResp from abd375109e...5804月 24 18:10:17 k8s-master-90 etcd[21531]: abd375109ec937d4 [logterm: 1, index: 3] sent MsgVote ...5804月 24 18:10:17 k8s-master-90 etcd[21531]: abd375109ec937d4 [logterm: 1, index: 3] sent MsgVote ...5804月 24 18:10:18 k8s-master-90 etcd[21531]: abd375109ec937d4 is starting a new election at term 5804月 24 18:10:18 k8s-master-90 etcd[21531]: abd375109ec937d4 became candidate at term 5814月 24 18:10:18 k8s-master-90 etcd[21531]: abd375109ec937d4 received MsgVoteResp from abd375109e...5814月 24 18:10:18 k8s-master-90 etcd[21531]: abd375109ec937d4 [logterm: 1, index: 3] sent MsgVote ...5814月 24 18:10:18 k8s-master-90 etcd[21531]: abd375109ec937d4 [logterm: 1, index: 3] sent MsgVote ...5814月 24 18:10:19 k8s-master-90 etcd[21531]: `publish error: etcdserver: request timed out` #提示连接超时。Hint: Some lines were ellipsized, use -l to show in full.[root@k8s-master-90 ~]# ps -ef|grep etcd # 查看etcd进程，是启动状态即可 那么第一个节点启动的状态就是这样，接下来需要配置第二个和第三个节点，步骤如上，需要的key自己cp到机器上即可。 记录下配置其他两个节点的命令和过程123456789[root@k8s-master-90 ~]# scp -r /opt/kubernetes/ 10.0.10.91:/opt[root@k8s-master-90 ~]# scp -r /opt/kubernetes/ 10.0.10.92:/opt[root@k8s-master-90 ~]# scp etcd.sh 10.0.10.91:~/ [root@k8s-master-90 ~]# scp etcd.sh 10.0.10.92:~/# 启动etcd节点[root@k8s-node-91 ~]# ./etcd.sh etcd02 10.0.10.91 etcd01=https://10.0.10.90:2380,etcd02=https://10.0.10.91:2380,etcd03=https://10.0.10.92:2380[root@k8s-node-92 ~]# ./etcd.sh etcd03 10.0.10.92 etcd01=https://10.0.10.90:2380,etcd02=https://10.0.10.91:2380,etcd03=https://10.0.10.92:2380Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service. 当三个节点部署完成后，随即查看一个节点，启动正常啦！1234567891011121314151617181920[root@k8s-node-92 ~]# systemctl status etcd● etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since 二 2018-04-24 18:40:36 CST; 21s agoMain PID: 26720 (etcd) Memory: 12.1M CGroup: /system.slice/etcd.service └─26720 /opt/kubernetes/bin/etcd --name=etcd03 --data-dir=/var/lib/etcd/default.etcd --li...4月 24 18:40:36 k8s-node-92 etcd[26720]: serving insecure client requests on 127.0.0.1:2379, th...ged!4月 24 18:40:36 k8s-node-92 systemd[1]: Started Etcd Server.4月 24 18:40:36 k8s-node-92 etcd[26720]: serving client requests on 10.0.10.92:23794月 24 18:40:36 k8s-node-92 etcd[26720]: established a TCP streaming connection with peer 2c5fd...ter)4月 24 18:40:36 k8s-node-92 etcd[26720]: established a TCP streaming connection with peer 2c5fd...ter)4月 24 18:40:36 k8s-node-92 etcd[26720]: established a TCP streaming connection with peer abd37...der)4月 24 18:40:38 k8s-node-92 etcd[26720]: updated the cluster version from 3.0 to 3.24月 24 18:40:38 k8s-node-92 etcd[26720]: enabled capabilities for version 3.24月 24 18:40:41 k8s-node-92 etcd[26720]: the clock difference against peer 2c5fdcec8fe643fa is ... 1s]4月 24 18:40:41 k8s-node-92 etcd[26720]: the clock difference against peer abd375109ec937d4 is ... 1s]Hint: Some lines were ellipsized, use -l to show in full. etcd集群配置完成. 4.5、查看集群健康状态123456[root@k8s-master-90 ~]# cd ssl/[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" cluster-healthmember 2c5fdcec8fe643fa is healthy: got healthy result from https://10.0.10.91:2379member abd375109ec937d4 is healthy: got healthy result from https://10.0.10.90:2379member e2de0f9eec5cb008 is healthy: got healthy result from https://10.0.10.92:2379`cluster is healthy` 当然，你可以停掉一个节点再查看集群状态，看看会有什么变化。 Tips： 2379用于客户端通信 2380用于节点通信 五、部署Flannel网络Overlay Network： 覆盖网络，在基础网络上叠加的一种虚拟网络技术模式，该网络中的主机通过虚拟链路连接起来。VXLAN：将源数据包封装到UDP中，并使用基础网络的IP/MAC作为外层报文头进行封装，然后在以太网上传输，到达目的地后有隧道端点解封装并将数据发送给目标地址。Flannel：是Overlay网络的一种，也是将源数据包封装在另一种网络包里进行路由转发和通信，目前已经支持UDP、VXLAN、AWS VPC和GCE路由等数据转发方式。多主机容器网络通信其他主流方案：隧道方案（Weave、OpenSwitch），路由方案（Calico）等。 5.1、VXLAN网络架构图 5.2、Flannel网络架构图 5.3、注册Flannel信息到Etcd集群12345678# 写入分配的子网段到etcd，供flanneld使用[root@k8s-master-90 ~]# cd ssl/[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" set /coreos.com/network/config '&#123; \"Network\": \"172.17.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'&#123; \"Network\": \"172.17.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;# GET 查看设置[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" get /coreos.com/network/config '&#123; \"Network\": \"172.17.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'&#123; \"Network\": \"172.17.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125; #能get到我们写入etcd集群的信息，即表明写入成功。 5.4、下载二进制包下载地址：https://github.com/coreos/flannel/releases123[root@k8s-master-90 ~]# wget https://github.com/coreos/flannel/releases/download/v0.9.1/flannel-v0.9.1-linux-amd64.tar.gz[root@k8s-master-90 ~]# cd flannel-v0.9.1-linux-amd64.tar.gz[root@k8s-master-90 ~]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 5.5、Flannel部署文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[root@k8s-master-90 ~]# cat flanneld.sh #!/bin/bashETCD_ENDPOINTS=$&#123;1:-\"http://127.0.0.1:2379\"&#125; #接收ETCD的参数 cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/flanneld # 生成Flanneld配置文件FLANNEL_OPTIONS=\"--etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\-etcd-cafile=/opt/kubernetes/ssl/ca.pem \\-etcd-certfile=/opt/kubernetes/ssl/server.pem \\-etcd-keyfile=/opt/kubernetes/ssl/server-key.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/flanneld.service # 生成Flanneld启动文件，使Systemd管理Flannel[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld --ip-masq \\$FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/docker.service # 配置Docker启动指定子网段[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.env # 添加了这行（使Docker在启动时指定子网段，这个子网段是Flanneld分配给它的）ExecStart=/usr/bin/dockerd \\$DOCKER_NETWORK_OPTIONS # 添加了DOCKER_NETWORK_OPTIONS 选项ExecReload=/bin/kill -s HUP \\$MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.targetEOF# 启动flanneld和Dockersystemctl daemon-reloadsystemctl enable flanneldsystemctl restart flanneldsystemctl restart docker 5.6、执行部署123[root@k8s-master-90 ~]# chmod +x flanneld.sh [root@k8s-master-90 ~]# ./flanneld.sh https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service. 5.7、生成的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# flanneld配置文件[root@k8s-master-90 ~]# cat /opt/kubernetes/cfg/flanneldFLANNEL_OPTIONS=\"--etcd-endpoints=https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379 -etcd-cafile=/opt/kubernetes/ssl/ca.pem -etcd-certfile=/opt/kubernetes/ssl/server.pem -etcd-keyfile=/opt/kubernetes/ssl/server-key.pem\"# Flanneld启动文件[root@k8s-master-90 ~]# cat /usr/lib/systemd/system/flanneld.service [Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.target# Flanneld的网络配置文件（Docker会根据这个网络配置文件信息生成子网段）[root@k8s-master-90 ~]# cat /run/flannel/subnet.envDOCKER_OPT_BIP=\"--bip=172.17.14.1/24\"DOCKER_OPT_IPMASQ=\"--ip-masq=false\"DOCKER_OPT_MTU=\"--mtu=1450\"DOCKER_NETWORK_OPTIONS=\" --bip=172.17.14.1/24 --ip-masq=false --mtu=1450\"# Docker启动文件[root@k8s-master-90 ~]# cat /usr/lib/systemd/system/docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.env #指定了Flanneld网络配置文件ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 5.8、查看Flanneld部署结果12345678910111213141516171819[root@k8s-master-90 ~]# ip a···略···2: em1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 20:47:47:92:22:18 brd ff:ff:ff:ff:ff:ff inet 10.0.10.90/24 brd 10.0.10.255 scope global em1 valid_lft forever preferred_lft forever inet6 fe80::2247:47ff:fe92:2218/64 scope link valid_lft forever preferred_lft forever···略···6: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:01:74:f4:09 brd ff:ff:ff:ff:ff:ff inet 172.17.14.1/24 brd 172.17.14.255 scope global docker0 valid_lft forever preferred_lft forever7: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 4a:db:19:d7:f5:31 brd ff:ff:ff:ff:ff:ff inet 172.17.14.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::48db:19ff:fed7:f531/64 scope link valid_lft forever preferred_lft forever 能看到docker0网卡的IP地址段已经变更为我们配置的地址段。该节点新启动了flannel.1网卡。 5.9、Node节点快速部署flannal网络正常情况下，Flanneld网络是在Node节点（也就是k8s工作节点）上部署的，我们在Master节点部署了也没关系。 部署步骤按照上面的来就行。 123456789101112131415161718192021222324# Master节点推送需要的文件到Node节点[root@k8s-master-90 ~]# scp /opt/kubernetes/bin/&#123;flanneld,mk-docker-opts.sh&#125; 10.0.10.91:/opt/kubernetes/bin/[root@k8s-master-90 ~]# scp /opt/kubernetes/bin/&#123;flanneld,mk-docker-opts.sh&#125; 10.0.10.92:/opt/kubernetes/bin/[root@k8s-master-90 ~]# scp flanneld.sh 10.0.10.91:~/[root@k8s-master-90 ~]# scp flanneld.sh 10.0.10.92:~/# 去各节点执行Flanneld部署[root@k8s-node-91 ~]# ./flanneld.sh https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service.[root@k8s-node-92 ~]# ./flanneld.sh https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379Created symlink from /etc/systemd/system/multi-user.target.wants/flanneld.service to /usr/lib/systemd/system/flanneld.service.# 查看一台Node的部署状态，如下部署正常。[root@k8s-node-92 ~]# ip a6: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:f0:ac:60:ce brd ff:ff:ff:ff:ff:ff inet 172.17.9.1/24 brd 172.17.9.255 scope global docker0 valid_lft forever preferred_lft forever7: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN link/ether 7a:b7:8e:db:ba:83 brd ff:ff:ff:ff:ff:ff inet 172.17.9.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::78b7:8eff:fedb:ba83/64 scope link valid_lft forever preferred_lft forever[root@k8s-node-92 ~]# 5.10、查看Etcd集群记录的通讯信息每个节点在部署Flanneld网络时，相关信息都记录在Etcd集群当中，我们来一下123456789# ls /coreos.com/network/ 查看etcd存储的网络配置信息[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" ls /coreos.com/network/# ls /coreos.com/network/subnets 查看Flanneld分配的子网信息[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" ls /coreos.com/network/subnets# get /coreos.com/network/subnets/172.17.14.0-24 查看该子网的详细信息[root@k8s-master-90 ssl]# /opt/kubernetes/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=\"https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379\" get /coreos.com/network/subnets/172.17.14.0-24&#123;\"PublicIP\":\"10.0.10.90\",\"BackendType\":\"vxlan\",\"BackendData\":&#123;\"VtepMAC\":\"4a:db:19:d7:f5:31\"&#125;&#125; 解读： 子网信息里记录了公网IP10.0.10.90，也就是说172.17.14.0这个网段的通讯，会通过公网IP转发出去与其他节点的网段进行通讯。 所以，只要是加入了Flanneld网络的主机节点，都可以通过etcd记录的信息，与其他子网进行通讯。 5.11、测试Flanneld网络通讯上面我们部署了三个节点的网络，这三个节点的Flannal网段信息分别为： k8s-master-90 flannel.1: 172.17.14.0/32 docker0: 172.17.14.1/24 k8s-master-91 flannel.1: 172.17.12.0/32 docker0: 172.17.12.1/24 k8s-master-92 flannel.1: 172.17.9.0/32 docker0: 172.17.9.1/24 通过ping检测他们之间的连通性，发现他们是不同的子网，为什么能连通呢？1234567891011121314[root@k8s-master-90 ~]# ping -c 2 172.17.12.1PING 172.17.12.1 (172.17.12.1) 56(84) bytes of data.64 bytes from 172.17.12.1: icmp_seq=1 ttl=64 time=0.466 ms64 bytes from 172.17.12.1: icmp_seq=2 ttl=64 time=0.370 ms[root@k8s-node-91 ~]# ping -c 2 172.17.9.1PING 172.17.9.1 (172.17.9.1) 56(84) bytes of data.64 bytes from 172.17.9.1: icmp_seq=1 ttl=64 time=0.468 ms64 bytes from 172.17.9.1: icmp_seq=2 ttl=64 time=0.471 ms[root@k8s-node-92 ~]# ping -c 2 172.17.14.1PING 172.17.14.1 (172.17.14.1) 56(84) bytes of data.64 bytes from 172.17.14.1: icmp_seq=1 ttl=64 time=0.535 ms64 bytes from 172.17.14.1: icmp_seq=2 ttl=64 time=0.396 ms 带着这个思考，我们来看一眼节点上的路由情况12345678[root@k8s-master-90 ssl]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.10.1 0.0.0.0 UG 100 0 0 em110.0.10.0 0.0.0.0 255.255.255.0 U 100 0 0 em1172.17.9.0 172.17.9.0 255.255.255.0 UG 0 0 0 flannel.1172.17.12.0 172.17.12.0 255.255.255.0 UG 0 0 0 flannel.1172.17.14.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 解读： docker0网卡是172.17.14.0网段，也就是节点自身的网段； 规则：172.17.12.0和172.17.9.0网段走flannel.1网卡出口，而flannel.1能与其他两个网段直接通讯； 规则：10.0.10.0是宿主机的网段，出口是物理网卡em1； 根据上面解读的路由规则，我尝试通过一个案例来理解网络请求走向：12# (k8s-master-90)172.17.14.0网段向(k8s-master-91)172.17.12.0网段发起一个ping请求[root@k8s-master-90 ssl]# ping -c 2 172.17.12.1 k8s-master-90节点匹配到本机路由规则：去往172.17.12.0 网段，走 flannel.1出口 ； 通过 /opt/kubernetes/bin/etcdctl get到172.17.14.0-24的公网IP是10.0.10.90，报文交由10.0.10.0网段处理； k8s-master-90节点再次匹配到本机路由规则：去往10.0.10.0网段，走em1（物理网卡）出口。 公网IP10.0.10.90通过em1网卡将报文发送给10.0.10.91（etcd里注册了172.17.12.0网段的公网IP是10.0.10.91）; 10.0.10.91转发flannel.1网卡， flannel.1网卡再转发到docker0网卡，等待报文被响应后，再原路返回。完成整个通讯。 Flanneld网络部署完毕。 六、获取K8S二进制包并部署官网：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.9.md二进制包：https://storage.googleapis.com/kubernetes-release/release/v1.9.5/kubernetes-server-linux-amd64.tar.gz我们选用1.9.5版本。 下载二进制包1[root@k8s-master-90 ~]# wget https://storage.googleapis.com/kubernetes-release/release/v1.9.5/kubernetes-server-linux-amd64.tar.gz 根据前期的环境规划，Master节点需要kube-scheduler,kube-controller-manager,kube-apiserver这三个组件，kubectl用于管理集群；而Node节点只需要kubelet,kube-proxy这两个组件即可。 1、复制Master需要的程序到bin目录1234567[root@k8s-master-90 ~]# tar zxf kubernetes-server-linux-amd64.tar.gz[root@k8s-master-90 ~]# cp kubernetes/server/bin/&#123;kubectl,kube-scheduler,kube-controller-manager,kube-apiserver&#125; /opt/kubernetes/bin/# 配置环境变量[root@k8s-master-90 ~]# echo \"export PATH=$PATH:/opt/kubernetes/bin\" &gt;&gt;/etc/profile[root@k8s-master-90 ~]# source /etc/profile[root@k8s-master-90 ~]# which kubectl /opt/kubernetes/bin/kubectl 2、复制Node节点需要的程序到bin目录12[root@k8s-master-90 ~]# scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; 10.0.10.91:/opt/kubernetes/bin/[root@k8s-master-90 ~]# scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; 10.0.10.92:/opt/kubernetes/bin/ K8S的各组件部署完成 七、创建Kubeconfig文件7.1、k8s的api-server认证方式这个是知识了解，api-server有如下几种认证方式： CA证书认证 Token认证：token-auth 基本认证：basic-auth kubernetes 认证主要分为三种：CA 证书认证、Token 认证、Base 认证。可以同时配置多种认证方式，只要其中任意一个方式认证通过即可。 参考： Kubernetes 的安全机制 APIServer 认证、授权、准入控制 Kubernetes apiserver认证 本次部署使用第二种：Token认证。 7.2、创建配置文件需要创建以下三个配置文件。 1、创建TLS Bootstrapping Token 2、创建kubelet kubeconfig 3、创建kube-proxy kubeconf 我们使用一个脚本将所有的配置文件一起创建出来12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[root@k8s-master-90 ssl]# cat kubeconfig.sh # 放到Master节点的/root/ssl目录下# 创建 TLS Bootstrapping Tokenexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ') # 生成随机token（随机字符串）cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF#----------------------# 创建kubelet bootstrapping kubeconfig export KUBE_APISERVER=\"https://10.0.10.90:6443\" # 这个脚本唯一需要更改的就是这个地方，填写成你的Master IP地址# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=./ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig#----------------------# 创建kube-proxy kubeconfig文件kubectl config set-cluster kubernetes \\ --certificate-authority=./ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \\ --client-certificate=./kube-proxy.pem \\ --client-key=./kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig[root@k8s-master-90 ~]# which kubectl /opt/kubernetes/bin/kubectl[root@k8s-master-90 ssl]# ./kubeconfig.sh # 前提是你的环境必须要能使用kubectl Cluster \"kubernetes\" set.User \"kubelet-bootstrap\" set.Context \"default\" created.Switched to context \"default\".Cluster \"kubernetes\" set.User \"kube-proxy\" set.Context \"default\" created.Switched to context \"default\". 7.3、创建Token租户并绑定角色这是我们创建的tuken信息，需要把租户信息创建出来，Node节点在部署kubelet组件时需要通过token租户进行权限验证12cat token.csve7041b846ee11c3b7581f1e210f83159,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" 创建123456789[root@k8s-master-90 ~]# kubectl create --help[root@k8s-master-90 ~]# kubectl create clusterrolebinding --helpUsage: kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname][root@k8s-master-90 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap clusterrolebinding \"kubelet-bootstrap\" created[root@k8s-master-90 ~]# kubectl get clusterrole # 查看集群角色（system:node-bootstrapper由此得来） 7.3、配置文件的用途总共创建了三个文件，用途分别为： bootstrap.kubeconfig # Node节点的kubelet组件使用 kube-proxy.kubeconfig # Node节点的kube-proxy组件使用 token.csv # Master节点的apiserver组件使用 7.4、查看创建的配置文件token.csv1234567[root@k8s-master-90 ssl]# cat token.csve7041b846ee11c3b7581f1e210f83159,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"注解：- e7041b846ee11c3b7581f1e210f83159：随机生成的token- kubelet-bootstrap ：用户名- 10001：用户ID- system:kubelet-bootstrap：用户组 bootstrap.kubeconfig1234567891011121314151617181920[root@k8s-master-90 ssl]# cat bootstrap.kubeconfig apiVersion: v1 #api的版本clusters: # 集群的内容- cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVUndYVWFRSnJVbjVjb0haMFF0bm8wa1M5VTBBd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU0TURReE56RXdNemd3TUZvWERUSXpNRFF4TmpFd016Z3dNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbGFXcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBMEkrQTkyajNGdHUvOGljckRBaVUKY0M5aUVKNTU2MWlrUllFaXN4TkV6UHIrdXdSOFY1MUY0cDlCajhQKzBDN1JtNW9qa01HTEdWZGNMZjZBT21XTwo3U25jZjJzQVc3N2ljQmNOQ1ExbDhXcG9vVEMzNDRVbWVuUVZZa3lxRWJrOUlMdGVkVHpnMm9NYnR2Z253cXV5CnFYOFFVUWZ0N0hDdXNLeE1ZUU5VSTRSajQyNzBUWmJxRDgwVFQ3ZVRmTFFtL0krSUdLeHcvWWtEOHdtQmlFWFQKR21yakNUMGtURVZZKzFVQUVKOGNYN0pvT3RCazV6SW5NYVFvbkxDTFlNaHNuY3ljVytwZjZwZTFROE5STU9VYwo4SzZCNWY5SHlqZXczcTZtdkdjR1JDSGxZQ3Qzb2ZBSjFYQWdmT1llVUYrMFhwNXJOZE5jUEFscTVkYW83OFV3Ck1RSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVcU1kdVdoMEoxbDMvdFArWCtzc2UxbFRPakgwd0h3WURWUjBqQkJnd0ZvQVVxTWR1V2gwSgoxbDMvdFArWCtzc2UxbFRPakgwd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFCYU8xa1JpS0MrQzBJUkZMSmRlCjFXWStyV3NJSDZaL1VHN1BiUVVxZ1JQcWd2M3FRRytFSWJjZkhicEpRV3k0UjhxUk1ZWGpsTU1ZallUVlhqMzcKbC92UXNQTm1Wc3I4Y0dlcnRrY3ZaVUx6WmJyclY1bWZhNWlrMGltSzZHMzdnQUZ6S0NoRHNqazRRVVg4dEpnUworWlNaRmpLRzRMcGQwRzltbEd6OHZ6YVpseGs0YWlsaldvWTNxb2wvZWR6d1BBVVBnL0pyMW1LT21DWnpVTUdBCnZFTkkvV0pFRFU4eCs5VXZoditQUTdURGtCNGFIR3dXSG53RjA0RlFjOGR0TWMwOUFhMmlmQWk1Uy80ZE1sVVYKKzJKVXN0MHJOeDdNSHVjTzE0RFRuOUVDQ01DVEJxNkpna29PK2VhY3Y2SDU3RTAyMnp1bDNyNHJ1V3JFSmFVeQpLcmc9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K # CA数字证书 server: https://10.0.10.90:6443 # server的地址 name: kubernetes # 集群的名字contexts: # 上下文内容- context: cluster: kubernetes user: kubelet-bootstrap # k8s用户（可改变，是从token.csv里定义的） name: default # 上下文名称current-context: default # 当前默认使用的上下文kind: Configpreferences: &#123;&#125;users: # 用户的信息- name: kubelet-bootstrap # 用户名 user: as-user-extra: &#123;&#125; token: e7041b846ee11c3b7581f1e210f83159 # token，关键！必须要与token.csv里的token对应！不对应则没有相应的权限，会认证失败。 kube-proxy.kubeconfig123456789101112131415161718192021[root@k8s-master-90 ssl]# cat kube-proxy.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVUndYVWFRSnJVbjVjb0haMFF0bm8wa1M5VTBBd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU0TURReE56RXdNemd3TUZvWERUSXpNRFF4TmpFd016Z3dNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbGFXcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBMEkrQTkyajNGdHUvOGljckRBaVUKY0M5aUVKNTU2MWlrUllFaXN4TkV6UHIrdXdSOFY1MUY0cDlCajhQKzBDN1JtNW9qa01HTEdWZGNMZjZBT21XTwo3U25jZjJzQVc3N2ljQmNOQ1ExbDhXcG9vVEMzNDRVbWVuUVZZa3lxRWJrOUlMdGVkVHpnMm9NYnR2Z253cXV5CnFYOFFVUWZ0N0hDdXNLeE1ZUU5VSTRSajQyNzBUWmJxRDgwVFQ3ZVRmTFFtL0krSUdLeHcvWWtEOHdtQmlFWFQKR21yakNUMGtURVZZKzFVQUVKOGNYN0pvT3RCazV6SW5NYVFvbkxDTFlNaHNuY3ljVytwZjZwZTFROE5STU9VYwo4SzZCNWY5SHlqZXczcTZtdkdjR1JDSGxZQ3Qzb2ZBSjFYQWdmT1llVUYrMFhwNXJOZE5jUEFscTVkYW83OFV3Ck1RSURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVcU1kdVdoMEoxbDMvdFArWCtzc2UxbFRPakgwd0h3WURWUjBqQkJnd0ZvQVVxTWR1V2gwSgoxbDMvdFArWCtzc2UxbFRPakgwd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFCYU8xa1JpS0MrQzBJUkZMSmRlCjFXWStyV3NJSDZaL1VHN1BiUVVxZ1JQcWd2M3FRRytFSWJjZkhicEpRV3k0UjhxUk1ZWGpsTU1ZallUVlhqMzcKbC92UXNQTm1Wc3I4Y0dlcnRrY3ZaVUx6WmJyclY1bWZhNWlrMGltSzZHMzdnQUZ6S0NoRHNqazRRVVg4dEpnUworWlNaRmpLRzRMcGQwRzltbEd6OHZ6YVpseGs0YWlsaldvWTNxb2wvZWR6d1BBVVBnL0pyMW1LT21DWnpVTUdBCnZFTkkvV0pFRFU4eCs5VXZoditQUTdURGtCNGFIR3dXSG53RjA0RlFjOGR0TWMwOUFhMmlmQWk1Uy80ZE1sVVYKKzJKVXN0MHJOeDdNSHVjTzE0RFRuOUVDQ01DVEJxNkpna29PK2VhY3Y2SDU3RTAyMnp1bDNyNHJ1V3JFSmFVeQpLcmc9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://10.0.10.90:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: as-user-extra: &#123;&#125; client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQzakNDQXNhZ0F3SUJBZ0lVSlZ6aUJQNWI0YzJvQ1BBT2pQS1FnalI1SjhVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEU0TURReE56RXdNemd3TUZvWERUSTRNRFF4TkRFd016Z3dNRm93YkRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbGFVcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HVTNsemRHVnRNUm93R0FZRFZRUURFeEZ6ZVhOMFpXMDZhM1ZpClpTMXdjbTk0ZVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT0x2UUgvQmNWZ0kKdlgrYlRVck1xMmU1ZzAwQVdyRHMzbkpobTM2eFQyZUpVTmVPdU80NGJUY25RTzlVQlkrSHFyMWJIcmEvYjZBVQoxTEJIa0xLZEh1TEl6S0grVklIR3JVaExCZjFCazBqeGxUWkV5SGV0ajkxQlV0bWtYWkhnRUY4RkY1VDdZeElpCjdNOTVsZGFaZnpXMFkwaW4vVEIzSUlyejJkWVRjekhVUzNNYVFqeFRicE5XSUpLTHplOGpVTDVQakF1eVVvS0UKcUp1S0pRRE53S1pGT05DanU2TDVhVWF6Vy9oejJHdVNuSm1xUXRkR3dDT3pVSStTMjlwV2NTVHR5Mk1DNG5CeQp0ZWg2L2c1M0Z0WG8wTkJMMVpoNEhqQTYxZlZIeklKYVVucnlUeEs2bmVmR2UwRVJ6R0RUM2VhZXFmdjdPYjJXClJNcU5wR0M4OVIwQ0F3RUFBYU4vTUgwd0RnWURWUjBQQVFIL0JBUURBZ1dnTUIwR0ExVWRKUVFXTUJRR0NDc0cKQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFNQmdOVkhSTUJBZjhFQWpBQU1CMEdBMVVkRGdRV0JCUmhEMEQzckZkdQozdG5CZzd5SllWRHJ6R1FmR0RBZkJnTlZIU01FR0RBV2dCU294MjVhSFFuV1hmKzAvNWY2eXg3V1ZNNk1mVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQUxEa0tCTTh6MWpKRmZyNDFLZ0RhN20zWGcrN3RmNm0yU3Y2T0pVTUUKTjRVVmZWd3oyM1djQ2lUR0lGTmh6VFpJZUFWY1c2SnZ3TE5iUVJMRXY5d3BhMmVmL2dIOFp0azlZbXVvRFdicQpXU1Nkd3VzNUx0Zk9yN0pSVTlneXJQeWlOWitlVUtUc2tMSUZqZS9mZnFaZVc0VFY3VnN6U3lZOXl1Z2MvZ255CmdjYXN0V045YlF1am1EaEQwL09EWkdpNWJ1eUlPOFgxc1EyZzVxWHZYSEYvblJSUkZkdDVSRmZOQXExWGozcWcKMnVGNnh6ZjdIdWJXcWJnemEybTQvUXJteStyQVgvWWQ3M3RxTEc1cnJ5Mms2dmNwVVdVaENvdE5mZHp3K0pBeApGS2JseC9kYlZSL1pyRmRVeTd2ZlZpbTVmKzBlV2hneTltVFVyOU4xT2lZOWhRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: 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 7.5、传送配置文件到Node节点文件是在Master节点上生成的，需要在Node节点上使用，这里传送过去即可1234567[root@k8s-master-90 ssl]# scp bootstrap.kubeconfig kube-proxy.kubeconfig 10.0.10.91:/opt/kubernetes/cfg/bootstrap.kubeconfig 100% 2186 2.1KB/s 00:00 kube-proxy.kubeconfig 100% 6288 6.1KB/s 00:00 [root@k8s-master-90 ssl]# scp bootstrap.kubeconfig kube-proxy.kubeconfig 10.0.10.92:/opt/kubernetes/cfg/bootstrap.kubeconfig 100% 2186 2.1KB/s 00:00 kube-proxy.kubeconfig 100% 6288 6.1KB/s 00:00 [root@k8s-master-90 ssl]# 八、启动Master组件根据前面环境规划，Master节点将启动apiserver、controller-manager和scheduler组件，下面是启动详情。 官网有启动组件的相关脚本：https://github.com/kubernetes/kubernetes/tree/master/cluster/centos/master/scripts 8.1、apiserver1、启动的脚本apiserver.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@k8s-master-90 ~]# cat apiserver.sh #!/bin/bashMASTER_ADDRESS=$&#123;1:-\"192.168.1.195\"&#125;ETCD_SERVERS=$&#123;2:-\"http://127.0.0.1:2379\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--etcd-servers=$&#123;ETCD_SERVERS&#125; \\\\--insecure-bind-address=127.0.0.1 \\\\--bind-address=$&#123;MASTER_ADDRESS&#125; \\\\--insecure-port=8080 \\\\--secure-port=6443 \\\\--advertise-address=$&#123;MASTER_ADDRESS&#125; \\\\--allow-privileged=true \\\\--service-cluster-ip-range=10.10.10.0/24 \\\\--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\\\--kubelet-https=true \\\\--enable-bootstrap-token-auth \\\\--token-auth-file=/opt/kubernetes/cfg/token.csv \\\\--service-node-port-range=30000-50000 \\\\--tls-cert-file=/opt/kubernetes/ssl/server.pem \\\\--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\\\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--etcd-cafile=/opt/kubernetes/ssl/ca.pem \\\\--etcd-certfile=/opt/kubernetes/ssl/server.pem \\\\--etcd-keyfile=/opt/kubernetes/ssl/server-key.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-apiserversystemctl restart kube-apiserver 从上面apiserver脚本中，你不难发现它需要的证书有这些：12345/opt/kubernetes/cfg/token.csv/opt/kubernetes/ssl/server.pem/opt/kubernetes/ssl/server-key.pem/opt/kubernetes/ssl/ca.pem/opt/kubernetes/ssl/ca-key.pem 所以你的ssl目录必须要确保有这些证书。12# 注意：token.scv需要放在cfg目录而非ssl目录。[root@k8s-master-90 ~]# cp ssl/token.csv /opt/kubernetes/cfg/ 2、启动apiserver1234# apiserver.sh 接收两个参数，第一个是apiserver的地址，第二个是ETCD集群的地址，我们集群有三个节点，都填写上。[root@k8s-master-90 ~]# chmod +x apiserver.sh [root@k8s-master-90 ~]# ./apiserver.sh 10.0.10.90 https://10.0.10.90:2379,https://10.0.10.91:2379,https://10.0.10.92:2379Created symlink from /etc/systemd/system/multi-user.target.wants/kube-apiserver.service to /usr/lib/systemd/system/kube-apiserver.service. 3、查看启动状态1234[root@k8s-master-90 ~]# netstat -lntup|grep 6443tcp 0 0 10.0.10.90:6443 0.0.0.0:* LISTEN 24140/kube-apiserve [root@k8s-master-90 ~]# ps -ef|grep kube-apiserver# 信息太多，不贴了 注意： 如果你照着上面的脚本执行后，如没有进程存活，你应该查看下日志和检查上面的证书都是否存在。 一定要先启动apiserver！controller-manager和scheduler可以没有先后顺序。 8.2、controller-manager1、启动的脚本controller-manager.sh123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master-90 ~]# cat controller-manager.sh #!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect=true \\\\--address=127.0.0.1 \\\\--service-cluster-ip-range=10.10.10.0/24 \\\\ # 指定了k8s集群的网段--cluster-name=kubernetes \\\\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--root-ca-file=/opt/kubernetes/ssl/ca.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-controller-managersystemctl restart kube-controller-manager 2、启动controller-manager123[root@k8s-master-90 ~]# chmod +x controller-manager.sh [root@k8s-master-90 ~]# ./controller-manager.sh Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service. 3、查看启动状态1234# controller-manager启动的端口是10252[root@k8s-master-90 ~]# netstat -lntup|grep controlltcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 25479/kube-controll [root@k8s-master-90 ~]# ps -ef|grep controller-manager 8.3、scheduler1、启动的脚本scheduler.sh12345678910111213141516171819202122232425262728293031[root@k8s-master-90 ~]# cat scheduler.sh #!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-schedulersystemctl restart kube-scheduler 2、启动scheduler123[root@k8s-master-90 ~]# chmod +x scheduler.sh [root@k8s-master-90 ~]# ./scheduler.sh Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service. 3、查看启动状态1234# scheduler启动的端口是10251[root@k8s-master-90 ~]# netstat -lntup|grep kube-scheduletcp6 0 0 :::10251 :::* LISTEN 25829/kube-schedule[root@k8s-master-90 ~]# ps -ef|grep scheduler 8.3、查看组件运行状态kubectl get cs # 查看k8s集群资源的健康信息相关阅读：Kubernetes kubectl get 命令详解 1234567[root@k8s-master-90 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;\"health\": \"true\"&#125; etcd-1 Healthy &#123;\"health\": \"true\"&#125; etcd-2 Healthy &#123;\"health\": \"true\"&#125; 看到以上信息都为健康状态，到此，Kubernetes集群的Master节点即部署完成。 九、启动Node组件根据前面环境规划，Node节点将启动kubelet和kube-proxy组件，下面是启动详情。 官网有启动组件的相关脚本：https://github.com/kubernetes/kubernetes/tree/master/cluster/centos/node/scripts 9.1、kubelet1、启动的脚本 kubelet.sh123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-node-91 ~]# cat kubelet.sh #!/bin/bashNODE_ADDRESS=$&#123;1:-\"10.0.10.91\"&#125; # 节点的IP地址，通过参数传入DNS_SERVER_IP=$&#123;2:-\"10.10.10.2\"&#125; # DNS地址，在apiserver.sh里指定的地址端，可以写10.10.10.0/24段的任意IPcat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kubeletKUBELET_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--address=$&#123;NODE_ADDRESS&#125; \\\\--hostname-override=$&#123;NODE_ADDRESS&#125; \\\\--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\\ #这里的配置文件在第七章节生成，并推送到Node节点上了的--experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\\--cert-dir=/opt/kubernetes/ssl \\\\--allow-privileged=true \\\\--cluster-dns=$&#123;DNS_SERVER_IP&#125; \\\\--cluster-domain=cluster.local \\\\--fail-swap-on=false \\\\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\" # 在启动时拉取pause镜像，用于管理网络，pod的一些信息。这个容器必须是可以docker pull下载的，不然的话pods运行不起来。EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/kubeletExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet 如果你想使用上面的脚本，请将我写的注释删除掉再使用。 2、启动kubelet12[root@k8s-node-91 ~]# ./kubelet.sh 10.0.10.91Failed to execute operation: File exists # 提示文件存在，忽略这个问题 3、查看启动状态123456789101112# 1. 查看Systemd启动状态[root@k8s-node-91 ~]# systemctl status kubelet # 查看进程启动状态，是active表明启动正常。● kubelet.service - Kubernetes Kubelet Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Active: active (running) since 四 2018-04-26 11:57:14 CST; 13s ago# 2. ps -ef查看进程启动细节[root@k8s-node-91 ~]# ps -ef|grep kubeletroot 9440 1 1 11:57 ? 00:00:00 /opt/kubernetes/bin/kubelet --logtostderr=true --v=4 --address=10.0.10.91 --hostname-override=10.0.10.91 --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig --cert-dir=/opt/kubernetes/ssl --allow-privileged=true --cluster-dns=10.10.10.2 --cluster-domain=cluster.local --fail-swap-on=false --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0root 9501 45523 0 11:57 pts/0 00:00:00 grep --color=auto kubelet[root@k8s-node-91 ~]# ls -lh /opt/kubernetes/cfg/kubelet.kubeconfig ls: 无法访问/opt/kubernetes/cfg/kubelet.kubeconfig: 没有那个文件或目录 注意：你会发现没有kubelet.kubeconfig文件，这是为什么呢？是因为k8s-Master需要给Node节点的kubelet组件颁发证书，Node节点才会生成这个证书文件。 承上，我们继续到Master节点看看证书请求状况123[root@k8s-master-90 ~]# kubectl get csr # 查看Node节点的kubelet证书申请请求NAME AGE REQUESTOR CONDITIONnode-csr-LXWVqCf2FKgHCRGvCskSEnG13F4BPrqQbDiR2epHRg4 5m kubelet-bootstrap Pending # 等待颁发状态 9.2、k8s集群为kubelet颁发证书12345678910111213141516171819[root@k8s-master-90 ~]# kubectl --help- certificate # 修改 certificate 资源.[root@k8s-master-90 ~]# kubectl certificate --help- approve 同意一个自签证书请求- deny 拒绝一个自签证书请求[root@k8s-master-90 ~]# kubectl certificate approve --help Usage: kubectl certificate approve (-f FILENAME | NAME) [options]# 上面几步help我们找打了用法[root@k8s-master-90 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-LXWVqCf2FKgHCRGvCskSEnG13F4BPrqQbDiR2epHRg4 6m kubelet-bootstrap Pending[root@k8s-master-90 ~]# kubectl certificate approve node-csr-LXWVqCf2FKgHCRGvCskSEnG13F4BPrqQbDiR2epHRg4certificatesigningrequest \"node-csr-LXWVqCf2FKgHCRGvCskSEnG13F4BPrqQbDiR2epHRg4\" approved[root@k8s-master-90 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-LXWVqCf2FKgHCRGvCskSEnG13F4BPrqQbDiR2epHRg4 7m kubelet-bootstrap Approved,Issued # 同意办法证书后，变为：批准状态。 9.3、查看Node节点加入k8s集群123[root@k8s-master-90 ~]# kubectl get node # 查看集群中的Node节点信息，Ready表示该节点健康，已准备就绪NAME STATUS ROLES AGE VERSION10.0.10.91 Ready &lt;none&gt; 18m v1.9.5 9.4、查看Node节点签发的证书123456789[root@k8s-node-91 ~]# ls -lh /opt/kubernetes/cfg/kubelet.kubeconfig # 已经有这个文件了，你可以cat查看内容细节-rw------- 1 root root 2.3K 4月 26 12:00 /opt/kubernetes/cfg/kubelet.kubeconfig # ssl目录生成如下证书[root@k8s-node-91 ~]# ls -lh /opt/kubernetes/ssl/kubelet*-rw-r--r-- 1 root root 1.1K 4月 26 12:00 /opt/kubernetes/ssl/kubelet-client.crt-rw------- 1 root root 227 4月 26 11:52 /opt/kubernetes/ssl/kubelet-client.key-rw-r--r-- 1 root root 1.1K 4月 26 11:32 /opt/kubernetes/ssl/kubelet.crt-rw------- 1 root root 1.7K 4月 26 11:32 /opt/kubernetes/ssl/kubelet.key 整个自签发证书颁发流程完结。 9.5、kube-proxy1、启动的脚本 proxy.sh12345678910111213141516171819202122232425262728293031[root@k8s-node-91 ~]# cat proxy.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"10.0.10.91\"&#125; # kube-proxy的节点IPcat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=$&#123;NODE_ADDRESS&#125; \\--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-proxysystemctl restart kube-proxy 2、启动kubelet123[root@k8s-node-91 ~]# chmod +x proxy.sh [root@k8s-node-91 ~]# ./proxy.sh 10.0.10.91Created symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service. 3、查看启动状态12$ ps -ef|grep kube-proxy$ systemctl status kube-proxy 上面只添加了k8s-node-91这个节点，我这里还有k8s-node-92第二个节点。如果你有多个节点，按照上面的方法继续添加即可 十、部署Web UI（Dashboard）部署UI这部分网上有很多方法，如果下文有你看不懂的地方，可以不用参考我这里的方案 在kubernetes源码里有关于Dashboard的部署文件：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard 文件的细节这里不展开解释，需要具备一定阅读能力。 10.1、YAML文件dashboard-rbac.yaml123456789101112131415161718192021222324252627[root@k8s-master-90 ui]# cat dashboard-rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kubernetes-dashboard-minimal namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system dashboard-deployment.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master-90 ui]# cat dashboard-deployment.yaml apiVersion: apps/v1beta2kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: kubernetes-dashboard containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.7.1 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 100Mi ports: - containerPort: 9090 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" dashboard-service.yaml1234567891011121314151617[root@k8s-master-90 ui]# cat dashboard-service.yaml apiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090 部署k8s Web UI 需要用到上面的三个YAML文件。 10.3、执行部署123# kubectl delete -f dashboard-deployment.yaml# kubectl delete -f dashboard-rbac.yaml # kubectl delete -f dashboard-service.yaml 查看部署情况12345678[root@k8s-master-90 ui]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.10.10.149 &lt;none&gt; 80:38276/TCP 19s # 对外暴露了38276端口，这是能访问UI界面的端口[root@k8s-master-90 ui]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkubernetes-dashboard-698bb888c5-cbt5q 1/1 Running 0 33s # 能看到pods已经运行[root@k8s-master-90 ui]# kubectl describe pods/kubernetes-dashboard-698bb888c5-cbt5q -n kube-system # 这个命令能查看pods创建过程信息 10.3、UI界面http://Node节点IP:38276 （节点IP之间，能负载均衡，所有随便访问哪个节点IP+端口 都能访问到UI）http://10.0.10.92:38276 10.4、启动一个测试示例1234# kubectl run nginx --image=nginx --replicas=3# kubectl get pod# kubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort# kubectl get svc nginx 访问示例1234[root@k8s-master-90 ui]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.10.10.1 &lt;none&gt; 443/TCP 1dnginx NodePort 10.10.10.61 &lt;none&gt; 88:37224/TCP 1m 我们部署了nginx这个service，集群IP是10.10.10.61，暴露的端口是88。也就是说在k8s集群里，我们可以通过如下方式来访问1234567891011121314151617181920212223242526[root@k8s-node-91 ~]# curl 10.10.10.61:88&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 如果需要在集群外部访问的话，那访问pod的端口也就是37224即可。 这篇部署有点长，就先在这里结尾吧，前面有些技术细节并没有写清楚，如果你有恰好到此文章，有看不懂的地方欢迎与我探讨，QQ：1432753451，Nicksors。","tags":[{"name":"k8s集群部署","slug":"k8s集群部署","permalink":"//nicksors.cc/tags/k8s集群部署/"}]},{"title":"私人定制yilia主题的代码高亮风格","date":"2018-05-18T09:04:06.000Z","path":"2018/05/18/私人定制yilia主题的代码高亮风格.html","text":"前言:yilia主题默认的代码高亮风格个人不是太喜欢，就想着怎么样能改变下？于是就baidu、google了解了下！然而发现很多解决方案大都是两年前的，根本没法用。于是着手自己看源码，不就是改改样式么？哼，我还不行搞不定了！ 在哪里更改？其实主题说的私人定制，其实就是更改yilia默认的代码块方案，改成你想要的即可。 截止目前(20180518)，更改的样式文件是:themes/yilia/source/main.0cf68a.css为什么要说个时间呢？因为如果在将来，很可能这个文件或目录会变化，只是提醒你注意下。 其实还有个文件，在themes/yilia/source-src/css/highlight.scss，这儿可以更改highlight样式。但据我测试，更改这里不生效，因为这是src源文件，我们得更改main.0cf68a.css这个压缩文件里的内容才能生效。 我更改了什么？首先，我必须把highlight.scss这个源文件列出来，因为我是照着这里面的关键字提示去更改的，下面我注解下我更改过的关键字。tips: 更改好的代码块如下，如果你喜欢的话，就用吧~！123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162 $ cat themes/yilia/source-src/css/highlight.scss.article-entry pre,.article-entry .highlight &#123; background: #f4f6f9; &lt;== 这里是默认的那个黑黑的背景，我改成了GitHub的代码块背景颜色 margin: 10px 0; padding: 10px 10px; overflow: auto; color: #1c1f23; &lt;==这里是字体的颜色，默认字体颜色是白色的，我更改成了黑色 font-size: 0.9em; line-height: 22.400000000000002px;&#125;.article-entry .highlight .gutter pre,.article-entry .highlight .gutter pre .line,.article-entry .gist .gist-file .gist-data .line-numbers &#123; color: #666;&#125;.article-entry pre,.article-entry code &#123; font-family: \"Source Code Pro\", Consolas, Monaco, Menlo, Consolas, monospace;&#125;.article-entry code &#123; background: #eee; padding: 0 0.3em; border: none;&#125;.article-entry pre code &#123; background: none; text-shadow: none; padding: 0; color: #fff;&#125;.article-entry .highlight &#123; border-radius: 4px;&#125;.article-entry .highlight pre &#123; border: none; margin: 0; padding: 0;&#125;.article-entry .highlight table &#123; margin: 0; width: auto;&#125;.article-entry .highlight td &#123; border: none; padding: 0;&#125;.article-entry .highlight figcaption &#123; color: highlight-comment; line-height: 1em; margin-bottom: 1em;&#125;.article-entry .highlight figcaption:before,.article-entry .highlight figcaption:after &#123; content: \"\"; display: table;&#125;.article-entry .highlight figcaption:after &#123; clear: both;&#125;.article-entry .highlight figcaption a &#123; float: right;&#125;.article-entry .highlight .gutter pre &#123; text-align: right; padding-right: 20px;&#125;.article-entry .highlight .gutter pre .line &#123; text-shadow: none;&#125;.article-entry .highlight .line &#123; color: #fff; min-height: 19px;&#125;.article-entry .gist &#123; margin: 0 -20px; border-style: solid; border-color: #ddd; border-width: 1px 0; background: #f4f6f9; padding: 15px 20px 15px 0;&#125;.article-entry .gist .gist-file &#123; border: none; font-family: \"Source Code Pro\", Consolas, Monaco, Menlo, Consolas, monospace; margin: 0;&#125;.article-entry .gist .gist-file .gist-data &#123; background: none; border: none;&#125;.article-entry .gist .gist-file .gist-data .line-numbers &#123; background: none; border: none; padding: 0 20px 0 0;&#125;.article-entry .gist .gist-file .gist-data .line-data &#123; padding: 0 !important;&#125;.article-entry .gist .gist-file .highlight &#123; margin: 0; padding: 0; border: none;&#125;.article-entry .gist .gist-file .gist-meta &#123; background: #f4f6f9; color: highlight-comment; font: 0.85em \"Helvetica Neue\", Helvetica, Arial, sans-serif; text-shadow: none; &lt;==这里需要更改，是字体阴影设置参数，我不需要所有改掉了，默认：0 1px padding: 0; margin-top: 1em; margin-left: 20px;&#125;.article-entry .gist .gist-file .gist-meta a &#123; color: #258fb8; font-weight: normal;&#125;.article-entry .gist .gist-file .gist-meta a:hover &#123; text-decoration: underline;&#125;pre .comment &#123; color: #878874; # 注释，井号“#”后面的字体颜色，默认是：75715e, 我将颜色改的更浅了&#125;pre .keyword,pre .function .keyword,pre .class .params &#123; color: #66d9ef; &lt;==关键字颜色，如：log、sudo等系统预设的关键字，这里我保持默认&#125;pre .tag,pre .doctype,pre .params,pre .function,pre .css .value &#123; color: #fff;&#125;pre .css ~ * .tag,pre .title,pre .at_rule,pre .at_rule .keyword,pre .preprocessor,pre .preprocessor .keyword &#123; color: #f92672;&#125;pre .attribute,pre .built_in,pre .class,pre .css ~ * .class,pre .function .title &#123; color: #a6e22e;&#125;pre .value,pre .string &#123; color: #e6db74; &lt;== 用分号包裹起来的字体颜色，默认是：e6db74（黄色）&#125;pre .number &#123; color: #7163d7;&#125;pre .id,pre .css ~ * .id &#123; color: #fd971f;&#125; 罗列一下上面我做的更改项： background: #f4f6f9; # 代码块背景颜色 color: #1c1f23; # 代码块内主要字体颜色 color: #878874; # 井号注释的字体颜色 color: #e6db74; # 分号体里的字体颜色 text-shadow: none; #代码块字体阴影效果，我不需要，禁用之。 我已更改好的源文件前面已经说了，更改highlight.scss无效，我们必须更改main.0cf68a.css才行，那我就不费话了，直接上更改好的压缩文件，因为如果你闲麻烦，而且恰好喜欢我更改的方案，你就可以拿着这个源文件直接使用了。 点击下载：main.0cf68a.css点击下载: highlight.scss（备用，如果你替换main.0cf68a.css无效的话，将highlight.scss的内容也更改成与我相同.）tips：浏览器点击下载，会直接打开文件，你可以复制粘贴到本地也行。 操作方法1、备份：main.0cf68a.css main.0cf68a.css_bak;2、下载：wget https://nicksors.cc/download/main.0cf68a.css；3、替换你本地的themes/yilia/source/main.0cf68a.css文件；4、然后hexo g &amp;&amp; hexo d 即可完成华丽变身。 如果你不喜欢我更改的方案，有必要告诉你一下更改技巧： 首先你需要一个HTML拾色器，因为它能帮你找到想要的颜色； 上面罗列了四个颜色，其中背景颜色我在main.0cf68a.css文件里更改了三处才达到我想要的效果，而最不好更改的就是代码块里的主要字体颜色，我更改了起码有5、6处，到底改了哪5、6处呢？你可以下载我改过的源码文件看一下即知。 如果你想要更改字体颜色的话，可以执行命令：sed -i ‘s/1c1f23/你喜欢的颜色/g’ main.0cf68a.css，这个命令的作用是将1c1f23全部替换成”你喜欢的颜色”，不需要你一个一个的更改了。其他颜色也可以用这种方式更改哈。 私人定制方案到此为止，比较简单，但解决了我的问题。若你更改或使用遇到问题，欢迎issue.","tags":[{"name":"Heox","slug":"Heox","permalink":"//nicksors.cc/tags/Heox/"},{"name":"yilia","slug":"yilia","permalink":"//nicksors.cc/tags/yilia/"}]},{"title":"电影：爆裂鼓手","date":"2018-05-17T17:21:26.000Z","path":"2018/05/18/电影：爆裂鼓手.html","text":"http://www.le.com/ptv/vplay/21688582.html 20180518：凌晨01:04分 刚看完爆裂鼓手，看完最后一段后，我知道，又一个查理·帕克诞生了，从魔鬼导师的眼里能看出来。 此时的我眼里少许热泪，想到了自己人生里是否也能出现一位这样的魔鬼导师？压迫到自己喘不过气来。 魔鬼导师在与安德鲁对话中，导师说道一句话让我特别感触，他说：“我是去逼他们突破自己的极限的”。这句话特别触动了我。 非常希望在30岁之前能遇到我的魔鬼导师，能逼迫我挑战自己的极限。 在我成长的同时，我也需要学习魔鬼导师的精神，去找到属于我的查理·帕克！ 这个片子值得我过几年后再次看看，那时候我应该是热泪盈眶，到那时希望溜的不是泪水，而是故事。 同样适合我被上司，公司等对我严厉，严格的人事物们，特别是有种上班入上坟的感觉时，最应该看看这个片子，我应该会想通，过几年后我拿着高薪，生活过得美满时，应该感谢的人是谁！","tags":[{"name":"电影","slug":"电影","permalink":"//nicksors.cc/tags/电影/"}]},{"title":"Kubernetes是什么？","date":"2018-05-17T04:08:29.000Z","path":"2018/05/17/Kubernetes是什么？.html","text":"这是一篇从零开始了解Kubernetes的理论文章，从概念介绍到其关键技术再到架构设计，都能帮助我能更好的理解Kubernetes。 一、Kubernetes简介 Kubernetes是Google在2014年6月开园的一个容器集群管理系统，该系统使用Google研发的Golang语言开发。Kubernetes也被叫做k8s，因为k和s之前有8个字母。 k8s原本是Google内部一个叫Borg的容器集群管理系统衍生出来的，Borg已经在Google大规模生产运行十年之久。 k8s主要用于自动化部署、扩展和管理容器应用，它提供了资源调度、部署管理、服务发现、扩容缩容、监控等一整套功能。 Kubernetes的目标是让部署容器化应用简单高效。官网地址：https://www.kubernetes.io 二、Kubernetes主要功能2.1、数据卷在Pod中，容器之前共享数据，可以使用数据卷。这个功能与docker里volume功能相等。 2.2、应用程序健康检查检查容器内的服务、进程是否异常；可以设置监控检测策略来保证应用的健壮性。 2.3、复制应用程序实例控制维护Pod副本数，保证一个Pod或一组同类的Pod数始终可用。 2.4、弹性缩容根据设定的指标（CPU利用率）自动缩放Pod副本数。 2.5、服务发现使用环境变量或DOS服务插件保证容器中程序发现Pod入口访问地址。 2.6、负载均衡一组Pod副本分配一个私有的集群IP地址，负载均衡转发请求到后端容器。在集群内部，其他Pod可通过这个ClusterIP访问应用。 2.7、滚动更新更新服务不中断，异常更新一个Pod，而不是同时删除整个服务。 2.8、服务编排通过文件描述部署服务，使得应用程序部署变得更高效。 2.9、资源监控Node节点组件继承CAdvisor资源收集工具，可通过Heapster汇总整个集群节点资源数据，然后存储到InfluxDB时序数据库中，再由Grafana展示。 2.10、提供认证和授权支持角色访问控制（RBAC）认证授权等策略。 三、基本概念 在k8s中，它的概念非常的多！而且一定要去了解这些概念，知道它的作用是必须的，你需要知道这些概念，才能更好的应用它。 3.1、PodPod是k8s最小部署单元，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，在同一个Docker主机上运行。官网解读：https://www.kubernetes.org.cn/kubernetes-pod 3.2、ServiceService一个应用服务的抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。官网解读：https://www.kubernetes.org.cn/kubernetes-services 3.3、Volume数据卷，共享Pod中容器使用的数据。官网解读：https://www.kubernetes.org.cn/kubernetes-volumes 3.4、Namespace命名空间将对象逻辑上分配到不同Namespace，可以是不同的项目、用户等分区管理，并设定控制策略，从而实现多租户。官网解读：https://www.kubernetes.org.cn/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%EF%BC%9Anamespace 3.5、Lable标签用于区分对象（比如Pod、Service），键/值对存在；每个对象可以有多个标签，通过标签关联对象。官网解读：https://www.kubernetes.org.cn/kubernetes-labels 四、更高层次的抽象4.1、ReplicaSet（RS）ReplicaSet是下一代复本控制器。官网解读：https://www.kubernetes.org.cn/replicasets 4.2、DeploymentDeployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。官方建议使用Department管理ReplicaSets,而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象。 咱们在实际应用当中，基本上都是去创建一个Deployment，然后由Deployment去创建RS和Pod等。官网解读：https://www.kubernetes.org.cn/deployment 4.3、StatefulSetStatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。官网解读：https://www.kubernetes.org.cn/statefulset 4.4、DaemonSetDaemonSet确保所有（或一些）节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。官网解读：https://www.kubernetes.org.cn/daemonset 4.5、Job一次性任务，运行完成后Pod销毁，不在重新启动新容器。还可以任务定时运行。官网解读：https://www.kubernetes.org.cn/job 五、Kubernetes系统架构拓扑图 六、组件功能介绍6.1、Master组件 kube-apiserverKubernetes API，集群的统一入口，各组件协调者。以HTTP API提供接口服务，所有对象的增删改查和监听操作都交给API Server处理后再提交给Etcd存储。 kube-controller-master处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。 kube-scheduler根据调度算法为新创建的Pod选择一个Node节点。 6.2、Node组件 kubeletkubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。 kube-proxy在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。 docker或rocket/rktk8s可选的容器运行底层支持技术，一般我们都使用docker。 6.3、第三方服务 etcd分布式键值存储系统。用于保持集群状态，比如Pod、Service等对象信息。 本文完.","tags":[]},{"title":"使用Docker构建持续集成环境","date":"2018-05-16T06:02:21.000Z","path":"2018/05/16/使用Docker构建持续集成环境.html","text":"前言：本文将使用Docker+Git+Maven+Jenkins构建一套持续集成环境，可上线生产，本文内容请酌情参考. 0、服务器部署规划 服务名称 主机名 IP 开发机客户端 k8s-mster 1 72.16.194.128 测试服务器（Docker）、Jenkins服务器 k8s-node01 172.16.194.129 Git/Docker registry k8s-node02 172.16.194.130 ———-我是分割线——— 工具 版本 CentOS 7.2_x64 Maven 3.5 Tomcat 8 JDK 1.8 Jenkins 2.6 Docker CE 18.03 1、CI/CD 2、发布流程设计 3、部署Git服务器1.安装git1[root@k8s-node02 ~]# yum install git -y 2.创建用户12useradd gitpasswd git 3.创建仓库123su - gitmkdir app.gitgit --bare init app.git 4.克隆仓库在另外的节点上操作1234567891011121314[root@k8s-master ~]# yum install git -y[root@k8s-master ~]# git clone git@172.16.194.130:/home/git/app.git正克隆到 'app'...The authenticity of host '172.16.194.130 (172.16.194.130)' can't be established.ECDSA key fingerprint is 60:eb:ee:4d:7d:12:10:5d:49:0c:7a:f3:81:5d:1f:2b.Are you sure you want to continue connecting (yes/no)? yssWarning: Permanently added '172.16.194.130' (ECDSA) to the list of known hosts.git@172.16.194.130's password:warning: 您似乎克隆了一个空版本库。[root@k8s-master ~]# cd app/[root@k8s-master app]# ls -lh总用量 0[root@k8s-master app]# 5.生成秘钥对，配置免密clone12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:5a:f9:66:7a:53:5a:2c:f9:a4:bc:98:5e:29:7d:2d:6f root@k8s-masterThe key's randomart image is:+--[ RSA 2048]----+| || || || . || S o || o oo.=. || . ..BOo . || O*..oE || .=..o .. |+-----------------+[root@k8s-master ~]# ls -lh /root/.ssh/总用量 12K-rw------- 1 root root 1.7K 3月 29 18:46 id_rsa-rw-r--r-- 1 root root 397 3月 29 18:46 id_rsa.pub-rw-r--r-- 1 root root 176 3月 29 17:57 known_hosts[root@k8s-master ~]# ssh-copy-id git@172.16.194.130 /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysgit@172.16.194.130's password:Number of key(s) added: 1Now try logging into the machine, with: \"ssh 'git@172.16.194.130'\"and check to make sure that only the key(s) you wanted were added.[root@k8s-master ~]## 查看master节点cp过来的秘钥[root@k8s-node02 ~]# su - git上一次登录：四 3月 29 17:52:09 CST 2018pts/1 上[git@k8s-node02 ~]$ ls -lh .ssh/authorized_keys-rw-------. 1 git git 397 Mar 29 18:49 .ssh/authorized_keys[git@k8s-node02 ~]$ cat !$cat .ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHp/RZmgz5R5Nw1pQXyPjVynDu0kkn/3i05WR7CRNS31ylGkkI5k7AOmw8sEEDn3mAQg/z/xEI+LIoCRu68MESJEkqXX2oHzsEa6/JB6X3OW4VIwAkwD8W1FUppK1dueAmnZmx4k7ZiIgE43QWxcOuDthQ2mO3ybWjk13tR0viz4mrcuK/ulkW5AOlLP1I2A4NkTVmkXktk/CuTrULph/VXz10kbX+u0OPNxDLGzUdC94Issm4IPZRwYBhgYviCNEIxZW2pBrpnL3rXmVHHE4eeNGNJ0DnhaX32p4wYAKqujU1VK7udED2Af9rYgE9yArRfTc53di6OXTrlJ7polG/ root@k8s-master# 删除app项目后，重新clone就不需要密码了[root@k8s-master ~]# git clone git@172.16.194.130:/home/git/app.git正克隆到 'app'...warning: 您似乎克隆了一个空版本库。 6.克隆博客系统，提交到自建git仓库1.git clone tale1[root@k8s-master ~]# git clone https://github.com/otale/tale.git 2.提交到我们自己的git仓库里12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@k8s-master ~]# \\cp -a tale/* app/[root@k8s-master ~]# ls -lh app/总用量 24Kdrwxr-xr-x 2 root root 21 3月 29 18:45 bin-rw-r--r-- 1 root root 1.1K 3月 29 18:45 LICENSE-rw-r--r-- 1 root root 1.5K 3月 29 18:45 package.xml-rw-r--r-- 1 root root 7.1K 3月 29 18:45 pom.xmlf-rw-r--r-- 1 root root 2.8K 3月 29 18:45 README.md-rw-r--r-- 1 root root 2.7K 3月 29 18:45 README_ZH.mddrwxr-xr-x 3 root root 17 3月 29 18:45 src[root@k8s-master app]# git add .[root@k8s-master app]# git status[root@k8s-master app]# git config --global user.email \"you@example.com\" # 这两步需要执行下，内容随便，不然commit时会提示[root@k8s-master app]# git config --global user.name \"Your Name\"[root@k8s-master app]# git commit -m 'all'[root@k8s-master app]# git pushwarning: push.default 未设置，它的默认值将会在 Git 2.0 由 'matching'修改为 'simple'。若要不再显示本信息并在其默认值改变后维持当前使用习惯，进行如下设置： git config --global push.default matching若要不再显示本信息并从现在开始采用新的使用习惯，设置： git config --global push.default simple参见 'git help config' 并查找 'push.default' 以获取更多信息。（'simple' 模式由 Git 1.7.11 版本引入。如果您有时要使用老版本的 Git，为保持兼容，请用 'current' 代替 'simple' 模式）No refs in common and none specified; doing nothing.Perhaps you should specify a branch such as 'master'.fatal: The remote end hung up unexpectedlyerror: 无法推送一些引用到 'git@172.16.194.130:/home/git/app.git'[root@k8s-master app]# git push origin master # 需要提交到origin master 主干仓库Counting objects: 297, done.Delta compression using up to 4 threads.Compressing objects: 100% (255/255), done.Writing objects: 100% (297/297), 6.35 MiB | 0 bytes/s, done.Total 297 (delta 22), reused 0 (delta 0)To git@172.16.194.130:/home/git/app.git* [new branch] master -&gt; master 4、Docker私有镜像仓库搭建1.创建私有仓库容器123# 在node2上运行仓库容器[root@k8s-node02 ~]# docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registryb45a684be68fbecebe6c433e38eb98ca805fe3b45e44594057cbb3c5bf99bb13 2.构建tale开源博客基础镜像12345678910111213141516171819202122232425262728293031mkdir tale &amp;&amp; cd talecat &gt; Dockerfile&lt;&lt;EOFFROM centos:7RUN yum install epel-release -yRUN yum install nginx supervisor -y &amp;&amp; yum clean allRUN sed -i '47a proxy_pass http://127.0.0.1:9000;' /etc/nginx/nginx.confCOPY supervisord.conf /etc/supervisord.confENV PATH /usr/local/jdk/bin/:$PATHWORKDIR /taleCMD [\"/usr/bin/supervisord\"]EOFcat &gt;supervisord.conf&lt;&lt;EOF[supervisord]nodaemon=true[program:tale]command=/usr/local/jdk/bin/java -jar /tale/tale-least.jarautostart=trueautorestart=true[program:nginx]command=/usr/sbin/nginx -g \"daemon off;\"autostart=trueautorestart=trueEOF[root@k8s-node02 tale]# docker build -t tale:v1 .[root@k8s-node02 tale]# docker images|grep taletale v1 cd96b2fd2d6b 41 seconds ago 372MB 3.提交tale基础镜像到私有仓库123456# 这一步尤为重要，因为后面Jenkins会来拉取这个镜像进行应用部署docker tag tale:v1 172.16.194.130:5000/tale:v1docker push 172.16.194.130:5000/tale:v1# 查看提交的镜像版本信息curl http://172.16.194.130:5000/v2/tale/tags/list 5、测试服务器安装Docker1.安装docker123456789yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce 2.配置官方国内镜像源、添加私有仓库可信任12345678[root@k8s-node02 ~]# cat &gt;/etc/docker/daemon.json &lt;&lt;EOF&#123; \"registry-mirrors\": [ \"https://registry.docker-cn.com\"] \"insecure-registries\":[\"172.16.194.130:5000\"]&#125;EOF[root@k8s-node01 ~]# systemctl restart docker 6、Jenkins安装1.编译镜像12345678910111213141516[root@k8s-node01 ~]# mkdir jenkins[root@k8s-node01~]# cd jenkins/[root@k8s-node01 jenkins]# cat DockerfileFROM jenkinsUSER rootRUN echo '' &gt; /etc/apt/sources.list.d/jessie-backports.list &amp;&amp; \\wget http://mirrors.163.com/.help/sources.list.jessie -O /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y git[root@k8s-node01 jenkins]# docker build -t jenkins:v1 .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM jenkins···中间略···---&gt; 8711202c6fdaSuccessfully built 8711202c6fdaSuccessfully tagged jenkins:v1 2.准备应用包在k8s-node01的宿主机上准备以下两个包，事先放在下面位置：12/usr/local/apache-maven-3.5.0/usr/local/jdk1.8.0_45 3. 运行Jenkins容器12345678docker run -d \\--name jenkins \\-p 8080:8080 \\-v /var/jenkins_home/:/var/jenkins_home \\-v /usr/local/apache-maven-3.5.0:/usr/local/maven \\-v /usr/local/jdk1.8.0_45:/usr/local/jdk \\-v ~/.ssh:/root/.ssh \\jenkins:v1 访问：http://172.16.194.129:8080/ 7、Jenkins基本配置 1.系统ssh连接配置点击：系统管理–&gt;系统管理（将页面拉到最后，配置ssh连接） 测试配置是否正常：Success，最后保存即可。 2.全局工具配置点击：系统管理–&gt; Global Tool Configuration 1.配置JDK环境2.git 默认即可3.配置maven 配置好了就保存。 8、Jenkins创建项目1.创建项目步骤新建–&gt;构建一个maven项目–&gt;点击ok 2.项目的一些配置2.1 配置git这里的git仓库配置，就是我们之前拉取的那个地址，分支指定master 2.2 构建触发器构建触发器，定制每小时出发一次，发现代码仓库有变化，就进行构建动作 2.3构建之前执行的动作123# 如果多次构建的话，需要清空上次构建的环境docker rm -f tale-test &amp;&amp; \\docker image rm 172.16.194.130:5000/tale:base 2.3构建之后执行的动作1234567# 构建好后，将环境使用docker运行到一个容器里，接下来就是测试啦！docker run -itd \\--name tale-test \\-p 88:80 \\-v /usr/local/jdk1.8.0_45:/usr/local/jdk \\-v /data/tale:/tale \\ # 这个构建后的Command一定是能在服务器上手动执行成功的，在这里才能执行成功。172.16.194.130:5000/tale:base 配置好后，应用–&gt;保存. 9、测试1、这是构建的结果12345678910111213141516# 第一次构建会比较慢，因为需要下载maven相关插件Started by user nicksors[EnvInject] - Loading node environment variables.Building in workspace /var/jenkins_home/workspace/tale-test &gt; git rev-parse --is-inside-work-tree # timeout=10···中间略···Digest: sha256:f82f2e175479d6d232efab45f81a4495cc4ad0a48135fd839dc27fdee8c13c77Status: Downloaded newer image for 172.16.194.130:5000/tale:baseccc80d6215724027d9111069b5c8afa48c0b9df5694386656d611e19796b34b3SSH: EXEC: completed after 21,474 msSSH: Disconnecting configuration [172.16.194.129] ...SSH: Transferred 183 file(s)Finished: SUCCESS 2、查看构建的docker运行情况运行良好，访问：http://172.16.194.129:88/12345[root@k8s-node01 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESccc80d621572 172.16.194.130:5000/tale:base \"/usr/bin/supervisord\" 11 minutes ago Up 11 minutes 0.0.0.0:88-&gt;80/tcp tale-testaa80a34ec7b6 jenkins:v1 \"/bin/tini -- /usr/l…\" 20 hours ago Up 4 hours 0.0.0.0:8080-&gt;8080/tcp, 50000/tcp jenkins[root@k8s-node01 ~]# 3、tale博客运行起来啦！第一次访问会让你设置一下站点信息设置账号密码：nicksors/abc123!! 博客页面： 后台： CI/CD：持续集成/持续部署 的集成环境搞定~！","tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"//nicksors.cc/tags/CI-CD/"},{"name":"持续集成","slug":"持续集成","permalink":"//nicksors.cc/tags/持续集成/"}]},{"title":"使用Compose打包部署Docker项目","date":"2018-05-15T07:02:30.000Z","path":"2018/05/15/使用Compose打包部署Docker项目.html","text":"前言：Docker Compose 是一个编排多容器分布式部署的工具，提供命令集管理容器化应用的完整开发周期，包括服务构建，启动和停止。一些学习资料:Docker Compose 配置文件详解:https://www.jianshu.com/p/2217cfed29d7Docker Compose 项目：https://yeasy.gitbooks.io/docker_practice/content/compose/ 一、Compose简介Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 二、安装与卸载官网：https://docs.docker.com/compose/install/ 二进制包：1234# sudo curl -L https://github.com/docker/compose/releases/download/1.20.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose# sudo chmod +x /usr/local/bin/docker-compose[root@k8s-node02 ~]# docker-compose --versiondocker-compose version 1.20.1, build 5d8c71b PIP安装：1pip install -U docker-compose 卸载： 如果是二进制包方式安装的，删除二进制文件即可。1$ sudo rm /usr/local/bin/docker-compose 如果是通过 pip 安装的，则执行如下命令即可删除。1$ sudo pip uninstall docker-compose 三、使用首先介绍几个术语。 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。 可见，一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。 案例一：构建web网站最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。 下面我们用 Python 来建立一个能够记录页面访问次数的 web 网站。 web应用 新建文件夹，在该目录中编写 app.py 文件12345678910111213141516[root@k8s-node02 ~]# mkdir web_server[root@k8s-node02 ~]# cd web_server/[root@k8s-node02 web_server]#vim app.pyfrom flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host='redis', port=6379)@app.route('/')def hello(): count = redis.incr('hits') return 'Hello World! 该页面已被访问 &#123;&#125; 次。\\n'.format(count)if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", debug=True) Dockerfile 编写Dockerfile文件，内容为123456[root@k8s-node02 web_server]# cat DockerfileFROM python:3.6-alpineADD . /codeWORKDIR /codeRUN pip install redis flaskCMD [\"python\", \"app.py\"] docker-compase.yml编写 docker-compose.yml 文件，这个是 Compose 使用的主模板文件。1234567891011[root@k8s-node02 web_server]# cat docker-compose.ymlversion: '3'services: web: build: . ports: - \"5000:5000\" redis: image: \"redis:alpine\" 运行 compose 项目12345678910111213141516171819[root@k8s-node02 web_server]# docker-compose upBuilding webStep 1/5 : FROM python:3.6-alpine3.6-alpine: Pulling from library/python81033e7c1d6a: Already exists9b61101706a6: Pull complete35b21c1a8b97: Pull complete4856f5aeeec4: Pull complete84607ac623a4: Pull completeDigest: sha256:e10e26000b4dcfb66c52c11a6a7cc5251f6a95f9512fa9228bb3a66efc6c7075Status: Downloaded newer image for python:3.6-alpine---&gt; 4fcaf5fb5f2bStep 2/5 : ADD . /code---&gt; 061db7e464f1Step 3/5 : WORKDIR /codeRemoving intermediate container 7986a3cb8c7e---&gt; 92a3ea98dfffStep 4/5 : RUN pip install redis flask---&gt; Running in f0bed49a550d 此时访问本地 5000 端口，每次刷新页面，计数就会加 1。 注意事项 使用-d参数放入后台运行 docker-compose ps必须是在当前项目下执行，否则报错 1234567891011121314151617181920[root@k8s-node02 web_server]# docker-compose up -dStarting webserver_redis_1 ... doneStarting webserver_web_1 ... done[root@k8s-node02 web_server]#[root@k8s-node02 web_server]# docker-compose ps Name Command State Ports-----------------------------------------------------------------------------------webserver_redis_1 docker-entrypoint.sh redis ... Up 6379/tcpwebserver_web_1 python app.py Up 0.0.0.0:5000-&gt;5000/tcp[root@k8s-node02 web_server]#[root@k8s-node02 web_server]# cd[root@k8s-node02 ~]# docker-compose ps &lt;==找不到项目目录ERROR: Can't find a suitable configuration file in this directory or any parent. Are you in the right directory? Supported filenames: docker-compose.yml, docker-compose.yaml[root@k8s-node02 ~]# 案例二：一键部署LNMP网站平台项目结构12345678910111213141516171819[root@k8s-node02 compose_lnmp]# tree.├── docker-compose.yml # lnmp项目的yaml文件，默认文件名：docker-compose.yml，可更改├── mysql # 数据库目录│ ├── conf # 数据库配置目录│ │ └── my.cnf # 数据库配置文件│ └── data # 存储数据的目录（mysql的这些配置，是在容器起来所需要的东西）├── nginx # Nginx目录│ ├── Dockerfile # 会通过Dockerfile去构建Nginx│ ├── nginx-1.12.1.tar.gz│ └── nginx.conf├── php # PHP目录│ ├── Dockerfile # php 的Dockerfile文件│ ├── php-5.6.31.tar.gz│ └── php.ini # 配置文件└── wwwroot # 网站根目录 └── index.php6 directories, 9 files docker-compose.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-node02 compose_lnmp]# cat docker-compose.ymlversion: '3' # 版本，这里是指定compose的版本，compose有三个版本，分别为1、2、3；services: # 这里是一个顶级标签，这个是服务标签，下面定义了nginx、php、mysql等服务，当然今后你有多少个就写多少个。 nginx: # 具体服务名称 hostname: nginx # 服务的主机名 build: # 编译，同等于docker build Dockerfile ... context: ./nginx # 这里是指定Dockerfile的位置，这个位置是相对于docker-compose.yml所在位置而定的 dockerfile: Dockerfile # Dockerfile文件名称 ports: # 指定暴露的端口 - 81:80 networks: # 指定网络 - lnmp # 这里是具体的网络名称 volumes: # 映射的数据卷 - ./wwwroot:/usr/local/nginx/html php: hostname: php build: context: ./php dockerfile: Dockerfile networks: # 指定网络，要与Nginx、mysq同一个网络 - lnmp volumes: - ./wwwroot:/usr/local/nginx/html mysql: hostname: mysql image: mysql:5.6 # 要拉取的mysql容器镜像 ports: # 暴露的数据库端口 - 3306:3306 networks: # 指定到同一个网络 - lnmp volumes: # 映射数据卷 - ./mysql/conf:/etc/mysql/conf.d # ./mysql/conf是当前的目录，这里面有我们配置好的my.cnf配置文件 - ./mysql/data:/var/lib/mysql # ./mysql/data 这也是之前创建好的数据库data目录，用于存储mysql数据 command: --character-set-server=utf8 # 上面指定的mysql:5.6镜像在run的时候会接收到这个命令，并执行 environment: # 其他设置，这块属于数据库容器里的设置了，用户名、密码、要创建的数据库 MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: wordpress MYSQL_USER: user MYSQL_PASSWORD: user123networks: # 这里是一个顶级标签，创建网络（compose会先执行顶级标签，然后在执行后面的，这里的体现就是会先创建网络，再安装Nginx、php、mysql） lnmp: # 指定要创建的网络名称 nginx:Dockerfile123456789101112131415161718[root@k8s-node02 compose_lnmp]# cat nginx/Dockerfile # 通过Dockerfile编译一个Nginx容器FROM centos:7 # 选择一个基础镜像MAINTAINER www.xxx.com # 作者名称 RUN yum install -y gcc gcc-c++ make openssl-devel pcre-devel # 初始安装包ADD nginx-1.12.1.tar.gz /tmp # 将本地的Nginx软件添加到容器的/tmp目录下RUN cd /tmp/nginx-1.12.1 &amp;&amp; \\ # 运行命令，解压和执行编译安装 ./configure --prefix=/usr/local/nginx &amp;&amp; \\ make -j 2 &amp;&amp; \\ make installRUN rm -rf /tmp/nginx-1.12.1* &amp;&amp; yum clean all # 清除一些不需要的东东COPY nginx.conf /usr/local/nginx/conf # 然后再拷贝我们准备好的Nginx配置文件WORKDIR /usr/local/nginx # 设置工作目录EXPOSE 80 # 开放80端口，注意这里是开放，是运行这个容器时会打开80端口，不是对外暴露端口CMD [\"./sbin/nginx\", \"-g\", \"daemon off;\"] #运行容器时所需要执行的命令 php:Dockerfile1234567891011121314151617181920212223242526# 上面说过的选项，这里就不在赘述了哈！[root@k8s-node02 compose_lnmp]# cat php/DockerfileFROM centos:7MAINTAINER www.xxx.comRUN yum install -y gcc gcc-c++ make gd-devel libxml2-devel libcurl-devel libjpeg-devel libpng-devel openssl-develADD php-5.6.31.tar.gz /tmp/RUN cd /tmp/php-5.6.31 &amp;&amp; \\ ./configure --prefix=/usr/local/php \\ --with-config-file-path=/usr/local/php/etc \\ --with-mysql --with-mysqli \\ --with-openssl --with-zlib --with-curl --with-gd \\ --with-jpeg-dir --with-png-dir --with-iconv \\ --enable-fpm --enable-zip --enable-mbstring &amp;&amp; \\ make -j 4 &amp;&amp; \\ make install &amp;&amp; \\ cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf &amp;&amp; \\ sed -i \"s/127.0.0.1/0.0.0.0/\" /usr/local/php/etc/php-fpm.conf &amp;&amp; \\ sed -i \"21a \\daemonize = no\" /usr/local/php/etc/php-fpm.confCOPY php.ini /usr/local/php/etcRUN rm -rf /tmp/php-5.6.31* &amp;&amp; yum clean allWORKDIR /usr/local/phpEXPOSE 9000CMD [\"./sbin/php-fpm\", \"-c\", \"/usr/local/php/etc/php-fpm.conf\"] 一键部署12345678[root@k8s-node02 compose_lnmp]# docker-compose up -d···一个漫长的编译、安装、拉取镜像过程···[root@k8s-node02 compose_lnmp]# docker-compose ps Name Command State Ports-------------------------------------------------------------------------------------composelnmp_mysql_1 docker-entrypoint.sh --cha ... Up 0.0.0.0:3306-&gt;3306/tcpcomposelnmp_nginx_1 ./sbin/nginx -g daemon off; Up 0.0.0.0:81-&gt;80/tcpcomposelnmp_php_1 ./sbin/php-fpm -c /usr/loc ... Up 9000/tcp 将WordPress下载到站点解压并安装，最终运行结果如下： 四、应用场景 互联网产品交付客户 将产品封装成docker compose文件项目文件，打包交付给客户；省却手动部署环境的麻烦，高效精简整个交付过程。 特制的环境应用 比如咱们经常部署的lnmp环境，那这种环境不希望重复部署，就可以编写好yaml文件，通过compase 来管理整个环境。","tags":[{"name":"Compose","slug":"Compose","permalink":"//nicksors.cc/tags/Compose/"}]},{"title":"使用cAdvisor+InfluxDB+Grafana构建容器监控系统","date":"2018-05-14T07:08:37.000Z","path":"2018/05/14/使用cAdvisor+InfluxDB+Grafana构建容器监控系统.html","text":"概述： cadvisor：数据采集 Influxdb：数据存储 grafana：数据展示 三个都直接下载镜像启动容器即可。 一、启动三个应用容器123456789101112131415161718192021222324252627282930313233343536# 先创建一个专属于他们几个小基友的网络docker network create monitor#InfluxDBdocker run -d \\--name=influxdb \\--net monitor \\-p 8083:8083 \\-p 8086:8086 \\tutum/influxdb# cAdvisordocker run -d \\--name=cadvisor \\--net monitor \\-p 8081:8080 \\--mount type=bind,src=/,dst=/rootfs,ro \\--mount type=bind,src=/var/run,dst=/var/run \\--mount type=bind,src=/sys,dst=/sys,ro \\--mount type=bind,src=/var/lib/docker,dst=/var/lib/docker,ro \\google/cadvisor \\-storage_driver=influxdb \\-storage_driver_db=cadvisor \\-storage_driver_host=influxdb:8086#Grafanadocker run -d \\--name=grafana \\--net monitor \\-p 3000:3000 \\grafana/grafana# 如果想让容器异常退出后重启，你需要加上下面的参数--restart=always 二、各个应用访问界面Grafana： http://172.16.194.130:3000/login 用户名和密码默认：admin InfluxDB： http://172.16.194.130:8083/ cAdvisor：http://172.16.194.130:8081/ 三、配置3.1、在influxdb里创建数据库，用于cadvisor存储数据CREATE DATABASE “cadvisor” 3.2、influxdb里创建一个用户，用于grafana连接influxdb数据库CREATE USER “grafana” WITH PASSWORD ‘grafana’ 3.3、grafana添加数据源 登录grafana –&gt;点击Add data source Type：选择influxdbURL：influxdb （monitor 网络互连，所以直接使用name名称即可）database: cadvisor 上面创建好的，账号密码也是填写之前创建好的 3.4、grafana创建Dashboard 添加过程及细节，自己研究，这里不做输出 最终添加的结果，这里只是添加了两个示例 那么，“容器监控系统” 解决方案在这里就结束了，期待更多的玩法ing.","tags":[{"name":"cAdvisor","slug":"cAdvisor","permalink":"//nicksors.cc/tags/cAdvisor/"},{"name":"InfluxDB","slug":"InfluxDB","permalink":"//nicksors.cc/tags/InfluxDB/"},{"name":"Grafana","slug":"Grafana","permalink":"//nicksors.cc/tags/Grafana/"}]},{"title":"Docker图形界面管理Portainer","date":"2018-05-14T06:48:38.000Z","path":"2018/05/14/Docker图形界面管理Portainer.html","text":"官网：https://portainer.io/ 一、创建卷1docker volume create portainer_data 二、创建portainer容器 (默认可以自动创建卷)123456# docker run -d \\--name portainer \\-p 9000:9000 \\-v /var/run/docker.sock:/var/run/docker.sock \\-v portainer_data:/data \\portainer/portainer 三、配置portainerhttp://172.16.194.130:9000/ 用户名默认：admin需要设置密码，输入：abc123!! 选择本地或远程连接，我这里选择本地，点击Connect 这是整个管理页面的全貌，自己研究玩吧~","tags":[{"name":"portainer","slug":"portainer","permalink":"//nicksors.cc/tags/portainer/"}]},{"title":"搭建Hexo博客所用到的技术","date":"2018-05-10T08:43:55.000Z","path":"2018/05/10/搭建Hexo所用到的技术.html","text":"前言：这里记录着我搭建博客时用到了以下技术，基本上是参考各位博友写的博文。为了防止哪天我的博客崩掉后，我还能想起来如何重建它。 Mac上hexo博客的搭建 hexo使用yilia主题 Gitment：使用 GitHub Issues 搭建评论系统 添加gitment评论系统 添加Gitment评论系统踩过的坑（这些坑我全部踩了一遍…） 遇到的坑：点击login会url链接报错 hexo yilia主题添加文章访问量统计 百度、谷歌统计配置 网站访问量显示 Hexo七牛插件安装与使用 使用七牛为Hexo存储图片等资源 在Hexo中自动为Yilia主题增加版权声明 以上，非常感谢各位博友的辛苦总结.","tags":[{"name":"Heox","slug":"Heox","permalink":"//nicksors.cc/tags/Heox/"}]},{"title":"Docker企业级私有仓库方案(harbor)","date":"2018-05-10T06:37:16.000Z","path":"2018/05/10/Docker企业级私有仓库方案(harbor).html","text":"前言： Docker Hub作为Docker默认官方公共镜像，如果自己想搭建私有镜像仓库，官网也提供registry镜像，使得搭建私有仓库非常简单。 一、搭建私有镜像仓库:registry1.1、下载registry镜像并启动12$ docker pull registry$ docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry 1.2、测试，查看镜像仓库中所有镜像12[root@k8s-node02 ~]# curl http://127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125; 1.3、私有镜像仓库管理1）配置私有仓库可信任1234567891011$ vim /etc/docker/daemon.json&#123;\"insecure-registries\":[\"127.0.0.1:5000\"]&#125;systemctl restart docker这里需要注意一点，如果daemon.json里已经有配置，\"需要在原有配置后面加逗号，不然失效\" 譬如：[root@k8s-node02 ~]# cat /etc/docker/daemon.json&#123; \"registry-mirrors\": [ \"https://registry.docker-cn.com\"], \"insecure-registries\": [\"172.16.194.130:5000\"]&#125; 2）打标签1$ docker tag centos:6 127.0.0.1:5000/centos:7 3）上传1$ docker push 127.0.0.1:5000/centos:7 4）下载1$ docker pull 127.0.0.1:5000/centos:7 5）列出镜像标签12[root@k8s-node02 ~]# curl http://127.0.0.1:5000/v2/centos/tags/list&#123;\"name\":\"centos\",\"tags\":[\"7\"]&#125; 注意：127.0.0.1可以换成你网卡的地址 二、Docker Hub公共镜像仓库使用因为dockerHub是国外的服务器，push和pull操作都比较慢，甚至有连接超时的情况，个人研究玩玩还是可以的。https://cloud.docker.com/swarm/nicksors 1）注册帐号https://hub.docker.com 2）登录Docker Hub12345$ docker login或[root@k8s-node02 ~]# docker login --username=nicksors --password=与GitHub密码一样WARNING! Using --password via the CLI is insecure. Use --password-stdin.Login Succeeded 3）镜像打标签1$ docker tag nginx:1.12 nicksors/nginx:v2 4）上传1$ docker push nicksors/nginx:v2 5）下载1$ docker pull nicksors/nginx:v2 三、基于Harbor搭建Docker私有镜像仓库（推荐:很多企业都用这个）3.1、什么是Harbor？Harbor是VMware开源的又一个Docker Registry企业级私有仓库，其项目地址为https://github.com/vmware/harbor；相比Docker公司自己提供的Registry私有镜像仓库而言，Harbor提供了更多的功能，如下： 基于角色的访问控制 - 用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 镜像复制 - 镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。 图形化用户界面 - 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。 AD/LDAP 支持 - Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。 审计管理 - 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。 国际化 - 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。 RESTful API - RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。 部署简单 - 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。 以上来自官网介绍：https://vmware.github.io/harbor/cn/ 3.2、准备环境 自己创建的虚拟机：CentOS7.2、配置是2G2C； Docker版本：Docker version 18.03.0-ce Docker-compose：docker-compose version 1.20.1 Harbor版本：harbor-offline-installer-v1.4.0.tgz 3.3、安装Harbor在安装Harbor之前，必须保证你的环境已经安装好docker和docker-compose了,这两个安装方法在Docker官网都有：12安装Docker方法：https://docs.docker.com/install/linux/docker-ce/centos安装Docker-Compose方法：https://docs.docker.com/compose/install/#install-compose 你可以在 Harbor版本https://github.com/vmware/harbor/releases 地址下载你想要装的版本，这里我选择最新的1.4.0，当然你看到的时候已经不是最新版本了。123# 选择离线安装版本$ wget https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-offline-installer-v1.4.0.tgz（如果下载慢的话，你可以使用迅雷下载，有的网友就这么干，会快很多） 解压下载的包，进入解压后的harbor目录，里面有个harbor.cfg就是配置文件啦，简单说下：这里面可以配置LDAP，数据库，邮件信息，ssl证书等。12345$ vim harbor.cfg# 我配置了两个地方，主机名和Harbor admin的密码，其他默认hostname = 172.16.194.130harbor_admin_password = abc123!! 下面奉上一份harbor.cfg的关键参数说明：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145## Configuration file of Harbor#hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhosthostname = 172.16.194.130 #这里我使用本机IP# 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置onui_url_protocol = http#Maximum number of job workers in job servicemax_job_workers = 3#Determine whether or not to generate certificate for the registry's token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer's cert.customize_crt = on# 指定的证书文件，生产环境一定要使用ssl证书ssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.key# 存放证书的路径,这个路径会挂载到宿主机的/data/目录下secretkey_path = /data#Admiral's url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.log_rotate_count = 50#Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.#If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G#are all valid.log_rotate_size = 200M#************************BEGIN INITIAL PROPERTIES************************# 配置邮件server信息email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = falseemail_insecure = false# 启动Harbor后，管理员UI登录的密码，默认是Harbor12345harbor_admin_password = abc123!!# 认证方式，这里支持多种认证方式，如LADP、本次存储、数据库认证。默认是db_auth，mysql数据库认证auth_mode = db_auth# ldap配置ldap_url = ldaps://ldap.mydomain.com#A user's DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/ADldap_uid = uid#the scope to search for users, 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREEldap_scope = 2#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5#Verify certificate from LDAP serverldap_verify_cert = true#Turn on or off the self-registration featureself_registration = on#The expiration time (in minute) of token created by token service, default is 30 minutestoken_expiration = 30# 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）project_creation_restriction = everyone#************************END INITIAL PROPERTIES************************#######Harbor DB configuration section########The address of the Harbor database. Only need to change when using external db.db_host = mysql#The password for the root user of Harbor DB. Change this before any production use.db_password = root123#The port of Harbor database hostdb_port = 3306#The user name of Harbor databasedb_user = root##### End of Harbor DB configuration########The redis server address. Only needed in HA installation.redis_url =##########Clair DB configuration#############Clair DB host address. Only change it when using an exteral DB.clair_db_host = postgres#The password of the Clair's postgres database. Only effective when Harbor is deployed with Clair.#Please update it before deployment. Subsequent update will cause Clair's API server and Harbor unable to access Clair's database.clair_db_password = password#Clair DB connect portclair_db_port = 5432#Clair DB usernameclair_db_username = postgres#Clair default databaseclair_db = postgres##########End of Clair DB configuration#############The following attributes only need to be set when auth mode is uaa_authuaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pem### Docker Registry setting ####registry_storage_provider can be: filesystem, s3, gcs, azure, etc.registry_storage_provider_name = filesystem#registry_storage_provider_config is a comma separated \"key: value\" pairs, e.g. \"key1: value, key2: value2\".#Refer to https://docs.docker.com/registry/configuration/#storage for all available configuration.registry_storage_provider_config = 3.4、启动Harbor修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当期目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动各个服务。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889$ ./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.03.0Note: docker-compose version: 1.20.1[Step 1]: loading Harbor images ...Loaded image: vmware/notary-server-photon:v0.5.1-v1.4.0Loaded image: vmware/notary-signer-photon:v0.5.1-v1.4.0Loaded image: vmware/harbor-db:v1.4.0Loaded image: vmware/clair-photon:v2.0.1-v1.4.0Loaded image: vmware/postgresql-photon:v1.4.0Loaded image: vmware/harbor-adminserver:v1.4.0Loaded image: vmware/harbor-ui:v1.4.0Loaded image: vmware/harbor-log:v1.4.0Loaded image: vmware/harbor-jobservice:v1.4.0Loaded image: vmware/nginx-photon:v1.4.0Loaded image: vmware/registry-photon:v2.6.2-v1.4.0Loaded image: vmware/photon:1.0Loaded image: vmware/mariadb-photon:v1.4.0Loaded image: vmware/harbor-db-migrator:1.4[Step 2]: preparing environment ...Clearing the configuration file: ./common/config/adminserver/envClearing the configuration file: ./common/config/ui/envClearing the configuration file: ./common/config/ui/app.confClearing the configuration file: ./common/config/ui/private_key.pemClearing the configuration file: ./common/config/db/envClearing the configuration file: ./common/config/jobservice/envClearing the configuration file: ./common/config/jobservice/app.confClearing the configuration file: ./common/config/registry/config.ymlClearing the configuration file: ./common/config/registry/root.crtClearing the configuration file: ./common/config/nginx/nginx.confClearing the configuration file: ./common/config/log/logrotate.confloaded secret from file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/jobservice/app.confGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ...Creating harbor-log ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-adminserver ... doneCreating harbor-ui ... doneCreating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://172.16.194.130.For more details, please visit https://github.com/vmware/harbor .# 这时候你可以通过docker-compose 或docker ps来查看Harbor依赖运行的一些容器# 当然你也可以通过docker-compose来管理这些容器[root@k8s-node02 harbor]# docker-compose ps Name Command State Ports-------------------------------------------------------------------------------------------------------------------------------------harbor-adminserver /harbor/start.sh Up (healthy)harbor-db /usr/local/bin/docker-entr ... Up (healthy) 3306/tcpharbor-jobservice /harbor/start.sh Up (healthy)harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcpharbor-ui /harbor/start.sh Up (healthy)nginx nginx -g daemon off; Up 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpregistry /entrypoint.sh serve /etc/ ... Up (healthy) 5000/tcp[root@k8s-node02 harbor]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbaf9d3e586f8 vmware/harbor-jobservice:v1.4.0 \"/harbor/start.sh\" About an hour ago Up About an hour (healthy) harbor-jobservice484d5c4fca4b vmware/nginx-photon:v1.4.0 \"nginx -g 'daemon of…\" About an hour ago Up About an hour 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginxdd7c62b45af1 vmware/harbor-ui:v1.4.0 \"/harbor/start.sh\" About an hour ago Up About an hour (healthy) harbor-uie5494bd12f64 vmware/registry-photon:v2.6.2-v1.4.0 \"/entrypoint.sh serv…\" About an hour ago Up About an hour (healthy) 5000/tcp registry915b753623b7 vmware/harbor-adminserver:v1.4.0 \"/harbor/start.sh\" About an hour ago Up About an hour (healthy) harbor-adminserver55ca16b86243 vmware/harbor-db:v1.4.0 \"/usr/local/bin/dock…\" About an hour ago Up About an hour (healthy) 3306/tcp harbor-db30ca0cb76dd0 vmware/harbor-log:v1.4.0 \"/bin/sh -c /usr/loc…\" About an hour ago Up About an hour (healthy) 127.0.0.1:1514-&gt;10514/tcp 3.5、登录Harbor启动完成后，会提示你Harbor的访问地址：http://172.16.194.130 登录界面 输入账号和我们预先设定的密码：admin/abc123!! 我们可以看到系统各个模块如下： 项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等 日志：仓库各个镜像create、push、pull等操作日志 系统管理 用户管理：新增/删除用户、设置管理员等 复制管理：新增/删除从库目标、新建/删除/启停复制规则等 配置管理：认证模式、复制、邮箱设置、系统设置等 其他设置 用户设置：修改用户名、邮箱、名称信息 修改密码：修改用户密码 注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。 3.6、向Harbor仓库中心提交私有镜像我们要尝试下能不能把自己 Docker 里面的镜像 push 到 Harbor 的 library 里来（默认这个 library 项目是公开的，所有人都可以有读的权限，都不需要 docker login 进来，就可以拉取里面的镜像）。 3.6.1、配置Docker registry仓库地址在/etc/docker/daemon.json里添加配置如下：123&#123; &quot;insecure-registries&quot;: [&quot;172.16.194.130&quot;]&#125; 配置好后，别忘了重启systemctl restart docker 3.6.2、Docker 登录Harbor为什么要登录呢？跟Docker Hub一样，你得登录才能表明你是合法用户，才能push；1234$ docker login 172.16.194.130Username: adminPassword: (这里输入harbor平台设置的admin密码)Login Succeeded 3.6.3、本地私有镜像打tag，提交到Harbor1234567891011$ docker tag tale:base 172.16.194.130/library/tale:base$ docker push 172.16.194.130/library/taleThe push refers to repository [172.16.194.130/library/tale]a3ece4722ead: Pusheded61150eb02c: Pushed0f9f3d37a459: Pushed8ed018b01f91: Pushedb17185091796: Pushedb03095563b79: Pushedbase: digest: sha256:f82f2e175479d6d232efab45f81a4495cc4ad0a48135fd839dc27fdee8c13c77 size: 1574 提交成功，我们来看看Harbor仓库里的信息 能看到已经提交到libary公共仓库中。 同理，你也可以测试下从 Harbor pull 镜像到你的 Docker 中去，操作如下：12345678$ docker rmi 172.16.194.130/library/tale:base$ docker pull 172.16.194.130/library/tale:basebase: Pulling from library/taleDigest: sha256:f82f2e175479d6d232efab45f81a4495cc4ad0a48135fd839dc27fdee8c13c77Status: Downloaded newer image for 172.16.194.130/library/tale:base[root@k8s-node02 harbor]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE172.16.194.130/library/tale base ab8e3ca33cd0 5 days ago 372MB 镜像在被我删除后，从Harbor里成功pull了回来。 3.7、Harbor配置ssl认证3.7.1、创建证书1$ cd /data/cert/ 1、创建 CA 根证书1$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt -subj &quot;/C=CN/L=xian/O=nova/CN=harbor-registry&quot; 2、生成一个证书签名, 设置访问域名为harbor.moxiu.cn1$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout harbor.moxiu.cn.key -out server.csr -subj &quot;/C=CN/L=xian/O=nova/CN=harbor.moxiu.cn&quot; 3、生成主机的证书1$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out harbor.moxiu.cn.crt 3.7.2、配置harbor以https方式访问12345$ vim harbor.cfghostname = harbor.moxiu.cn:443ui_url_protocol = httpsssl_cert = /data/cert/harbor.moxiu.cn.crtssl_cert_key = /data/cert/harbor.moxiu.cn.key 3.7.3、配置Docker registry仓库地址在/etc/docker/daemon.json里添加配置如下：123&#123; \"insecure-registries\": [\"harbor.moxiu.cn\"]&#125; 然后，重启docker服务生效 3.7.4、登录验证1、验证admin登录方法11234$ docker login harbor.moxiu.cnUsername (admin): adminPassword: Login Succeeded 2、验证admin登录方法212$ docker login -u admin -p abc123!! harbor.moxiu.cnLogin Succeeded 3、Web页面登录验证http://harbor.moxiu.cn/harbor/sign-in用户名/密码：admin/abc123!! 因为不是有效机构颁发的证书，所有浏览器会提示不安全。如果企业需要使用，那需要买商用的证书更换即可。","tags":[{"name":"harbor","slug":"harbor","permalink":"//nicksors.cc/tags/harbor/"}]}]