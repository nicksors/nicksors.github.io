<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes系列之《PersistentVolumes》]]></title>
    <url>%2F2019%2F07%2F03%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8APersistentVolumes%E3%80%8B.html</url>
    <content type="text"><![CDATA[官网：https://kubernetes.io/docs/concepts/storage/persistent-volumes/ 1、理解PV&amp;PVC当pod中定义volume的时候，我们只需要使用pvs存储卷就可以，pvc必须与对应的pv建立关系，pvc会根据定义去pv申请，而pv是由存储空间创建出来的。pv和pvc是kubernetes抽象出来的一种存储资源。 Persistent Volumes（PV）：对存储资源创建和使用的抽象，使得存储作为集群中的资源管理。PV分为静态和动态，动态能够自动创建PV； PersistentVolumeClaims（PVC）：让用户不需要关心具体的Volume实现细节； 容器与PV、PVC之间的关系：PV是提供者，PVC是消费者，消费的过程就是绑定 2、Persistent Volumes（持久卷）静态绑定根据上图我们可以三个部分： 数据卷定义：(调用PVC) 卷需求模板：(PVC) 容器应用：（PV） 2.1、配置数据卷和卷需求模板1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master-128 volume]# cat pvc-pod.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx # 启用数据卷的名字为wwwroot,并挂载到nginx的html目录下 volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html ports: - containerPort: 80 # 定义数据卷名字为wwwroot,类型为pvc volumes: - name: wwwroot persistentVolumeClaim: claimName: my-pvc---# 定义pvc的数据来源,根据容量大小来匹配pvapiVersion: v1kind: PersistentVolumeClaimmetadata: # 对应上面的名字 name: my-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi 创建：123456789[root@k8s-master-128 volume]# kubectl create -f pvc-pod.yamldeployment.apps/nginx-deployment createdpersistentvolumeclaim/my-pvc created[root@k8s-master-128 volume]# kubectl get pod|grep deploynginx-deployment-5cd778bdb4-77rk8 0/1 Pending 0 26hnginx-deployment-5cd778bdb4-wrcjd 0/1 Pending 0 26h[root@k8s-master-128 volume]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmy-pvc Pending 49s 因为还没有创建PV，所以新创建的PVC和deployment都处于Pending状态 2.2、定义数据卷PV我们利用NFS做后端的空间来源12345678910111213[root@k8s-master-128 volume]# vim pv-pod.yamlapiVersion: v1kind: PersistentVolumemetadata: name: my-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: path: /opt/container_data server: 172.16.194.130 创建：1234567891011[root@k8s-master-128 volume]# kubectl create -f pv-pod.yamlpersistentvolume/my-pv created[root@k8s-master-128 volume]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmy-pv 5Gi RWX Retain Bound default/my-pvc 8s[root@k8s-master-128 volume]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmy-pvc Bound my-pv 5Gi RWX 6m11s[root@k8s-master-128 volume]# kubectl get pod|grep deploynginx-deployment-5cd778bdb4-77rk8 1/1 Running 0 26hnginx-deployment-5cd778bdb4-wrcjd 1/1 Running 0 26h 此时我们可以看到，PVC根据选定的容量大小，自动匹配上了我们刚刚创建的 PV，Deployment也正常运行起来。 2.3、测试访问首先为上面的Pod创建一个Service，通过NodePort暴露端口：123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: NodePort 通过kubectl get svc查看暴露的服务端口：12[root@k8s-master-128 volume]# kubectl get svc|grep nginx-servicenginx-service NodePort 10.0.0.190 &lt;none&gt; 80:48528/TCP 79s 浏览器访问：http://NodeIP:Port, 上面信息能访问到，全在NFS共享目录里可以定义：12345[root@k8s-node-130 container_data]# pwd/opt/container_data[root@k8s-node-130 container_data]# cat index.htmlhello PVC[root@k8s-node-130 container_data]# 3、PersistentVolumeClaims PV动态供给当k8s业务上来的时候，会存在大量的PVC申请，此时我们人工创建PV匹配的话，工作量就会非常大，就需要动态的自动挂载相应的存储。 我们需要使用到StorageClass来对接存储，靠它来自动关联PVC并创建PV Kubernetes支持动态供给的存储插件：https://kubernetes.io/docs/concepts/storage/storage-classes/ 部署测试：因为NFS不支持动态存储，所以我们需要借用这个存储插件。nfs动态相关部署可以参考：https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client/deploy 3.1、定义一个Storage123456789[root@k8s-master-128 ~]# mkdir storage &amp;&amp; cd storage[root@k8s-master-128 storage]# cat class.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment&apos;s env PROVISIONER_NAME&apos;parameters: archiveOnDelete: &quot;false&quot; 3.2、部署RBAC授权因为storage自动创建pv需要经过kube-apiserver,所以要进行授权1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@k8s-master-128 storage]# cat rbac.yamlkind: ServiceAccountapiVersion: v1metadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3.3、部署一个自动创建PV的服务这里自动创建pv的服务由nfs-client-provisioner 完成，123456789101112131415161718192021222324252627282930313233[root@k8s-master-128 storage]# cat deployment.yamlkind: DeploymentapiVersion: extensions/v1beta1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 172.16.194.130 - name: NFS_PATH value: /opt/container_data volumes: - name: nfs-client-root nfs: server: 172.16.194.130 path: /opt/container_data 只需要更改NFS的地址以及共享目录即可； 创建：12345[root@k8s-master-128 storage]# kubectl create -f .[root@k8s-master-128 storage]# kubectl get deploy|grep nfsnfs-client-provisioner 1/1 1 1 2m37s[root@k8s-master-128 storage]# kubectl get pod|grep nfsnfs-client-provisioner-7f6869cc64-7tgt4 1/1 Running 0 44h 查看创建好的Storage：123[root@k8s-master-128 storage]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 3m48s 3.4、测试自动创建PV还是使用静态部署Pod的例子，这次让其html下面自动挂载数据卷123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master-128 storage]# cat pvc-pod.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx # 启用数据卷的名字为wwwroot,并挂载到nginx的html目录下 volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html ports: - containerPort: 80 # 定义数据卷名字为wwwroot,类型为pvc volumes: - name: wwwroot persistentVolumeClaim: claimName: my-pvc---# 定义pvc的数据来源,根据容量大小来匹配pvapiVersion: v1kind: PersistentVolumeClaimmetadata: # 对应上面的名字 name: my-pvcspec: storageClassName: &quot;managed-nfs-storage&quot; # 增加此行，于自动申请创建PV（此名字需与创建的storage资源名称相同） accessModes: - ReadWriteMany resources: requests: storage: 1Gi 创建：1[root@k8s-master-128 storage]# kubectl create -f pvc-pod.yaml 查看创建的Pod及资源：1234567891011[root@k8s-master-128 storage]# kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEnfs-client-provisioner 1/1 1 1 62mnginx 1/1 1 1 17dnginx-deployment 2/2 2 2 25s[root@k8s-master-128 storage]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-7f6869cc64-7tgt4 1/1 Running 0 45hnginx-7db9fccd9b-gr8nv 1/1 Running 0 45hnginx-deployment-5cd778bdb4-lzcfx 1/1 Running 0 44hnginx-deployment-5cd778bdb4-pvrrm 1/1 Running 0 44h 查看手动创建的PVC和自动创建的PV：123456[root@k8s-master-128 storage]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-83032e77-7ec7-11e9-9b62-000c29f4daa9 1Gi RWX Delete Bound default/my-pvc managed-nfs-storage 44h[root@k8s-master-128 storage]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmy-pvc Bound pvc-83032e77-7ec7-11e9-9b62-000c29f4daa9 1Gi RWX managed-nfs-storage 45s 进入其中一个容器，创建资源文件：123[root@k8s-master-128 storage]# kubectl exec -it nginx-deployment-5cd778bdb4-pvrrm /bin/bashroot@nginx-deployment-5cd778bdb4-pvrrm:/# cd /usr/share/nginx/html/root@nginx-deployment-5cd778bdb4-pvrrm:/usr/share/nginx/html# echo &quot;hello this is PVC&quot; &gt;index.html 访问Pod里的容器：1234567[root@k8s-master-128 storage]# kubectl get pod -o wide|grep nginx-deploymentnginx-deployment-5cd778bdb4-lzcfx 1/1 Running 0 44h 172.17.81.2 172.16.194.130 &lt;none&gt; &lt;none&gt;nginx-deployment-5cd778bdb4-pvrrm 1/1 Running 0 44h 172.17.41.3 172.16.194.129 &lt;none&gt; &lt;none&gt;# 在Node节点上访问[root@k8s-node-129 ~]# curl 172.17.81.2 hello this is PVC 查看NFS存储的该文件：123[root@k8s-node-130 ~]# cd /opt/container_data/default-my-pvc-pvc-83032e77-7ec7-11e9-9b62-000c29f4daa9/[root@k8s-node-130 default-my-pvc-pvc-83032e77-7ec7-11e9-9b62-000c29f4daa9]# cat index.htmlhello this is PVC 至此，则动态申请资源环境配置完成，且部署Pod进行测试，动态资源分配成功。 删除资源：1234567[root@k8s-master-128 storage]# kubectl delete -f pvc-pod.yamldeployment.apps &quot;nginx-deployment&quot; deletedpersistentvolumeclaim &quot;my-pvc&quot; deleted[root@k8s-master-128 storage]# kubectl get pvNo resources found.[root@k8s-master-128 storage]# kubectl get pvcNo resources found. 如若直接删除，PV将不复存在，当然后面存储的资源文件也将灰飞烟灭。 倘若想删除Pod等资源时，想要保留为其分配的PV资源，则需将配置如下：12345678[root@k8s-master-128 storage]# cat class.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment&apos;s env PROVISIONER_NAME&apos;parameters: archiveOnDelete: &quot;true&quot; # 将此参数改为true即可启用删除前备份PV。 完。 参考文献： 持久卷 PersistentVolumes Kubernetes-持久化存储卷PersistentVolume Kubernetes-基于StorageClass的动态存储供应 Kubernetes之存储]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>PVC</tag>
        <tag>PV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《Volume》]]></title>
    <url>%2F2019%2F06%2F27%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AVolume%E3%80%8B.html</url>
    <content type="text"><![CDATA[1、了解k8s的Volume官网：https://kubernetes.io/docs/concepts/storage/volumes/ kubernetes中的Volume提供了在容器中挂载外部存储的能力，Pod需要设置卷来源（spec.volume）和挂载点（spec.containers.volumeMounts）良哥信息后才可以使用相应的Volume volume根据使用可以分为以下三类： Volume本地和网络数据卷 Persistent Volume 持久数据卷 Persistent Volume 动态供给 数据卷 本地数据卷：emptrDir、hostPath网络数据卷：NFS 2、emptyDir（空目录）kubectl explain pod.spec.volumes.emptyDir 指定emptyDir存储卷 medium 指定媒介类型 disk Memory 两种 sizeLimit 现在资源使用情况 kubectl explain pod.spec.containers.volumeMounts 指定挂载哪些存储卷 mountPath 挂载那个路径 name 指定挂载的volumes名称 readOnly 是否只读挂载 subPath 是否挂载子路径 创建一个空卷，挂载到Pod中的容器。Pod删除该卷也不会被删除。应用场景：Pod中容器之间数据共享，我们可以创建2个容器，一个写，一个读，来测试我们数据是否共享1234567891011121314151617181920212223242526272829[root@k8s-master-128 ~]# mkdir volume &amp;&amp; cd volume/[root@k8s-master-128 volume]# vim empty.yamlapiVersion: v1kind: Podmetadata: name: my-podspec: containers: - name: write image: centos command: [&quot;bash&quot;,&quot;-c&quot;,&quot;for i in &#123;1..100&#125;;do echo $i &gt;&gt; /data/hello;sleep 1;done&quot;] volumeMounts: # 根据下面的数据卷名字使用名为data的数据卷，挂载在/data下面 - name: data mountPath: /data - name: read image: centos command: [&quot;bash&quot;,&quot;-c&quot;,&quot;tail -f /data/hello&quot;] volumeMounts: # 根据下面的数据卷名字使用名为data的数据卷，挂载在/data下面 - name: data mountPath: /data # 定义一个数据卷来源 volumes: # 定义数据卷名字 - name: data emptyDir: &#123;&#125; 这样，我们2个容器都将名为data的数据卷挂载在/data目录下 创建：12345[root@k8s-master-128 volume]# kubectl create -f empty.yamlpod/my-pod created[root@k8s-master-128 volume]# kubectl get pods my-podNAME READY STATUS RESTARTS AGEmy-pod 2/2 Running 1 3m4s 检测：1234[root@k8s-master-128 volume]# kubectl logs -f my-pod read123 因为我们是每隔一秒写入，所以我们能够不停的看见打印的数据。 3、hostPath（本地挂载）kubectl explain pod.spec.volumes.hostPath path 指定宿主机的路径 type DirectoryOrCreate 宿主机上不存在创建此目录 Directory 必须存在挂载目录 FileOrCreate 宿主机上不存在挂载文件就创建 File 必须存在文件 挂载Node文件系统上的文件或目录到Pod中的容器。应用场景：Pod中容器需要访问宿主机文件1234567891011121314151617181920212223[root@k8s-master-128 volume]# cat hostpath.yamlapiVersion: v1kind: Podmetadata: name: my-podspec: containers: - name: busybox image: busybox args: - /bin/sh - -c - sleep 36000 # 挂载点 volumeMounts: - name: data mountPath: /data volumes: - name: data #挂载来源，宿主机的/tmp目录 hostPath: path: /tmp type: Directory 创建：12345[root@k8s-master-128 volume]# kubectl create -f hostpath.yamlpod/hostpath-pod created[root@k8s-master-128 volume]# kubectl get pods hostpath-pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEShostpath-pod 1/1 Running 0 97s 172.17.4.3 172.16.194.129 &lt;none&gt; &lt;none&gt; 使用-o wide参数知道hostpath-pod运行在172.16.194.129这个节点上，我们可以查看该节点的/tmp目录是否被挂载到容器中： 先查看172.16.194.129的/tmp目录所存在的文件：12345678910[root@k8s-node-129 ~]# ls -lha /tmp/总用量 0drwxrwxrwt. 8 root root 172 5月 23 20:25 .dr-xr-xr-x. 17 root root 224 4月 26 01:10 ..drwxrwxrwt. 2 root root 6 4月 26 01:06 .font-unixdrwxrwxrwt. 2 root root 6 4月 26 01:06 .ICE-unixdrwx------ 3 root root 17 5月 23 10:40 systemd-private-e3f8a01e9f24475b9544ebbb9406fe01-chronyd.service-vzW5qSdrwxrwxrwt. 2 root root 6 4月 26 01:06 .Test-unixdrwxrwxrwt. 2 root root 6 4月 26 01:06 .X11-unixdrwxrwxrwt. 2 root root 6 4月 26 01:06 .XIM-unix 在进入Pod中查看：1234567891011121314[root@k8s-master-128 volume]# kubectl exec -it hostpath-pod /bin/sh/ # ls # 能看到挂载的data目录bin data dev etc home proc root sys tmp usr var/ # ls -lha data/ total 0drwxrwxrwt 8 root root 172 May 23 12:26 .drwxr-xr-x 1 root root 41 May 23 12:10 ..drwxrwxrwt 2 root root 6 Apr 25 17:06 .ICE-unixdrwxrwxrwt 2 root root 6 Apr 25 17:06 .Test-unixdrwxrwxrwt 2 root root 6 Apr 25 17:06 .X11-unixdrwxrwxrwt 2 root root 6 Apr 25 17:06 .XIM-unixdrwxrwxrwt 2 root root 6 Apr 25 17:06 .font-unixdrwx------ 3 root root 17 May 23 02:40 systemd-private-e3f8a01e9f24475b9544ebbb9406fe01-chronyd.service-vzW5qS/ # 上面信息证明Pod中容器的/data挂载目录与172.16.194.129的/tmp目录一致，挂载成功。当然，你也可以创建一个文件试试看，Node节点的挂载目录也会同步。 4、NFS(文件共享存储，网络卷)kubectl explain pod.spec.volumes.nfs path 源目录 readOnly 是否只读 默认false server NFS服务地址 安装NFS：（需要使用nfs服务，每台Node节点都需要安装）1$ yum install -y nfs-utils 我们设置172.16.194.130为服务端，配置/opt/container_data/共享目录出来，并且具有读写权限12[root@k8s-node-130 ~]# cat /etc/exports/opt/container_data 172.16.194.0/24(rw,no_root_squash) 然后启动服务12$ systemctl start rpcbind$ systemctl start nfs 在客户端节点上查看共享目录：123[root@k8s-node-129 ~]# showmount -e k8s-node-130Export list for k8s-node-130:/opt/container_data 172.16.194.0/24 如果能看到共享目录信息，则表明NFS配置完成，接下来创建Pod使用之。123456789101112131415161718192021222324252627282930[root@k8s-master-128 volume]# vim nfs.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx # 启用数据卷的名字为wwwroot,并挂载到nginx的html目录下 volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html ports: - containerPort: 80 # 定义数据卷名字为wwwroot,类型为nfs volumes: - name: wwwroot nfs: server: 172.16.194.130 path: /opt/container_data 创建：1234567[root@k8s-master-128 volume]# kubectl create -f nfs.yamldeployment.apps/nginx-deployment created[root@k8s-master-128 volume]# kubectl get deploy|grep nginx-nginx-deployment 0/2 2 0 12s[root@k8s-master-128 volume]# kubectl get pods |grep nginx-deploynginx-deployment-668db478c9-fj9np 1/1 Running 0 61snginx-deployment-668db478c9-wsc5n 1/1 Running 0 61s 查看挂载点下的文件：1234[root@k8s-node-130 container_data]# echo &quot;hello world&quot; &gt;index.html[root@k8s-node-130 container_data]# ls -lh总用量 4.0K-rw-r--r-- 1 root root 12 5月 23 20:54 index.html 进入Pod的容器中，查看/usr/share/nginx/html 挂载文件数据：123456[root@k8s-master-128 volume]# kubectl exec -it nginx-deployment-668db478c9-wsc5n bashroot@nginx-deployment-668db478c9-wsc5n:/# ls -lh /usr/share/nginx/html/total 4.0K-rw-r--r-- 1 root root 12 May 23 12:54 index.htmlroot@nginx-deployment-668db478c9-wsc5n:/# cat /usr/share/nginx/html/index.htmlhello world 访问测试：找到容器的IP地址1$ kubectl get pods -o wide|grep nginx-deploy 在Node上测试：12[root@k8s-node-129 ~]# curl 172.17.34.7hello world 至此，使用NFS挂载网站根目录完成，今后类似需求可采用此方案。 参考文章： Kubernetes-存储卷Volume kunbernetes-基于NFS的存储]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Volume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《Ingress》]]></title>
    <url>%2F2019%2F06%2F26%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AIngress%E3%80%8B.html</url>
    <content type="text"><![CDATA[1、 Pod与lngress的关系 通过Service相关联 铜鼓lngress Contrller实现Pod的负载均衡 支持TCP/UDP 4层和HTTP 7层 2、 lngress Controller 用户访问lngress控制器，由控制器根据规则来实现对Service的访问，从而实现访问k8s内部的pod 2.1、 部署Controller部署文档：https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.mdingress的控制器由很多，官方维护的控制器是ingress-nginx，因此大多数情况下都是使用这个控制器， 12345678910111213141516171819202122232425[root@k8s-master-128 ~]# mkdir ingress-nginx &amp;&amp; cd ingress-nginx[root@k8s-master-128 ingress-nginx]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml[root@k8s-master-128 ingress-nginx]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml[root@k8s-master-128 ingress-nginx]# vim mandatory.yaml211 serviceAccountName: nginx-ingress-serviceaccount212 hostNetwork: true # 使当前的Pod使用宿主机的网络213 containers:214 - name: nginx-ingress-controller215 image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.24.1 # 如果下载不了需要换成国内源216 args:217 - /nginx-ingress-controller218 - --configmap=$(POD_NAMESPACE)/nginx-configuration219 - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services220 - --udp-services-configmap=$(POD_NAMESPACE)/udp-services221 - --publish-service=$(POD_NAMESPACE)/ingress-nginx222 - --annotations-prefix=nginx.ingress.kubernetes.io注意：- 镜像如果下载不了需要换成国内的- 使用宿主机网络：hostNetwork: true # https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/baremetal.md[root@k8s-master-128 ingress-nginx]# kubectl create -f mandatory.yaml[root@k8s-master-128 ingress-nginx]# kubectl create -f cloud-generic.yaml 部署报错：Error creating: pods “nginx-ingress-controller-565dfd6dff-g977n” is forbidden: SecurityContext.RunAsUser is forbidden解决：需要对Apiserver配置的准入控制器进行修改，然后重启Apiserver123456$ vim /opt/kubernetes/cfg/kube-apiserver将：--admission-control= NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction更换为：--admission-control= NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction不启用SecurityContextDeny就行了。 2.1.1、理解准入控制器要查看启用了哪些准入插件：kube-apiserver -h | grep enable-admission-plugins1.14版本默认启动的控制器：NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota NamespaceLifecycle：此准入控制器强制执行正在终止的命令空间中不能创建新对象，并确保Namespace拒绝不存在的请求。此准入控制器还防止缺失三个系统保留的命名空间default、kube-system、kube-public。 LimitRanger：此准入控制器将确保所有资源请求不会超过 namespace 的 LimitRange。 SecurityContextDeny：此准入控制器将拒绝任何试图设置某些升级的SecurityContext字段的pod 。 ServiceAccount：此准入控制器实现serviceAccounts的自动化。 ResourceQuota：此准入控制器将观察传入请求并确保它不违反命名空间的ResourceQuota对象中列举的任何约束。 NodeRestriction：该准入控制器限制了 kubelet 可以修改的Node和Pod对象。 NamespaceExists：此许可控制器检查除 Namespace 其自身之外的命名空间资源上的所有请求。如果请求引用的命名空间不存在，则拒绝该请求。 2.1.2、检测部署情况1234567891011121314151617[root@k8s-master-128 ingress-nginx]# kubectl get deploy -n ingress-nginx # 我配置了两个副本NAME READY UP-TO-DATE AVAILABLE AGEnginx-ingress-controller 2/2 2 2 46m[root@k8s-master-128 ingress-nginx]# kubectl get pods -n ingress-nginxNAME READY STATUS RESTARTS AGEnginx-ingress-controller-76f9fddcf8-7hn2k 1/1 Running 0 13hnginx-ingress-controller-76f9fddcf8-9lg7d 1/1 Running 0 13h[root@k8s-master-128 ingress-nginx]# POD_NAMESPACE=ingress-nginx[root@k8s-master-128 ingress-nginx]# POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;)[root@k8s-master-128 ingress-nginx]# kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version-------------------------------------------------------------------------------NGINX Ingress controller Release: 0.24.1 Build: git-ce418168f Repository: https://github.com/kubernetes/ingress-nginx------------------------------------------------------------------------------- NGINX Ingress controller安装正常， 上面在部署的时候配置了hostNetwork: true参数，即共用宿主机网络，因此在Node节点上会启动一个80和443端口123456[root@k8s-node-129 ~]# netstat -lntup|egrep &apos;80|443&apos;tcp 0 0 172.16.194.129:2380 0.0.0.0:* LISTEN 6346/etcdtcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 28589/nginx: mastertcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 28589/nginx: mastertcp6 0 0 :::80 :::* LISTEN 28589/nginx: mastertcp6 0 0 :::443 :::* LISTEN 28589/nginx: master Node节点上的80和443端口就是ingress规则的访问入口，官网架构图： 2.2、 创建 ingress规则12345678910111213141516171819202122232425# 查看到Service 服务有nginx，并且在集群内暴露了80端口[root@k8s-master-128 ingress-nginx]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 16dnginx NodePort 10.0.0.86 &lt;none&gt; 80:43364/TCP 15d# 编写一个ingress规则，代理上面的nginx Service[root@k8s-master-128 ingress-nginx]# vim ingress.yamlapiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: example-ingress # 集群内服务标识spec: rules: - host: example.foo.com # 对外提供的域名（类似于nginx的 server_name 字段） http: paths: - backend: serviceName: nginx # Service服务名称 servicePort: 80 # 集群内暴露的端口[root@k8s-master-128 ingress-nginx]# kubectl create -f ingress.yaml[root@k8s-master-128 ingress-nginx]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEexample-ingress example.foo.com 80 32s 2.3、 测试访问通过绑定本机hosts来测试访问服务 1234567$ sudo vim /etc/hosts172.16.194.129 example.foo.com$ ping example.foo.comPING example.foo.com (172.16.194.129): 56 data bytes64 bytes from 172.16.194.129: icmp_seq=0 ttl=64 time=0.445 ms64 bytes from 172.16.194.129: icmp_seq=1 ttl=64 time=0.722 ms 本机浏览器访问： 查看k8s集群的Pod响应日志12[root@k8s-master-128 ingress-nginx]# kubectl logs -f nginx-7db9fccd9b-9bcpv172.17.4.0 - - [23/May/2019:03:44:32 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36&quot; &quot;172.16.194.1&quot; 3、配置HTTPS之前在部署kubernetes Dashboard时是通过NodePort暴露的端口，端口为443，现在测试使用ingress方式代理这个服务；首先给dashboard生成一个证书：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@k8s-master-128 ingress-nginx]# mkdir https &amp;&amp; cd https[root@k8s-master-128 https]# cat dashboard-cret.sh#!/bin/bash# 创建kubernetes dashboard证书cat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;dashboard&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca -#-----------------------# 颁发域名证书cat &gt; dashboard.kubernetes.com-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;dashboard.kubernetes.com&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=dashboard dashboard.kubernetes.com-csr.json | cfssljson -bare dashboard.kubernetes.com[root@k8s-master-128 https]#[root@k8s-master-128 https]# ./dashboard-cret.sh[root@k8s-master-128 https]# lsca-config.json ca-csr.json ca.pem dashboard.kubernetes.com.csr dashboard.kubernetes.com-key.pemca.csr ca-key.pem dashboard-cret.sh dashboard.kubernetes.com-csr.json dashboard.kubernetes.com.pem 接下来根据上面生成的CA证书创建一个secret资源，来存储秘钥和证书。如果你的证书是.pem的，也是一样的，将下面的your_cert.crt改为your_cert.pem即可。1kubectl create secret tls tls_secret_name --key your_key.key --cert your_cert.crt 创建secret资源：12345[root@k8s-master-128 https]# kubectl -n kube-system create secret tls dashboard-secret --key dashboard.kubernetes.com-key.pem --cert dashboard.kubernetes.com.pemsecret/dashboard.kubernetes.com created[root@k8s-master-128 https]#[root@k8s-master-128 https]# kubectl -n kube-system get secret|grep dashboard-secretdashboard-secret kubernetes.io/tls 2 18s 创建ingress服务：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@k8s-master-128 ingress-nginx]# vim dashboard-ingress.ymalapiVersion: extensions/v1beta1kind: Ingressmetadata: name: dashboard-ingress namespace: kube-system annotations: nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot; nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;spec: tls: - hosts: - dashboard.kubernetes.com secretName: dashboard-secret rules: - host: dashboard.kubernetes.com http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443[root@k8s-master-128 ingress-nginx]# kubectl create -f dashboard-ingress.ymalingress.extensions/dashboard-ingress created[root@k8s-master-128 ingress-nginx]# kubectl get ingress -n kube-systemNAME HOSTS ADDRESS PORTS AGEdashboard-ingress dashboard.kubernetes.com 80, 443 12s[root@k8s-master-128 ingress-nginx]#[root@k8s-master-128 ingress-nginx]# kubectl describe ingress dashboard-ingress -n kube-systemName: dashboard-ingressNamespace: kube-systemAddress:Default backend: default-http-backend:80 (&lt;none&gt;)TLS: dashboard-secret terminates dashboard.kubernetes.comRules: Host Path Backends ---- ---- -------- dashboard.kubernetes.com / kubernetes-dashboard:443 (172.17.63.8:8443)Annotations: nginx.ingress.kubernetes.io/backend-protocol: HTTPS nginx.ingress.kubernetes.io/ssl-passthrough: trueEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 10m nginx-ingress-controller Ingress kube-system/dashboard-ingress Normal CREATE 10m nginx-ingress-controller Ingress kube-system/dashboard-ingress 测试访问：1[root@k8s-master-128 ingress-nginx]# kubectl describe secrets -n kube-system $(kubectl get secrets -n kube-system |awk &apos;/dashboard-admin/&#123;print $1&#125;&apos;) 访问效果 在这个过程中遇到了坑，总结记录如下：1、参考“kubernetes1.13.1部署ingress-nginx并配置https转发dashboard”文章进行试验，在yaml部分直接拷贝过来用，发现代理不成功，查看Pod日志提示错误如下：123452019/05/24 08:13:24 http: TLS handshake error from 172.17.63.1:53934: tls: first record does not look like a TLS handshake2019/05/24 08:13:28 http: TLS handshake error from 172.17.63.1:53960: tls: first record does not look like a TLS handshake2019/05/24 08:13:28 http: TLS handshake error from 172.17.63.1:53964: tls: first record does not look like a TLS handshake2019/05/24 08:13:30 http: TLS handshake error from 172.17.63.1:53980: tls: first record does not look like a TLS handshake2019/05/24 08:13:32 http: TLS handshake error from 172.17.63.1:53996: tls: first record does not look like a TLS handshake 2、问题定位：于是怀疑yaml是否有写错，一步步确认yaml的写法，最终定位到annotations这个配置，此配置的参数查看官网：https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md与官网一番对比后，只因之前yaml配置文件里参数nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;在官网显示已被淘汰；官网解释： 3、理解参数意义Backend Protocol意既：后端协议；什么意识呢？就是指明后端的代理协议，如果Pod提供的直接就是需要通过HTTPS来访问，则在配置Ingress的时候需要配置Backend Protocol才可代理访问（HTTP不用配置），否则就会代理失效。切记，切记！ 参考博文： kubernetes1.13.1部署ingress-nginx并配置https转发dashboard Kubernetes 准入控制 Admission Controller 介绍 Kubernetes 使用 ingress 配置 https 集群 k8s ingress (http/https)部署与使用 kubernetes-核心资源之Ingress kubernetes之ingress及ingress controller]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Ingress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《Service》]]></title>
    <url>%2F2019%2F06%2F21%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AService%E3%80%8B.html</url>
    <content type="text"><![CDATA[简单了解Service： 防止Pod失去联系（服务发现） 定义一组Pod的访问策略（负载均衡） 支持ClusterIP，NodePort以及LoadBalancer三种类型 Service的底层实现主要有iptables和ipvs两种网络模式 Services在kubenetes中，是帮助Pod提供网络服务的。比如你要访问Pod里的应用，需要通过端口来访问，而Service可以通过端口代理转发，让你通过Node节点上的端口能够访问到你的Pod应用。 一、Pod与Service的关系 通过lable-selector相关联 通过Service实现Pod的负载均衡（TCP/UDP 4层） Yaml配置文件例子：1234567891011121314151617181920212223242526272829303132333435[root@k8s-master-128 dome]# cat deploy-nginx.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: # 这里是定义Deployment的标签 app: nginxspec: replicas: 3 selector: matchLabels: app: nginx # 选择关联Deployment标签 template: metadata: labels: # 给Pod定义一个标签，方便其他服务关联这个Pod app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: selector: # Service 的selector 指定标签 app:nginx 来进行对Pod进行关联 ；(这里的app:nginx就是上面Deployment配置里labels定义的标签 ) app: nginx ports: - protocol: TCP port: 80 targetPort: 80 type: NodePort 可以通过以下方式查看Service与Pod的关联： 首先查看创建的Service，通过-o wide输出详细信息1234[root@k8s-master-128 dome]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 5d5h &lt;none&gt;nginx-service NodePort 10.1.230.224 &lt;none&gt; 80:30234/TCP 24h app=nginx 通过 kubectl describe pod 查看pod的详情，里面就有标签信息1234567891011121314[root@k8s-master-128 dome]# kubectl describe pod nginx-deployment-6dd86d77d-kxkmtName: nginx-deployment-6dd86d77d-kxkmtNamespace: defaultPriority: 0PriorityClassName: &lt;none&gt;Node: k8s-node-130/172.16.194.130Start Time: Mon, 20 May 2019 16:30:32 +0800Labels: app=nginx # 这里就是pod定义的标签 pod-template-hash=6dd86d77dAnnotations: &lt;none&gt;Status: RunningIP: 10.244.2.14Controlled By: ReplicaSet/nginx-deployment-6dd86d77d···略··· 也可以通过-o wide查看Deployment的标签信息123[root@k8s-master-128 dome]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 3/3 3 3 24h nginx nginx:1.7.9 app=nginx 二、Service的三种类型发布的服务类型有三种：ClusterIP、NodePort和LoadBalancer官网地址：https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types 2.1、ClusterIP 分配一个内部集群IP地址，只能在集群内部访问（同Namespaces内的Pod），不对外提供访问服务！默认ServiceTpye。 来看一个示例说明：1234567891011121314151617[root@k8s-master-128 dome]# cat service-nginx.yamlapiVersion: v1kind: Servicemetadata: name: nginx-servicespec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80[root@k8s-master-128 dome]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 5d5hnginx-service ClusterIP 10.1.0.32 &lt;none&gt; 80/TCP 24h 我们能看到CLUSTER-IP下显示着 Service所提供集群内部使用的IP地址，这个地址只能在k8s集群内部访问哦！再看PORT(S)字段：80/TCP 80是暴露给集群的端口，通过集群ip加端口可以访问：curl http://10.1.0.32:80 Node节点上能查看到创建的iptables规则：123[root@k8s-node-129 ~]# iptables-save |grep 10.1.0.32-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.1.0.32/32 -p tcp -m comment --comment &quot;default/nginx-service: cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ-A KUBE-SERVICES -d 10.1.0.32/32 -p tcp -m comment --comment &quot;default/nginx-service: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-GKN7Y2BSGW4NJTYL 能看到我们创建了Service后，在所有Node节点上都会自动创建转发规则，来实现Pod的网络访问。 2.2、NodePort 分配一个内网集群IP地址，并在内个节点上启用一个端口来暴露服务，可以在集群外部访问。 访问地址：: 接上面ClusterIP的官网Yaml示例，继续研究NodePort：123456789101112apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - protocol: TCP port: 80 # 需要暴露的集群端口 targetPort: 9376 # 容器的端口 type: NodePort 上面说过Service默认的类型是ClusterIP，如果需更改成NodePort类型，则需要将type字段指定类型即可； 说说type: NodePort12345[root@k8s-master-128 dome]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 5d6h &lt;none&gt;nginx-service NodePort 10.1.230.224 &lt;none&gt; 80:30234/TCP 25h app=nginx# 创建出来的NodePort服务，也会有一个cluster ip； 根据上面的信息，nginx-service这个服务的NodePort所对应的则是30234，而这个端口是在Node节点上由kube-proxy来启动并监听的，因此我们可以通过NodeIP+端口的方式直接进行访问，就是因为kube-proxy进行了代理。继续扩展说明，kube-proxy是如何代理访问到容器的呢？因为kube-proxy在启动的时候通过--proxy-mode=ipvs可以指定底层内核代理模式，默认是iptables进行代理转发，kube-proxy通过这两种模式来代理直接对外提供服务。 2.3、LoadBalancer分配一个内部集群IP地址，并在每个节点上启用一个端口来暴露服务。除此之外，Kubernetes会请求底层云平台上的负载均衡器，将每个Node（[NodeIP]:[NodePort]）作为后端添加进去。 LoadBalancer只适用于云平台，AWS默认就支持，阿里云社区开发也支持。 三、Service代理模式service有三组代理模式：userspace、iptables和ipvs 3.1、Userspace模式 客户端访问ServiceIP，ServiceIP将请求交给了kube-proxy，然后分发到Pod进行通讯。 多了kube-proxy转发，效率较低！kube-proxy工作在用户态，故是Userspace模式。 3.2、Iptables模式 客户端访问ServiceIP，通过iptables转发给Pod。1.9x版本默认代理模式。 iptables基于linux内核（Netfilet）实现的，运行在内核态，内核直接处理请求，因此效率较高。 3.3、Ipvs模式 ipvs在k8s1.10以上版本中，将会是默认代理模式。 客户端访问ServierIP，通过虚拟服务器将请求转发给RS(也就是Pod)，该模式采用LVS技术，早在linux发行版本中就已将lvs嵌入进去，lvs运行在内核态，处理效率相比上面两个是最优的。 3.4、Iptables模式 vs Ipvs模式目前用到最多的就是这两个模式，上面也提到了ipvs会是将来版本中默认的代理模式，这里进行简单的对比： iptables： 灵活性，功能强大 规则遍历匹配和更新,呈线性延迟 可扩展性 ipvs： 工作在内核，有更高的性能 调度算法丰富：rr, wrr, lc, wlc, ip hash；LVS的调度算法皆可用 参考文章： Kubernetes-核心资源之Service Kubernetes之服务发现及负载Services]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《Pod》]]></title>
    <url>%2F2019%2F06%2F18%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8APod%E3%80%8B.html</url>
    <content type="text"><![CDATA[1、创建一个Pod的流程（面试易考） 用户通过kubectl命令创建一个Pod的流程： 客户端提交创建请求，可以通过API Server的Restful API，也可以使用kubectl工具，支持json和yaml格式； Api Server处理用户请求，存储Pod信息数据到etcd集群中； Scheduler调度器通过API Server查看未绑定的Pod，尝试为Pod进行分配主机，通过调度算法选择主机后，绑定到这台机器上并且把调度信息写入etcd集群； kubelet根据调度结果执行Pod创建操作，成功后将信息通过Api Server更新到etcd集群中； 整个Pod创建过程完成，每个组件都在于Api Server进行交互，Api Server就是k8s集群的中间者，组件之间的协同者，是一个集群访问入口； 2、 Pod的多种控制器 ReplicaSet: 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。 ReplicaSet主要三个组件组成： 用户期望的pod副本数量 标签选择器，判断哪个pod归自己管理 当现存的pod数量不足，会根据pod资源模板进行新建帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是RelicaSet不是直接使用的控制器，而是使用Deployment。 Deployment：工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。 参考文章：https://blog.csdn.net/bbwangj/article/details/82011573 DaemonSet：用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务,比如ingress,elk.服务是无状态的,服务必须是守护进程。参考文章：https://www.cnblogs.com/xzkzzz/p/9553321.html Job：只要任务或程序运行完成就立即退出，不需要重启或重建。 参考文章：https://blog.csdn.net/bbwangj/article/details/82011610 Cronjob：周期性任务控制，执行后就退出, 不需要持续后台运行， 参考文章：https://blog.csdn.net/bbwangj/article/details/82867830 StatefulSet：管理有状态应用,比如redis,mysql 3、YAML文件创建Pod12345678910apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: nginx 4、Pod基本管理123456789101112# 创建Pod资源$ kubectl create -f pod.yaml# 查看pods$ kubectl get pods nginx-pod# 查看pod描述$ kubectl describe pod/nginx-pod# 更新资源$ kubectl apply -f pod.yaml# 删除资源$kubectl delete -f pod.yamlor$kubectl delete pods nginx-pod 5、资源限制资源的现在对项目尤为重要，以免一些Pod占用资源过高，导致当前节点资源不可用，这也就直接影响k8s整个集群的状态。这不是我们想看到的结果，所以有资源限制，结合线上业务使用情况，进行资源分配限制。1234567891011121314151617apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: nginx resources: # 资源限制标签 requests: memory: &quot;64Mi&quot; cpu: &quot;250m&quot; limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; requests和limits都是做资源限制的，他们的区别在于： requests # pod在创建时向k8s请求的资源大小； limits # 限制了这个Pod运行的最大资源空间； 6、调度约束 Pod.spec.nodeName # 强制约束Pod调度到指定Node节点上 Pod.spec.nodeSelector # 通过lable-selector机制选择节点12345678910111213apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: # nodeName:node01 nodeSelector: env_role: dev containers: - name: nginx image: nignx 通过label给Node主机设置标签：1kubectl label nodes k8s-node-129 env_role=dev 通过–show-labels查看Node的标签：1$ kubectl get node --show-labels 7、重启策略 Always: 当容器停止，总是重建容器，默认策略。 OnFailure： 当容器异常退出（退出状态码非0）时，才重启容器。 Never：当容器终止退出，从不重启容器。1234567891011apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: nginx restartPolicy: OnFailure 8、镜像拉取策略 IfNotPresent：默认值，镜像不存在宿主机上时才拉取 Always：每次创建Pod时都会重新拉取一次镜像 Never：Pod永远不会主动拉取这个镜像1234567891011apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent 9、健康检查提供Probe机制，有以下两种类型： livenessProbe如果检查失败，将容器杀死，根据Pod的restartPolicy来操作。 readinessProbe如果检查失败，Kubeneres会把Pod从service endpoints中剔除。 Probe支持以下三种检查方法： httpGet发送HTTP请求，返回200-400范围状态码为成功。 exec执行Shell命令返回状态码是0为成功。 tcpSocket发起TCP Socket建立成功。 官网：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ 10、问题定位123kubectl describe type/namekubectl logs type/name [-c container]kubectl exec -it 容器名称 bash 参考文章： Kubernetes-守护进程(DaemonSet)应用 Kubernetes-核心资源之Pod Kubernetes之Deployment控制器]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Pod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《YAML配置文件管理对象》]]></title>
    <url>%2F2019%2F06%2F18%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AYAML%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%AF%B9%E8%B1%A1%E3%80%8B.html</url>
    <content type="text"><![CDATA[一、配置文件说明 定义配置时，指定最新稳定版API（当前为v1）； 配置文件应该存储在集群之外的版本控制仓库中。如果需要，可以快速回滚配置、重新创建和恢复； 应该使用YAML格式编写配置文件，而不是JSON。尽管这些格式都可以使用，但YAML对用户更加友好； 可以将相关对象组合成单个文件，通常会更容易管理； 不要没必要的指定默认值，简单和最小配置减少错误； 在注释中说明一个对象描述更好维护。 二、示例deployment1234567891011121314151617181920apiVersion: apps/v1beta2 # api版本kind: Deployment# 指定资源名称metadata: name: nginx-deployment # 元数据名称spec:# 资源控制器 replicas: 3 # 指定副本数量 selector: # 通过标签关联资源 matchLabels: # 匹配标签 app: nginx template: metadata: labels: app: nginx spec: containers: # 容器标签 - name: nginx # 容器名称 image: nginx:1.10# 容器镜像&amp;版本 ports: # 容器端口 - containerPort: 80 service123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-service labels: app: nginxspec: ports:80 selector: app: nginx - port: 88 targetPort: 三、快速配置YAML的方法3.1、用run命令生成123$ kubectl run --image=nginx my-deploy -o yaml --dry-run &gt;my-deploy.yaml$ kubectl create -f deploy-nginx.yaml -o yaml --dry-run &gt;my-deploy.yaml$ kubectl create -f deploy-nginx.yaml -o json --dry-run &gt;my-deploy.json # 指定输出json格式 – image # 指定模板镜像 my-deploy # 运行标签名称 –dry-run # 只测试运行,不会实际运行pod -o yaml # 指定输出格式 3.2、用get命令导出1$ kubectl get deploy/my-deploy -o=yaml --export &gt; my-deploy.yaml 3.3、查询Pod容器的字段资源内部文档使用kubectl explain –help 查询pod字段内部说明123$ kubectl explain pods # 每一个层级的指令都会有字段信息$ kubectl explain pods.spec$ kubectl explain pods.spec.containers]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《Kubectl管理工具常用方法》]]></title>
    <url>%2F2019%2F06%2F14%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AKubectl%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E3%80%8B.html</url>
    <content type="text"><![CDATA[这篇文章是我常用的kubectl使用方法总结，与前面的kubernetes部署环境不同，但使用方法大同小异，注意下即可。 一、kubectl高可用1.1、kubectl访问细节kubectl默认是使用apiserve监听的IP和端口进行对集群的访问操作123[root@k8s-master-90 ~]# netstat -lntup|grep apiservetcp 0 0 10.0.10.90:6443 0.0.0.0:* LISTEN 24489/kube-apiserve tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 24489/kube-apiserve kubectl 访问1234567[root@k8s-master-90 ~]# kubectl -s 127.0.0.1:8080 get podsNAME READY STATUS RESTARTS AGEnginx-8586cf59-c2j24 1/1 Running 0 32mnginx-8586cf59-hxcq9 1/1 Running 0 32mnginx-8586cf59-vm6x6 1/1 Running 0 32m[root@k8s-master-90 ~]# kubectl -s 10.0.10.90:6443 get pods # api的6443端口需要证书认证才能进行访问Unable to connect to the server: net/http: HTTP/1.x transport connection broken: malformed HTTP response &quot;\x15\x03\x01\x00\x02\x02&quot; 127.0.0.1:8080之所以能访问，是因为该监听地址未经过证书认证，相反10.0.10.90:6443则需要经过证书许可认证才可以连接到集群进行操作。 我们的目的是想要在任何linux机器上，使用kubectl工具能访问和管理集群，该如何实现呢？ 1.2、部署kubectlkubectl本身就是编译好的二进制文件，这里我们将它部署在node-91机器上12345[root@k8s-master-90 ~]# scp /opt/kubernetes/bin/kubectl 10.0.10.91:/opt/kubernetes/bin/[root@k8s-node-91 ~]# echo &quot;export PATH=$PATH:/opt/kubernetes/bin&quot; &gt;&gt;/etc/profile[root@k8s-node-91 ~]# source /etc/profile[root@k8s-node-91 ~]# which kubectl/opt/kubernetes/bin/kubectl 1.3、运行kubectl12[root@k8s-node-91 ~]# kubectl get podsThe connection to the server localhost:8080 was refused - did you specify the right host or port? 连接失败，找不到服务器，那比如这台kubectl管理节点是在你公司？你如何实现管理线上的k8s集群呢？ 1.3.1、拷贝需要的证书文件到Master节点看一下，在二进制部署Kubernetes集群时，我们已经创建好了证书12345[root@k8s-master-90 ~]# ls -lh ssl/-rw------- 1 root root 1.7K 4月 17 18:43 admin-key.pem-rw-r--r-- 1 root root 1.4K 4月 17 18:43 admin.pem-rw------- 1 root root 1.7K 4月 17 18:43 ca-key.pem-rw-r--r-- 1 root root 1.4K 4月 17 18:43 ca.pem admin.pem和admin-key.pem就是为了今后方便管理k8s集群的 需要ca证书和admin证书，将证书拷贝过去123456[root@k8s-master-90 ~]# scp ssl/admin*pem 10.0.10.91:/opt/kubernetes/ssl/admin-key.pem 100% 1675 1.6KB/s 00:00 admin.pem 100% 1399 1.4KB/s 00:00 [root@k8s-master-90 ~]# scp ssl/ca*pem 10.0.10.91:/opt/kubernetes/ssl/ca-key.pem 100% 1679 1.6KB/s 00:00 ca.pem 100% 1359 1.3KB/s 00:00 1.3.2、生成.kube/config文件kubectl 默认会访问~/.kube目录里的配置文件，通过配置文件里指定的api地址来访问和管理集群。123456789101112131415161718# 设置集群项中名为kubernetes的apiserver地址与根证书kubectl config set-cluster kubernetes --server=https://10.0.10.90:6443 --certificate-authority=/opt/kubernetes/ssl/ca.pem # 设置用户项中cluster-admin用户证书认证字段kubectl config set-credentials cluster-admin --certificate-authority=ca.pem --client-key=admin-key.pem --client-certificate=admin.pem# 设置环境项中名为default的默认集群和用户kubectl config set-context default --cluster=kubernetes --user=cluster-admin# 设置默认环境项为defaultkubectl config use-context default# 操作过程[root@k8s-node-91 ~]# kubectl config set-cluster kubernetes --server=https://10.0.10.90:6443 --certificate-authority=/opt/kubernetes/ssl/ca.pem Cluster &quot;kubernetes&quot; set.[root@k8s-node-91 ~]# kubectl config set-credentials cluster-admin --certificate-authority=/opt/kubernetes/ssl/ca.pem --client-key=/opt/kubernetes/ssl/admin-key.pem --client-certificate=/opt/kubernetes/ssl/admin.pemUser &quot;cluster-admin&quot; set.[root@k8s-node-91 ~]# kubectl config set-context default --cluster=kubernetes --user=cluster-adminContext &quot;default&quot; created.[root@k8s-node-91 ~]# kubectl config use-context defaultSwitched to context &quot;default&quot;. 查看生成的配置文件123456789101112131415161718192021[root@k8s-node-91 ~]# cat .kube/configapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem server: https://10.0.10.90:6443 name: kubernetescontexts:- context: cluster: kubernetes user: cluster-admin name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: cluster-admin user: as-user-extra: &#123;&#125; client-certificate: /opt/kubernetes/ssl/admin.pem client-key: /opt/kubernetes/ssl/admin-key.pem 我们知道kubectl 客户端是通过证书，去访问apiserver对外监听6443端口来实现管理集群的；上面生成了访问api的配置文件，接着就是使用它 1.4、访问k8s集群12345678910111213141516171819202122232425[root@k8s-node-91 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; [root@k8s-node-91 ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.10.10.1 &lt;none&gt; 443/TCP 1dnginx NodePort 10.10.10.61 &lt;none&gt; 88:37224/TCP 3h[root@k8s-node-91 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-8586cf59-c2j24 1/1 Running 0 3hnginx-8586cf59-hxcq9 1/1 Running 0 3hnginx-8586cf59-vm6x6 1/1 Running 0 3h[root@k8s-node-91 ~]# kubectl get nsNAME STATUS AGEdefault Active 1dkube-public Active 1dkube-system Active 1d[root@k8s-node-91 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSION10.0.10.91 Ready &lt;none&gt; 1d v1.9.510.0.10.92 Ready &lt;none&gt; 1d v1.9.5 是的，经过如上步骤，你可以配置多个kubectl客户端来管理线上的k8s集群，做到kubectl高可用。 当然，为了方便部署，你也可以将以上操作进行打包，实现在下一个节点上的快速部署。12345678[root@k8s-node-91 ~]# tar zcf kubectl-admin.tar.gz .kube/config /opt/kubernetes/ssl/&#123;admin-key.pem,admin.pem,ca.pem&#125; /opt/kubernetes/bin/kubectl tar: 从成员名中删除开头的“/”[root@k8s-node-91 ~]# tar tf kubectl-admin.tar.gz # 直接部署这个文件即可.kube/configopt/kubernetes/ssl/admin-key.pemopt/kubernetes/ssl/admin.pemopt/kubernetes/ssl/ca.pemopt/kubernetes/bin/kubectl 二、kubectl命令2.1、kubectl命令分类表命令 描述 create 通过文件或标准输入创建资源（常用） expose 将一个资源公开为一个新的Service run 在集群中运行一个特定的镜像 set 为objects设置一个指定的特征 get 显示一个或多个资源（常用） explain 查看资源的文档 edit 在服务器上编辑一个资源 delete 通过文件名、标准输入、资源名称或标签选择器来删除资源（常用） 2.1.2、部署命令命令 描述 rollout 管理资源的发布 rolling-update 对指定的复制控制器（rs）滚动升级 scale 扩容或缩容Pod数量，Department、ReplicaSet、RC或Job autoscale 创建一个能自动调整Department、ReplicaSet、RC的副本数量 2.1.3、集群管理命令（不常用）命令 描述 certificate 修改证书资源 cluster-info 显示集群信息 top 显示资源(CPU/Memory/Storage)使用情况，需要运行Heather cordon 标记节点不可调度 uncordon 标记节点可调度 drain 维护期间排除节点 taint 更新一个或多个节点上的污点 2.1.4、故障诊断和调试命令（常用）命令 描述 describe 显示指定资源或资源组的详细信息（常用） logs 输出容器在Pod中的日志（跟docker logs功能相同） attach 附加到一个运行的容器（跟docker attach功能相同） exec 在一个容器里执行命令（跟docker attach功能相同） port-forward 转发一个或多个本地端口到一个Pod上 proxy 运行一个proxy到kubernetes API server cp 拷贝文件或目录到容器中 auth 检查授权 2.1.5、高级命令命令 描述 apply 通过文件名或标准输入对资源进行配置 patch 使用补丁修改、更新资源的字段 replace 通过filename或标准输入替换一个资源 convert 在不同的API server上转换配置文件 2.1.6、设置命令命令 描述 label 更新资源上标签 annotate 更新一个资源的注解 completion 用于实现kubectl工具自动补全 2.1.7、其他命令命令 描述 api-versions 打印支持的API版本 config 修改kubeconfig文件(用于访问API，比如配置认证信息) help 所有命令帮助（最应该学会的命令参数） plugin 运行一个命令行插件 version 打印client和server版本信息 2.2、kubectl命令实践2.2.1、创建12$ kubectl run nginx --replicas=3 --labels=&quot;app=example&quot; --image=nginx:1.10 --port=80$ kubectl create xxxx-deployment.yaml 2.2.2、查看1234$ kubectl get deploy$ kubectl get pods --show-labels$ kubectl get pods -l app=exampl$ kubectl get pods -o wide 2.2.3、发布12$ kubectl expose deployment nginx --port=88 --type=NodePort --target-port=80 --name=nginx-service$ kubectl describe service nginx-service 2.2.4、故障排查123$ kubectl describe TYPE NAME_PREFIX$ kubectl logs nginx-xxx$ kubectl exec -it nginx-xxx bash 2.2.5、更新12345$ kubectl set image deployment/nginx nginx=nginx:1.11$ kubectl set image deployment/nginx nginx=nginx:1.12 --record # 滚动升级，记录操作命令（生产必须使用）or$ kubectl edit deployment/nginx 2.2.6、回滚12345$ kubectl rollout history deploy/nginx # 查看更新历史记录$ kubectl rollout status deploy/nginx # 查看更新状态$ kubectl rollout undo deployment/nginx-deployment # 回滚到前一个版本$ kubectl rollout undo deployment/nginx-deployment --to-revision NUM # 指定回滚到哪个版本 2.2.7、删除12$ kubectl delete deploy/nginx$ kubectl delete svc/nginx-service 3、Pod扩容Usage: kubectl scale [–resource-version=version] [–current-replicas=count] –replicas=COUNT (-f FILENAME | TYPE NAME) 1、将名为foo的副本缩容到31kubectl scale --replicas=3 rs/foo 2、从yaml文件中进行缩容1kubectl scale --replicas=3 -f foo.yaml 3、如果deployment名为mysql的当前副本集是2，那么扩容到31kubectl scale --current-replicas=2 --replicas=3 deployment/mysql 参考文章： Kubernetes-kubectl命令行工具]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Kubectl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《使用Kubeadm快速部署集群》]]></title>
    <url>%2F2019%2F06%2F14%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BD%BF%E7%94%A8Kubeadm%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4%E3%80%8B.html</url>
    <content type="text"><![CDATA[kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。本文将实战Kubeadm完整部署一套Kubernetes集群，这种部署方式官网在现在以及将来也可用于生产环境使用了。但如果你想快速体验Kubernetes集群， 不妨采用Kubeadm试试！ 这几个工具能通过两条指令完成一个kubernetes集群的部署：12345# 创建一个Master节点$ kubeadm init# 将一个Node节点加入到当前集群中$ kubeadm join &lt;Master节点的IP和端口&gt; 1、安装要求在开始之前，部署kubernetes集群机器需要满足一下几个条件： 一台或多台机器，操作系统CentOS7.x-86_x64； 硬件配置：2GB或更多内存，2个CPU或更多，30G硬盘或更； 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止使用swap分区 关闭iptables防火墙 做好时间定期同步 2、部署进程 在所有节点上安装docker和kubeadm 部署kubernetes Master 部署容器网络插件 部署kubernetes Node，将节点加入Kubernetes集群中 部署Dashboard Web页面，可视化查看Kubernetes资源 3、环境准备关闭防火墙：12$ systemctl stop firewalld$ systemctl disable firewalld 关闭selinux：1234$ vim /etc/selinux/configSELINUX=disabled# 临时禁用selinux$ setenforce 0 关闭swap：12$ swapoff -a$ swapon -s # 没有分区信息即可。 添加主机对应IP对应关系1234vim /etc/hosts172.16.194.128 k8s-master-128172.16.194.129 k8s-node-129172.16.194.130 k8s-node-130 根据上面主机规范请自行更改主机名哈。 将桥接的IPv4流量传递到iptables的链：12345$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl --system 配置时间同步：123$ crontab -e# 时间同步*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt;/dev/null 2&gt;&amp;1 4、所有节点安装Docker/kubeadm/kubeletKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。 4.1、安装Docker123456789101112131415$ yum install -y yum-utils \device-mapper-persistent-data \lvm2# 官网的repo源在中国用不了，咱们还是乖乖使用马爸爸提供的源好了$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo$ yum makecache fast$ yum install docker-ce -y$ mkdir /etc/docker/$ cat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;EOF$ systemctl enable docker &amp;&amp; systemctl start docker 4.2、添加阿里云YUM软件源1234567$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0EOF 4.3、安装kubeadm，kubelet和kubectl由于版本更新频繁，指定版本号进行部署：12$ yum install -y kubelet-1.14.1 kubeadm-1.14.1 kubectl-1.14.1$ systemctl enable kubelet 5、部署kubernetes Master使用kubeadm init来安装Master节点123456$ kubeadm init \--apiserver-advertise-address=172.16.194.128 \--image-repository registry.aliyuncs.com/google_containers \--kubernetes-version v1.14.1 \--service-cidr=10.1.0.0/16 \--pod-network-cidr=10.244.0.0/16 安装成功后提示：12345678910111213141516Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172.16.194.128:6443 --token zbqk3l.fe1dywo3ufnrcwff \ --discovery-token-ca-cert-hash sha256:a7497e212a62f6bd87b4f3b1fb23333fb725c9958990a54bed131185d667e9ed 非常顺利的就部署成功了，如果我们想使用非root用户操作kubectl，可以使用以下命令，这也是kubeadm init输出的一部分：123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 此时Master节点安装并启动了如下组件： kubelet kube-scheduler kube-controller-manager kube-apiserver etcd 通过ps -ef|grep kube能看到进程信息通过docker image 和 docker ps 能看到镜像与启动的容器 6、安装Pod网络插件（CNL）为了让Pods间可以相互通信，我们必须安装一个网络插件，并且必须在部署任何应用之前安装，CoreDNS也是在网络插件安装之后才会启动的。 网络的插件完整列表，请参考: Networking and Network Policy 在安装之前，我们先查看一下当前Pods的状态：123456789[root@k8s-master-128 ~]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-8686dcc4fd-84dfh 0/1 Pending 0 18mkube-system coredns-8686dcc4fd-ldw8r 0/1 Pending 0 18mkube-system etcd-k8s-master-128 1/1 Running 0 18mkube-system kube-apiserver-k8s-master-128 1/1 Running 0 18mkube-system kube-controller-manager-k8s-master-128 1/1 Running 0 17mkube-system kube-proxy-qqchb 1/1 Running 0 18mkube-system kube-scheduler-k8s-master-128 1/1 Running 0 18m 如上，可以看到CoreDND的状态是Pending，这是因为我们还没有安装网络插件。 默认情况下，flannel网络插件使用的的网段是10.244.0.0/16，在init的时候，我们保持了默认配置–pod-network-cidr=10.244.0.0/16，当然你也可以修改flannel.yml文件来指定不同的网段。 可以使用如下命令命令来安装flannel插件：1234$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml# 通过wget 下载kube-flannel.yml文件，发现里面有如下镜像所国外源,可以手动docker pull 尝试拉取，如果拉取失败则需要更换为国内的镜像才能用。$ docker pull quay.io/coreos/flannel:v0.11.0-amd64 稍等片刻，再使用kubectl get pods –all-namespaces命令来查看网络插件的安装情况：12345678910[root@k8s-master-128 ~]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-8686dcc4fd-84dfh 1/1 Running 0 33mkube-system coredns-8686dcc4fd-ldw8r 1/1 Running 0 33mkube-system etcd-k8s-master-128 1/1 Running 0 32mkube-system kube-apiserver-k8s-master-128 1/1 Running 0 32mkube-system kube-controller-manager-k8s-master-128 1/1 Running 0 32mkube-system kube-flannel-ds-amd64-wsrbp 1/1 Running 0 7m48skube-system kube-proxy-qqchb 1/1 Running 0 33mkube-system kube-scheduler-k8s-master-128 1/1 Running 0 32m 如上，STATUS全部变为了Running，表示安装成功，接下来就可以加入其他节点以及部署应用了。 7、加入Kubernetes Node如果我们忘记了Master节点的加入token，可以使用如下命令来查看：123[root@k8s-master-128 ~]# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSzbqk3l.fe1dywo3ufnrcwff 23h 2019-05-17T11:28:25+08:00 authentication,signing The default bootstrap token generated by &apos;kubeadm init&apos;. system:bootstrappers:kubeadm:default-node-token 默认情况下，token的有效期是24小时，如果我们的token已经过期的话，可以使用以下命令重新生成：123[root@k8s-master-128 ~]# kubeadm token create# 输出4j6te5.bja8setduvotvjm7 如果我们也没有–discovery-token-ca-cert-hash的值，可以使用以下命令生成：123[root@k8s-master-128 ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;# 输出a7497e212a62f6bd87b4f3b1fb23333fb725c9958990a54bed131185d667e9ed 现在，我们登录到工作节点服务器，然后运行如下命令加入集群（这也是上面init输出的一部分）：123456789101112131415161718$ kubeadm join 172.16.194.128:6443 --token 4j6te5.bja8setduvotvjm7 --discovery-token-ca-cert-hash sha256:a7497e212a62f6bd87b4f3b1fb23333fb725c9958990a54bed131185d667e9ed# 输出 [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [WARNING Service-Kubelet]: kubelet service is not enabled, please run &apos;systemctl enable kubelet.service&apos;[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.14&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &apos;kubectl get nodes&apos; on the control-plane to see this node join the cluster. 等待一会，我们可以在Master节点上使用kubectl get nodes命令来查看节点的状态：12345[root@k8s-master-128 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master-128 Ready master 47m v1.14.1k8s-node-129 Ready &lt;none&gt; 4m10s v1.14.1k8s-node-130 Ready &lt;none&gt; 99s v1.14.1 如上全部Ready，大功告成，我们可以运行一些命令来测试一下集群是否正常。 8、测试Kubernetes集群首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常：123456789101112# 部署一个 Nginx Deployment，包含两个Pod# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/$ kubectl create deployment nginx --image=nginx:alpine$ kubectl scale deployment nginx --replicas=2# 验证Nginx Pod是否正确运行，并且会分配192.168.开头的集群IP$ kubectl get pods -l app=nginx -o wide# 输出NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-77595c695-w6smk 1/1 Running 0 67s 10.244.2.2 k8s-node-130 &lt;none&gt; &lt;none&gt;nginx-77595c695-xnhps 1/1 Running 0 72s 10.244.1.2 k8s-node-129 &lt;none&gt; &lt;none&gt; 再验证一下kube-proxy是否正常：12345678910111213# 以 NodePort 方式对外提供服务 https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/$ kubectl expose deployment nginx --port=80 --type=NodePort# 查看集群外可访问的Port$ kubectl get svc nginx# 输出NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx NodePort 10.1.42.3 &lt;none&gt; 80:31319/TCP 21s# 可以通过任意 NodeIP:Port 在集群外部访问这个服务，本示例中部署的2台集群IP分别是172.16.194.129和172.16.194.130curl http://172.16.194.129:31319curl http://172.16.194.130:31319 最后验证一下dns, pod network是否正常：12345678910111213141516171819202122# 运行Busybox并进入交互模式kubectl run -it curl --image=radial/busyboxplus:curl# 输入`nslookup nginx`查看是否可以正确解析出集群内的IP，已验证DNS是否正常[ root@curl-66bdcf564-rxkpp:/ ]$ nslookup nginx# 输出Server: 10.1.0.10Address 1: 10.1.0.10 kube-dns.kube-system.svc.cluster.localName: nginxAddress 1: 10.1.42.3 nginx.default.svc.cluster.local# 通过服务名进行访问，验证kube-proxy是否正常[ root@curl-66bdcf564-rxkpp:/ ]$ curl http://nginx/# 输出如下： &lt;!DOCTYPE html&gt; ---省略# 分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常[ root@curl-66bdcf564-rxkpp:/ ]$ curl http://10.244.2.2[ root@curl-66bdcf564-rxkpp:/ ]$ curl http://10.244.1.2 验证通过，集群搭建成功，接下来我们就可以参考官方文档来部署其他服务，愉快的玩耍了。 9、部署Dashboard官网有安装文件：https://github.com/kubernetes/dashboard123456789101112131415161718192021222324252627$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml# 默认镜像国内无法访问，将文件下载下来，修改镜像：$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml$ vim kubernetes-dashboard.yaml # 搜索关键字：imageimage: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1# 将这里的官网镜像更改成国内镜像# 并且在Dashboard Service 配置项里（配置文件末尾）增加NodePort，对外暴露访问：# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort # 增加了这一项配置 ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard# 然后执行部署$ kubectl apply -f kubernetes-dashboard.yaml 等待一会，使用kubeadm get查看pod启动情况：123456789101112131415[root@k8s-master-128 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-8686dcc4fd-84dfh 1/1 Running 0 3h32mcoredns-8686dcc4fd-ldw8r 1/1 Running 0 3h32metcd-k8s-master-128 1/1 Running 0 3h31mkube-apiserver-k8s-master-128 1/1 Running 0 3h31mkube-controller-manager-k8s-master-128 1/1 Running 0 3h31mkube-flannel-ds-amd64-qs89g 1/1 Running 0 167mkube-flannel-ds-amd64-tsj6r 1/1 Running 0 170mkube-flannel-ds-amd64-wsrbp 1/1 Running 0 3h7mkube-proxy-bmwms 1/1 Running 0 170mkube-proxy-fwphm 1/1 Running 0 167mkube-proxy-qqchb 1/1 Running 0 3h32mkube-scheduler-k8s-master-128 1/1 Running 0 3h32mkubernetes-dashboard-5d9599dc98-bdg8q 1/1 Running 0 94s pod已经运行起来了，继续查看暴露的服务：1234[root@k8s-master-128 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.1.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 3h38mkubernetes-dashboard NodePort 10.1.239.63 &lt;none&gt; 443:31459/TCP 2m54s dashboard已经运行起来并且对外提供服务。 访问UI界面：https://Node节点IP:31459（节点IP之间，能负载均衡，所有随便访问哪个节点IP+端口 都能访问到UI）https://172.16.194.130:31459 创建Service account并绑定默认cluster-admin管理员集群角色：123456$ kubectl create serviceaccount dashboard-admin -n kube-system$ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin$ kubectl describe secrets -n kube-system $(kubectl get secrets -n kube-system |awk &apos;/dashboard-admin/&#123;print $1&#125;&apos;)输出：token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tNDZzbnoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTg0ODVkNTctNzdhYy0xMWU5LThiNDQtMDAwYzI5ZjRkYWE5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.BaJSW1taC633s6ZEqlnkSfA-L7c59VHUIfUEzaIK8CqiU64R1b1zgsQYiIjiIX465pZoWEeR2ao7_NiTtpP3E1dK8dZzGRhIwLvTrvTUJLj3p1YNDkM3z-kd7OWyo6pgP3yf3RdEWqhXGidf5_2GORF-9dSp3PJghuHc5CZVMu_64-PeT7ZTtyPvcMJw7b8UfW39K7DPzr36liT0y-vZNvmO17Gn-Q4KgIxxTzIwd5zwisXIA7qGY8bf9P6VYymfiacov0jKp3_KlLyrUTeit5OYIOVLuFLak22mPutPYuFre3YFnEgycRSj_QW9QynXms1OEvA3TgEVXgWQytG1rQ 将这个token放入Dashboard认证，即可登录到UI页面。 10、卸载集群想要撤销kubeadm执行的操作，首先要排除节点，并确保该节点为空, 然后再将其关闭。 在Master节点上运行：12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 然后在需要移除的节点上，重置kubeadm的安装状态：1sudo kubeadm reset 如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。 参考文章： https://my.oschina.net/Kanonpy/blog/3006129 http://www.mamicode.com/info-detail-2544943.html]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《集群部署（下）》.md]]></title>
    <url>%2F2019%2F06%2F11%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%EF%BC%88%E4%B8%8B%EF%BC%89%E3%80%8B.html</url>
    <content type="text"><![CDATA[本文继承上一篇文章《集群部署（上）》, 继续Kubernetes集群实战本文将实现： 完整部署一套Kubernetes 部署Kubernetes UI（Dashboard） 部署多Master节点的Kubernetes集群 部署Kubernetes集群内部DNS（CoreDNS ） 七、部署Master组件根据环境规划，Master节点应有以下组件： kube-apiserver kube-controller-manager kube-sheduler etcd etcd在上面已经部署完成，本文里Master节点是k8s-master-128，那该节点就剩下其他三个组件需部署。 7.1、获取二进制包官网：https://kubernetes.io/，Github: https://github.com/kubernetes/kubernetes/releases二进制包：https://dl.k8s.io/v1.14.0/kubernetes-server-linux-amd64.tar.gz 注意：二进制包比较大，400M大小，且有墙，我这里手动下载然后上传到服务器； 7.2、部署二进制文件1234567891011# 部署Master组件[root@k8s-master-128 ~]# cd soft/[root@k8s-master-128 soft]# tar zxf kubernetes-server-linux-amd64.tar.gz[root@k8s-master-128 soft]# cp kubernetes/server/bin/&#123;kubectl,kube-scheduler,kube-controller-manager,kube-apiserver&#125; /opt/kubernetes/bin/[root@k8s-master-128 soft]# echo &quot;export PATH=$PATH:/opt/kubernetes/bin&quot; &gt;&gt;/etc/profile[root@k8s-master-128 soft]# source /etc/profile[root@k8s-master-128 soft]# which kubectl/opt/kubernetes/bin/kubectl# 部署Node组件（方便后面使用）[root@k8s-master-128 soft]# scp kubernetes/server/bin/&#123;kubelet,kube-proxy&#125; k8s-node-129:/opt/kubernetes/bin/ 7.3、生成Kubeconfig文件7.3.1、api-server认证方式这个是知识了解，api-server有如下几种认证方式： CA证书认证 Token认证：token-auth 基本认证：basic-auth kubernetes 认证主要分为上面三种，可以同时配置多种认证方式，只要其中任意一个方式认证通过即可。 参考： Kubernetes 的安全机制 APIServer 认证、授权、准入控制 Kubernetes apiserver认证 本次部署使用第二种：Token认证。 7.3.2、创建配置文件需要创建以下三个配置文件。 1、创建TLS Bootstrapping Token 2、创建kubelet kubeconfig 3、创建kube-proxy kubeconfig 我们使用一个脚本将所有的配置文件一起创建出来12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master-128 ~]# mkdir ssl/k8s-cret[root@k8s-master-128 ~]# cd ssl/k8s-cret/[root@k8s-master-128 k8s-cret]# vim kubeconfig.sh# 创建 TLS Bootstrapping Tokenexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;) # 生成随机token（随机字符串）cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF#----------------------# 创建kubelet bootstrapping kubeconfigexport KUBE_APISERVER=&quot;https://172.16.194.128:6443&quot; # 这个脚本唯一需要更改的就是这个地方，填写成你的Master IP地址# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=./ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig#----------------------# 创建kube-proxy kubeconfig文件kubectl config set-cluster kubernetes \ --certificate-authority=./ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \ --client-certificate=./kube-proxy.pem \ --client-key=./kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 执行脚本，创建配置文件：12345678910[root@k8s-master-128 k8s-cret]# chmod +x kubeconfig.sh[root@k8s-master-128 k8s-cret]# ./kubeconfig.shCluster &quot;kubernetes&quot; set.User &quot;kubelet-bootstrap&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;.Cluster &quot;kubernetes&quot; set.User &quot;kube-proxy&quot; set.Context &quot;default&quot; created.Switched to context &quot;default&quot;. 配置文件的用途： 总共创建了三个文件，用途分别为： bootstrap.kubeconfig # Node节点的kubelet组件使用 kube-proxy.kubeconfig # Node节点的kube-proxy组件使用 token.csv # Master节点的apiserver组件使用 根据上面的用途说明，将生成的文件推送到各个节点的目录中（以备后面部署组件使用）：123456# Master节点只需要token.csv[root@k8s-master-128 k8s-cret]# cp token.csv /opt/kubernetes/cfg/# Node节点推送文件[root@k8s-master-128 k8s-cret]# scp bootstrap.kubeconfig kube-proxy.kubeconfig k8s-node-129:/opt/kubernetes/cfg/[root@k8s-master-128 k8s-cret]# scp bootstrap.kubeconfig kube-proxy.kubeconfig k8s-node-130:/opt/kubernetes/cfg/ 7.3.3、查看配置文件token.csv 12[root@k8s-master-128 k8s-cret]# cat token.csv38435b41e3861251dce8c2cbf968ca67,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; 注解： 38435b41e3861251dce8c2cbf968ca67：随机生成的token，也可以固定一个Token来使用 kubelet-bootstrap ：用户名 10001：用户ID system:kubelet-bootstrap：用户组 bootstrap.kubeconfig12345678910111213141516171819[root@k8s-master-128 k8s-cret]# cat bootstrap.kubeconfigapiVersion: v1 # api的版本clusters: # 集群的内容- cluster: certificate-authority-data: # 集群CA数字证书，有很大一堆，略······ server: https://172.16.194.128:6443 # server的地址 name: kubernetes # 集群的名字contexts: # 上下文内容- context: cluster: kubernetes user: kubelet-bootstrap # k8s用户（可改变，是从token.csv里定义的） name: default # 上下文名称current-context: default # 当前默认使用的上下文kind: Configpreferences: &#123;&#125;users: # 用户的信息- name: kubelet-bootstrap # 用户名 user: token: 38435b41e3861251dce8c2cbf968ca67 # token，关键！必须要与token.csv里的token对应！不对应则没有相应的权限，会认证失败。 kube-proxy.kubeconfig1234567891011121314151617181920[root@k8s-master-128 k8s-cret]# cat kube-proxy.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority-data: # 集群CA数字证书，有很大一堆，略······ server: https://192.16.194.128:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: client-certificate-data: # 客户端CA数字证书，有很大一堆，略······ client-key-data: # 客户端key证书，有很大一堆，略······ 7.4、启动Master组件7.4.1、启动kube-apiserver1、启动的脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master-128 ~]# cat apiserver.sh #!/bin/bashMASTER_ADDRESS=$&#123;1:-&quot;192.168.1.195&quot;&#125;ETCD_SERVERS=$&#123;2:-&quot;http://127.0.0.1:2379&quot;&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=&quot;--logtostderr=false \--log-dir=/opt/kubernetes/logs \--v=4 \--etcd-servers=$&#123;ETCD_SERVERS&#125; \--insecure-bind-address=127.0.0.1 \--bind-address=$&#123;MASTER_ADDRESS&#125; \--insecure-port=8080 \--secure-port=6443 \--advertise-address=$&#123;MASTER_ADDRESS&#125; \--allow-privileged=true \--service-cluster-ip-range=10.0.0.0/24 \--service-node-port-range=30000-50000 \--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--kubelet-https=true \--enable-bootstrap-token-auth \--token-auth-file=/opt/kubernetes/cfg/token.csv \--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 2、启动apiserver1234567# apiserver.sh 接收两个参数，第一个是apiserver的地址，第二个是ETCD集群的地址，我们集群有三个节点，都填写上。[root@k8s-master-128 ~]# chmod +x apiserver.sh[root@k8s-master-128 ~]# ./apiserver.sh 172.16.194.128 https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379[root@k8s-master-128 ~]# systemctl daemon-reload[root@k8s-master-128 ~]# systemctl enable kube-apiserver[root@k8s-master-128 ~]# systemctl start kube-apiserver[root@k8s-master-128 ~]# systemctl status kube-apiserver 3、查看启动状态123456[root@k8s-master-128 logs]# ls -lh /opt/kubernetes/logs/ # 详细日志[root@k8s-master-128 logs]# netstat -lntup|grep kubetcp 0 0 172.16.194.128:6443 0.0.0.0:* LISTEN 28201/kube-apiservetcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 28201/kube-apiserve[root@k8s-master-128 logs]# ps -ef|grep kube-apiserver# 信息太多，不贴了 注意： 如果你照着上面的脚本执行后，如没有进程存活，你应该查看下日志和检查上面的证书都是否存在。 一定要先启动apiserver！因为后面两个组件依赖apiserver，controller-manager和scheduler可以没有先后顺序。 7.4.2、启动kube-controller-manager1、启动的脚本12345678910111213141516171819202122232425262728293031323334[root@k8s-master-128 ~]# cat controller-manager.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-&quot;127.0.0.1&quot;&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=false \--log-dir=/opt/kubernetes/logs \--v=4 \--master=$&#123;MASTER_ADDRESS&#125;:8080 \--leader-elect=true \--address=127.0.0.1 \--service-cluster-ip-range=10.0.0.0/24 \--cluster-name=kubernetes \--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \--root-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \--experimental-cluster-signing-duration=87600h0m0s&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 2、启动controller-manager12345[root@k8s-master-128 ~]# chmod +x controller-manager.sh[root@k8s-master-128 ~]# ./controller-manager.sh[root@k8s-master-128 ~]# systemctl daemon-reload[root@k8s-master-128 ~]# systemctl enable kube-controller-manager[root@k8s-master-128 ~]# systemctl start kube-controller-manager 启动失败请检查这两个文件是否配置正确，以及启动日志:/opt/kubernetes/cfg/kube-controller-manager/usr/lib/systemd/system/kube-controller-manager.service/opt/kubernetes/logs/ 3、查看启动状态12345# controller-manager启动的端口是10252[root@k8s-master-128 ~]# netstat -lntup|grep kube-ctcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 7699/kube-controlletcp6 0 0 :::10257 :::* LISTEN 7699/kube-controlle[root@k8s-master-128 ~]# ps -ef|grep controller-manager 7.4.3、启动kube-sheduler1、启动的脚本1234567891011121314151617181920212223242526[root@k8s-master-128 ~]# cat scheduler.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-&quot;127.0.0.1&quot;&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=&quot;--logtostderr=false \--log-dir=/opt/kubernetes/logs \--v=4 \--master=$&#123;MASTER_ADDRESS&#125;:8080 \--leader-elect&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 2、启动scheduler12345[root@k8s-master-128 ~]# chmod +x scheduler.sh[root@k8s-master-128 ~]# ./scheduler.sh[root@k8s-master-128 ~]# systemctl daemon-reload[root@k8s-master-128 ~]# systemctl enable kube-scheduler[root@k8s-master-128 ~]# systemctl start kube-scheduler 3、查看启动状态12345# scheduler启动的端口是10251[root@k8s-master-128 ~]# netstat -lntup|grep kube-sctcp6 0 0 :::10251 :::* LISTEN 7897/kube-schedulertcp6 0 0 :::10259 :::* LISTEN 7897/kube-scheduler[root@k8s-master-128 ~]# ps -ef|grep scheduler 7.5、查看组件运行状态kubectl get cs # 查看k8s集群资源的健康信息相关阅读：Kubernetes kubectl get 命令详解1234567[root@k8s-master-128 ~]# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 看到以上信息都为健康状态，到此，Kubernetes集群的Master节点即部署完成。 八、部署Node组件根据前面环境规划，Node节点将启动kubelet和kube-proxy组件。 8.1、创建Token租户并绑定角色这是我们创建的tuken信息，需要把租户信息创建出来，Node节点在部署kubelet组件时需要通过token租户进行权限验证123456789[root@k8s-master-128 ~]# kubectl create --help[root@k8s-master-128 ~]# kubectl create clusterrolebinding --helpUsage: kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname][--serviceaccount=namespace:serviceaccountname] [--dry-run] [options][root@k8s-master-128 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrapclusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created[root@k8s-master-128 ~]# kubectl get clusterrole # 查看集群角色（system:node-bootstrapper由此得来） 8.2、部署Kubelet8.2.1、启动Kubelet组件1、启动的脚本12345678910111213141516171819202122232425262728293031323334353637[root@k8s-node-129 ~]# cat kubelet.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-&quot;127.0.0.1&quot;&#125; # 节点的IP地址，通过参数传入DNS_SERVER_IP=$&#123;2:-&quot;10.0.0.2&quot;&#125; # DNS地址，在apiserver.sh里指定的地址端，可以写10.10.10.0/24段的任意IPcat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kubeletKUBELET_OPTS=&quot;--logtostderr=false \--log-dir=/opt/kubernetes/logs \--v=4 \--address=$&#123;NODE_ADDRESS&#125; \--hostname-override=$&#123;NODE_ADDRESS&#125; \--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \--experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \--cert-dir=/opt/kubernetes/ssl \--allow-privileged=true \--cluster-dns=$&#123;DNS_SERVER_IP&#125; \--cluster-domain=cluster.local \--fail-swap-on=false \--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/kubeletExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.targetEOF 2、启动kubelet123456789# 两个Node节点同时启动kubelet[root@k8s-node-129 ~]# chmod +x kubelet.sh[root@k8s-node-129 ~]# ./kubelet.sh 172.16.194.129[root@k8s-node-130 ~]# ./kubelet.sh 172.16.194.130# 启动kubeletsystemctl daemon-reloadsystemctl enable kubeletsystemctl start kubelet 3、查看启动状态123[root@k8s-node-129 ~]# ps -ef|grep kubelet[root@k8s-node-129 ~]# ls -lh /opt/kubernetes/cfg/kubelet.kubeconfigls: 无法访问/opt/kubernetes/cfg/kubelet.kubeconfig: 没有那个文件或目录 注意：你会发现没有kubelet.kubeconfig文件，这是为什么呢？是因为k8s-Master需要给Node节点的kubelet组件颁发证书，Node节点才会生成这个证书文件。 承上，我们继续到Master节点看看证书请求状况:1234[root@k8s-master-128 ~]# kubectl get csr # 查看Node节点的kubelet证书申请请求NAME AGE REQUESTOR CONDITIONnode-csr-K50rilZHx2Gn_MHesRDr_wIJ5hOrg3buFjWuq3RNRhE 3m45s kubelet-bootstrap Pendingnode-csr-xW7Q15gU-88wPvObRXmia6y-eMyx5dbcMHbGIwINzD0 15m kubelet-bootstrap Pending # 等待颁发状态 8.2.2、k8s集群为kubelet颁发证书1234567891011121314151617181920[root@k8s-master-128 ~]# kubectl --help|grep certificate certificate 修改 certificate 资源.[root@k8s-master-128 ~]# kubectl certificate --help approve 同意一个自签证书请求 deny 拒绝一个自签证书请求[root@k8s-master-128 ~]# kubectl certificate approve --helpUsage: kubectl certificate approve (-f FILENAME | NAME) [options]# 上面几步help我们找到了用法[root@k8s-master-128 ~]# kubectl certificate approve node-csr-K50rilZHx2Gn_MHesRDr_wIJ5hOrg3buFjWuq3RNRhEcertificatesigningrequest.certificates.k8s.io/node-csr-K50rilZHx2Gn_MHesRDr_wIJ5hOrg3buFjWuq3RNRhE approved[root@k8s-master-128 ~]# kubectl certificate approve node-csr-xW7Q15gU-88wPvObRXmia6y-eMyx5dbcMHbGIwINzD0certificatesigningrequest.certificates.k8s.io/node-csr-xW7Q15gU-88wPvObRXmia6y-eMyx5dbcMHbGIwINzD0 approved[root@k8s-master-128 ~]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-K50rilZHx2Gn_MHesRDr_wIJ5hOrg3buFjWuq3RNRhE 7m8s kubelet-bootstrap Approved,Issuednode-csr-xW7Q15gU-88wPvObRXmia6y-eMyx5dbcMHbGIwINzD0 18m kubelet-bootstrap Approved,IssuedCRGvCskSEnG13F4BPrqQbDiR2epHRg4 7m kubelet-bootstrap Approved,Issued # 同意办法证书后，变为：批准状态。 我这里有两个Node节点，都为其颁发证书允许加入集群。 8.2.3、查看Node节点加入k8s集群1234[root@k8s-master-128 ~]# kubectl get nodes # 查看集群中的Node节点信息，Ready表示该节点健康，已准备就绪NAME STATUS ROLES AGE VERSION172.16.194.129 Ready &lt;none&gt; 54s v1.14.0172.16.194.130 Ready &lt;none&gt; 63s v1.14.0 8.2.4、查看Node节点签发的证书123456789[root@k8s-node-129 ~]# ls -lh /opt/kubernetes/cfg/kubelet.kubeconfig # 已经有这个文件了，你可以cat查看内容细节-rw------- 1 root root 2.3K 5月 7 17:04 /opt/kubernetes/cfg/kubelet.kubeconfig# ssl目录生成如下证书[root@k8s-node-129 ~]# ls -lh /opt/kubernetes/ssl/kubelet*-rw------- 1 root root 1.3K 5月 7 17:04 /opt/kubernetes/ssl/kubelet-client-2019-05-07-17-04-27.pemlrwxrwxrwx 1 root root 58 5月 7 17:04 /opt/kubernetes/ssl/kubelet-client-current.pem -&gt; /opt/kubernetes/ssl/kubelet-client-2019-05-07-17-04-27.pem-rw-r--r-- 1 root root 2.2K 5月 7 16:36 /opt/kubernetes/ssl/kubelet.crt-rw------- 1 root root 1.7K 5月 7 16:36 /opt/kubernetes/ssl/kubelet.key 整个自签发证书颁发流程完结。 8.3、部署kube-proxy1、启动的脚本12345678910111213141516171819202122232425[root@k8s-node-129 ~]# cat kube-proxy.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-&quot;127.0.0.1&quot;&#125; # kube-proxy的节点IPcat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=&quot;--logtostderr=false \--log-dir=/opt/kubernetes/logs \--v=4 \--hostname-override=$&#123;NODE_ADDRESS&#125; \--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \$KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF 2、启动kube-proxy12345678910# 两个Node节点都启动kube-proxy[root@k8s-node-129 ~]# chmod +x kube-proxy.sh[root@k8s-node-129 ~]# ./kube-proxy.sh 172.16.194.129[root@k8s-node-130 ~]# ./kube-proxy.sh 172.16.194.130# 启动kube-proxysystemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxy 3、查看启动状态12[root@k8s-node-129 ~]# ps -ef|grep kube-proxy[root@k8s-node-130 ~]# systemctl status kube-proxy 九、部署一个测试示例9.1、创建一个Nginx pods12345678[root@k8s-master-128 ~]# kubectl run --help # 能看到有很多用法[root@k8s-master-128 ~]# kubectl run nginx --image=nginx[root@k8s-master-128 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7db9fccd9b-kckml 0/1 ContainerCreating 0 45s # 第一次创建K8s会在Node节点上拉取镜像，启动时间稍长；[root@k8s-master-128 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7db9fccd9b-kckml 1/1 Running 0 3m57s 查看pod被分配到了哪个节点上：123[root@k8s-master-128 ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-7db9fccd9b-bhldc 1/1 Running 0 7m7s 172.17.64.2 172.16.194.129 &lt;none&gt; &lt;none&gt; 9.2、创建一个services用于将pod封装成一个service，提供外界访问；1234567[root@k8s-master-128 ~]# kubectl expose --help[root@k8s-master-128 ~]# kubectl expose deployment nginx --port=80 --target-port=80 --type=NodePortservice/nginx exposed[root@k8s-master-128 ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 22hnginx NodePort 10.0.0.86 &lt;none&gt; 80:43364/TCP 13s 9.3、访问services12345678[root@k8s-node-129 ~]# curl 10.0.0.86 # 对内访问[root@k8s-master-128 ~]# curl 172.16.194.129:43364 # 对外访问# 查看访问日志[root@k8s-master-128 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7db9fccd9b-bhldc 1/1 Running 0 13m[root@k8s-master-128 ~]# kubectl logs nginx-7db9fccd9b-bhldc 十、部署Web UI（Dashboard）部署UI这部分网上有很多方法，如果下文有你看不懂的地方，可以不用参考我这里的方案 在kubernetes源码里有关于Dashboard的部署文件：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboardhttps://github.com/kubernetes/dashboard 文件的细节这里不展开解释，需要具备一定阅读能力。 10.1、获取YAML文件从源码包获取文件123456789[root@k8s-master-128 ~]# cd /root/soft/kubernetes[root@k8s-master-128 kubernetes]# tar zxf kubernetes-src.tar.gz[root@k8s-master-128 kubernetes]# ls -lh cluster/addons/dashboard/ # 在源码包里获取这些文件总用量 32K-rw-rw-r-- 1 root root 264 3月 21 13:51 dashboard-configmap.yaml-rw-rw-r-- 1 root root 1.8K 3月 21 13:51 dashboard-controller.yaml-rw-rw-r-- 1 root root 1.4K 3月 21 13:51 dashboard-rbac.yaml-rw-rw-r-- 1 root root 551 3月 21 13:51 dashboard-secret.yaml-rw-rw-r-- 1 root root 322 3月 21 13:51 dashboard-service.yaml 10.2、更改YAML文件dashboard-controller配置文件里引用的国外镜像源，因此需要更改成国内的源才可启动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@k8s-master-128 kubernetes]# cd cluster/addons/dashboard[root@k8s-master-128 dashboard]# vim dashboard-controller.yamlapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: &apos;&apos; seccomp.security.alpha.kubernetes.io/pod: &apos;docker/default&apos; spec: priorityClassName: system-cluster-critical containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 # 将这里的官网镜像更改成国内镜像 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 50m memory: 100Mi ports: - containerPort: 8443 protocol: TCP args: # PLATFORM-SPECIFIC ARGS HERE - --auto-generate-certificates volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - name: tmp-volume mountPath: /tmp livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot; dashboard-service.yaml 文件里需要新增：type: NodePort来对外提供服务；1234567891011121314151617[root@k8s-master-128 dashboard]# cat dashboard-service.yamlapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 443 targetPort: 8443 10.3、执行部署12345678910111213[root@k8s-master-128 dashboard]# kubectl create -f dashboard-configmap.yamlconfigmap/kubernetes-dashboard-settings created[root@k8s-master-128 dashboard]# kubectl create -f dashboard-rbac.yamlrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created[root@k8s-master-128 dashboard]# kubectl create -f dashboard-secret.yamlsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-key-holder created[root@k8s-master-128 dashboard]# kubectl create -f dashboard-controller.yamlserviceaccount/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard created[root@k8s-master-128 dashboard]# kubectl create -f dashboard-service.yamlservice/kubernetes-dashboard created 查看部署情况123456789[root@k8s-master-128 dashboard]# kubectl get pods -n kube-system # 指定命名空间查看PodNAME READY STATUS RESTARTS AGEkubernetes-dashboard-7d5f7c58f5-mn44f 1/1 Running 0 2m44s[root@k8s-master-128 dashboard]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.0.0.47 &lt;none&gt; 443:34756/TCP 11s # 对外暴露了38276端口，这是能访问UI界面的端口[root@k8s-master-128 dashboard]# kubectl describe pods/kubernetes-dashboard-7d5f7c58f5-mn44f -n kube-system # 这个命令能查看pods创建过程信息 10.4、UI界面https://Node节点IP:34756（节点IP之间，能负载均衡，所有随便访问哪个节点IP+端口 都能访问到UI）https://172.16.194.130:34756 创建Service account并绑定默认cluster-admin管理员集群角色：1234567$ kubectl create serviceaccount dashboard-admin -n kube-system$ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin$ kubectl describe secrets -n kube-system $(kubectl get secrets -n kube-system |awk &apos;/dashboard-admin/&#123;print $1&#125;&apos;)输出：token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tOXA4ODciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMmRkODQyNzUtN2M2OS0xMWU5LTkzYzQtMDAwYzI5ZjRkYWE5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.e96EQXa1YIbhRXeckNthFpqNaVw7jrQFqlQTgvVIropqCGZyw_PiM9CF3F5fWwGKZmrojo1_OV4xPNEgxkA1pA-FizwBsUo6flPeVnvkxgaSw2ME6D_z0nj8rIPwyNoDe6x2mLGZNtZi4JNDu5ehoKhZqSF60-rDsoYmlwhY8WVq6uuNVSu086i25q3UU8Wz963TyRZgiywP5fTIbCfikBA5Aj7Mjar9IcCsrPGKeWOm0CxaF_IFPidMWR0scNOZfwdTHC2gU6MUxwMjAQ3KRcC1j7sNQnjXd_mPuJg96SDJsWT8T9IKaMXfXa0etb_b9F5FEZ3qAdFFsKjh-pbJ7g 将这个token放入Dashboard认证，即可登录到UI页面。 十一、部署多Master节点集群环境规划中，选定172.16.194.127来作为另一个Master节点，主要是实现kube-apiserver的负载均衡，首先对Master节点进行部署，然后使用nginx来进行对kube-apiserver进行负载代理，实现apiserver的高可用； 11.1、部署Master组件在172.16.194.128服务器上已经部署好了一台Master所需要的组件，这里可以直接将配置文件和启动服务文件直接拉过来即可使用 11.1.1、拷贝文件12345678910111213141516171819# 配置免密要环境[root@k8s-master-128 ~]# ssh-copy-id k8s-master-127 # kube-apiserver需要用到etcd的证书，因此我们将128上etcd所有文件拷贝到127[root@k8s-master-128 ~]# ssh k8s-master-127 mkdir -p /opt/etcd/&#123;bin,cfg,ssl&#125;[root@k8s-master-128 ~]# scp -r /opt/etcd/&#123;bin,cfg,ssl&#125; k8s-master-127:/opt/etcd/# 拷贝k8s配置文件[root@k8s-master-128 ~]# ssh k8s-master-127 mkdir -p /opt/kubernetes/&#123;bin,cfg,logs,ssl&#125;[root@k8s-master-128 ~]# scp -r /opt/kubernetes/&#123;bin,cfg,ssl&#125; k8s-master-127:/opt/kubernetes/[root@k8s-master-128 ~]# scp -r /usr/lib/systemd/system/kube-&#123;apiserver,controller-manager,scheduler&#125;.service k8s-master-127:/usr/lib/systemd/system/# 更改配置信息[root@k8s-master-127 ~]# cd /opt/kubernetes/cfg/[root@k8s-master-127 cfg]# vim kube-apiserver# 将以下参数的IP值更改为对应主机IP，其他的不变--bind-address=172.16.194.127--advertise-address=172.16.194.127# kube-controller-manager与kube-schedule组件的配置不用更改，并且这两个组件都有负载均衡功能：--leader-elect 11.1.2、启动组件1234567891011121314[root@k8s-master-127 ~]# systemctl start kube-apiserver[root@k8s-master-127 ~]# systemctl start kube-controller-manager[root@k8s-master-127 ~]# systemctl start kube-scheduler[root@k8s-master-127 ~]# systemctl enable kube-apiserver[root@k8s-master-127 ~]# systemctl enable kube-controller-manager[root@k8s-master-127 ~]# systemctl enable kube-scheduler[root@k8s-master-127 ~]# netstat -lntup|grep kubetcp 0 0 172.16.194.127:6443 0.0.0.0:* LISTEN 7344/kube-apiservertcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 7362/kube-controlletcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 7344/kube-apiservertcp6 0 0 :::10251 :::* LISTEN 7375/kube-schedulertcp6 0 0 :::10257 :::* LISTEN 7362/kube-controlletcp6 0 0 :::10259 :::* LISTEN 7375/kube-scheduler 启动完成。配置环境变量和查看集群状态：12345678910111213141516171819202122232425[root@k8s-master-127 ~]# echo &quot;export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/opt/kubernetes/bin&quot; &gt;&gt;/etc/profile[root@k8s-master-127 ~]# source /etc/profile[root@k8s-master-127 ~]# which kubectl/opt/kubernetes/bin/kubectl[root@k8s-master-127 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-7db9fccd9b-bhldc 1/1 Running 0 4d7h[root@k8s-master-127 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkubernetes-dashboard-7d5f7c58f5-dvz5m 1/1 Running 5 3d13h[root@k8s-master-127 ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5d6hnginx NodePort 10.0.0.86 &lt;none&gt; 80:43364/TCP 4d7h[root@k8s-master-127 ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.0.0.47 &lt;none&gt; 443:34756/TCP 4d6h[root@k8s-master-127 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-1 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-0 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;:&quot;true&quot;&#125; 能看到127节点可以正常接入集群并使用。 11.2、Nginx负载Master节点官网安装文档：http://nginx.org/en/linux_packages.html 选择自己的OS环境并进行nginx安装，因为实验环境机器有限，这里使用127和128两台k8s的Master节点来安装Nginx来进行实验。与前期环境规划一致。 安装：123456789[root@k8s-master-127 ~]# vim /etc/yum.repos.d/nginx.repo[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.key[root@k8s-master-127 ~]# yum install nginx -y 启动nginx：12[root@k8s-master-127 ~]# systemctl start nginx[root@k8s-master-127 ~]# systemctl enable nginx 配置：nginx配置目录在/etc/nginx/下，主配置文件nginx.conf，生产环境需要对这个配置文件进行调优，而这里做实验的话默认配置就行。 12345678910111213141516171819202122232425[root@k8s-master-127 nginx]# cat nginx.conf···略···events &#123; worker_connections 1024;&#125;# stream模块用于代理4层请求，且不能存在http模块内stream &#123; log_format main &apos;$remote_addr $upstream_addr $time_local $status&apos;; access_log /var/log/nginx/k8s-apiserver.com.access_log main; error_log /var/log/nginx/k8s-apiserver.com.error_log warn; upstream k8s-apiserver &#123; server 172.16.194.127:6443; server 172.16.194.128:6443; &#125; server &#123; listen 172.16.194.127:6444; proxy_pass k8s-apiserver; # 代理4层请求，不用加http:// &#125;&#125;http &#123;···略···&#125;···略··· 查看启动：12[root@k8s-master-127 nginx]# netstat -lntup|grep 6444tcp 0 0 172.16.194.127:6444 0.0.0.0:* LISTEN 7061/nginx: master 因为本机有6443，想要与代理理共存的话，nginx更改下端口即可。 11.3、Node节点配置使用更改Node节点的配置：1234567891011[root@k8s-node-129 ~]# cd /opt/kubernetes/cfg/[root@k8s-node-129 cfg]# grep &quot;6443&quot; ./* # 将这三个文件的地址更改为api-server的代理地址./bootstrap.kubeconfig: server: https://172.16.194.128:6443./kubelet.kubeconfig: server: https://172.16.194.128:6443./kube-proxy.kubeconfig: server: https://172.16.194.128:6443# 更改后：(所有节点都需要更改)[root@k8s-node-129 cfg]# grep &quot;6444&quot; ./*./bootstrap.kubeconfig: server: https://172.16.194.127:6444./kubelet.kubeconfig: server: https://172.16.194.127:6444./kube-proxy.kubeconfig: server: https://172.16.194.127:6444 重启Node节点组件：12[root@k8s-node-129 cfg]# systemctl restart kubelet[root@k8s-node-129 cfg]# systemctl restart kube-proxy 查看nginx代理日志：123456[root@k8s-master-127 nginx]# tail -f k8s-apiserver.com.access_log172.16.194.130 172.16.194.127:6443 13/May/2019:03:12:07 +0800 200172.16.194.130 172.16.194.127:6443 13/May/2019:03:12:07 +0800 200172.16.194.130 172.16.194.128:6443 13/May/2019:03:12:13 +0800 200172.16.194.130 172.16.194.128:6443 13/May/2019:03:12:13 +0800 200172.16.194.130 172.16.194.128:6443 13/May/2019:03:12:13 +0800 200 代理成功。 查看集群Node节点状态：1234[root@k8s-master-127 nginx]# kubectl get nodeNAME STATUS ROLES AGE VERSION172.16.194.129 Ready &lt;none&gt; 5d10h v1.14.0172.16.194.130 Ready &lt;none&gt; 5d10h v1.14.0 只有代理连接是正常的，集群节点才会是Ready状态，否则就是上面配置环境有问题。 11.4、Keepalived实现Nginx高可用目前只部署了一个Nginx节点，只有一个节点就存在单点故障的风险，一旦这个节点挂掉，那么整个集群将会不可用，市场有些可选的高可用开源解决方案，如：Keepalived、heartbeat等，这里我们选择使用前者。 大致的思路就是：1、将上面的Nginx代理节点配置多个（两个以上）；2、通过Keepalived控制VIP来进行漂移，如果Nginx代理节点挂掉后，Keepalived会将VIP漂移到正常的Nginx代理节点上，从而实现集群高可用，提高k8s集群的健壮性。 127节点已经安装和配置好Nginx，128节点按照上面的配置即可。 11.4.1、Keepalived安装与配置127Master节点部署：123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master-127 ~]# yum install keepalived -y[root@k8s-master-127 ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_MASTER&#125;vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot; interval 2 weight 2&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 50 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.194.111 &#125; track_script &#123; check_nginx &#125;&#125; 检查nginx的脚本：1234567[root@k8s-master-127 ~]# cat /etc/keepalived/check_nginx.shcount=$(ps -C nginx --no-header |wc -l)if [ $count -eq 0 ];then systemctl stop keepalivedfi[root@k8s-master-127 ~]# chmod +x /etc/keepalived/check_nginx.sh 启动主节点的Keepalived：12[root@k8s-master-127 ~]# systemctl enable keepalived[root@k8s-master-127 ~]# systemctl start keepalived 注意：Keepalived不可设置成开机自启动，一旦发生VIP漂移，则需要运维工程师介入排查问题，如果设置成开启启动，有可能会给业务带来二次伤害。 查看Keepalived绑定的VIP：123456789[root@k8s-master-127 ~]# ip a···略···2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:41:09:16 brd ff:ff:ff:ff:ff:ff inet 172.16.194.127/24 brd 172.16.194.255 scope global noprefixroute eth0 # 这是本机IP valid_lft forever preferred_lft forever inet 172.16.194.111/32 scope global eth0 # 这是VIP valid_lft forever preferred_lft forever···略··· 128Backup节点部署：123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master-128 nginx]# yum install keepalived -y[root@k8s-master-128 nginx]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_BACKUP&#125;vrrp_script check_nginx &#123; script &quot;/etc/keepalived/check_nginx.sh&quot; interval 2 weight 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 50 # 这个参数要与Master一样，不然不能识别及通讯 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.194.111/24 &#125; track_script &#123; check_nginx &#125;&#125; 检查nginx的脚本：1234567[root@k8s-master-128 nginx]# vim /etc/keepalived/check_nginx.shcount=$(ps -C nginx --no-header |wc -l)if [ $count -eq 0 ];then systemctl stop keepalivedfi[root@k8s-master-128 nginx]# chmod +x /etc/keepalived/check_nginx.sh 启动备节点的Keepalived：12[root@k8s-master-128 nginx]# systemctl enable keepalived[root@k8s-master-128 nginx]# systemctl start keepalived 11.4.2、故障模拟测试现在状态：127Master节点和128Backup节点都已经启动，VIP绑定在127的eth0网卡上；理论上，将nginx停掉，VIP会从当前节点消失，漂移到备节点上，那来看实际情况：1234567891011121314151617181920# 模拟故障停掉nginx，看看VIP是否会漂移到备用节点上[root@k8s-master-127 ~]# systemctl stop nginx[root@k8s-master-127 ~]# ip a···略···2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:41:09:16 brd ff:ff:ff:ff:ff:ff inet 172.16.194.127/24 brd 172.16.194.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever···略···# 查看备节点是否有VIP[root@k8s-master-128 ~]# ip a···略···2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:f4:da:a9 brd ff:ff:ff:ff:ff:ff inet 172.16.194.128/24 brd 172.16.194.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet 172.16.194.111/24 scope global secondary eth0 valid_lft forever preferred_lft forever···略··· 跟理论情况一样，因故障后，VIP漂移正常，那也就意味着负载均衡加高可用集群搭建完成。 11.5、k8s集群接入负载均衡使用kube-apiserver的负载均衡主备节点都已经搭建完成并测试成功，接下来就需要配置下k8s的Node组件使用即可 11.5.1、更改Nginx配置127和128节点的nginx更改如下1234567891011121314[root@k8s-master-127 ~]# vim /etc/nginx/nginx.confstream &#123; log_format main &apos;$remote_addr $upstream_addr $time_local $status&apos;; access_log /var/log/nginx/k8s-apiserver.com.access_log main; error_log /var/log/nginx/k8s-apiserver.com.error_log warn; upstream k8s-apiserver &#123; server 172.16.194.127:6443; server 172.16.194.128:6443; &#125; server &#123; listen 0.0.0.0:6444; # 将这里更改为监听所有网段 proxy_pass k8s-apiserver; &#125;&#125; 重新加载nginx，使配置生效：1234567[root@k8s-master-127 ~]# systemctl reload nginx[root@k8s-master-127 ~]# netstat -lntup|grep nginxtcp 0 0 0.0.0.0:6444 0.0.0.0:* LISTEN 20523/nginx: master[root@k8s-master-128 ~]# systemctl reload nginx[root@k8s-master-128 ~]# netstat -lntup|grep nginxtcp 0 0 0.0.0.0:6444 0.0.0.0:* LISTEN 21604/nginx: master 11.5.2、Node节点配置使用更改api-server地址的配置123456789[root@k8s-node-129 cfg]# grep &apos;111&apos; ./*./bootstrap.kubeconfig: server: https://172.16.194.111:6444./kubelet.kubeconfig: server: https://172.16.194.111:6444./kube-proxy.kubeconfig: server: https://172.16.194.111:6444[root@k8s-node-130 cfg]# grep &apos;111&apos; ./*./bootstrap.kubeconfig: server: https://172.16.194.111:6444./kubelet.kubeconfig: server: https://172.16.194.111:6444./kube-proxy.kubeconfig: server: https://172.16.194.111:6444 重启kubelet和kube-proxy组件：1234[root@k8s-node-129 cfg]# systemctl restart kubelet[root@k8s-node-129 cfg]# systemctl restart kube-proxy[root@k8s-node-130 cfg]# systemctl restart kubelet[root@k8s-node-130 cfg]# systemctl restart kube-proxy 查看127上的代理日志：12345[root@k8s-master-127 ~]# tail -f /var/log/nginx/k8s-apiserver.com.access_log172.16.194.130 172.16.194.127:6443 13/May/2019:05:32:55 +0800 200172.16.194.129 172.16.194.128:6443 13/May/2019:05:32:55 +0800 200172.16.194.129 172.16.194.127:6443 13/May/2019:05:32:55 +0800 200172.16.194.130 172.16.194.128:6443 13/May/2019:05:32:55 +0800 200 nginx默认是轮训代理。 11.5.3、K8集群状态能看到Node节点状态是Ready就没事1234[root@k8s-master-128 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSION172.16.194.129 Ready &lt;none&gt; 7d v1.14.0172.16.194.130 Ready &lt;none&gt; 7d v1.14.0 遇到的坑：1、在生成server证书的时候，需要将VIP填写进去并生成证书，不然就会报错如下：12# 启动kubelet查看/var/log/messagescertificate is valid for 127.0.0.1, 10.0.0.1, 172.16.194.127, 172.16.194.128, 172.16.194.129, 172.16.194.130, not 172.16.194.111 解决办法：将server证书重新生成（填写VIP进去），并重启api-server进程即可解决。 十二、部署集群内部DNS解析服务（CoreDNS）在kubernetes1.12之后的版本中，使用了CoreDNS作为默认的DNS；官网：https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/coredns/ 12.1、下载CoreDNS部署文件https://github.com/coredns/deployment/tree/master/kubernetes123[root@k8s-master-128 ~]# mkdir coredns &amp;&amp; cd coredns[root@k8s-master-128 coredns]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed[root@k8s-master-128 coredns]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh deploy.sh是一个便捷的脚本，用于生成用于在当前运行标准kube-dns的集群上运行CoreDNS的清单。使用coredns.yaml.sed文件作为模板，它创建一个ConfigMap和一个CoreDNS deployment，然后更新 Kube-DNS service selector以使用CoreDNS deployment。 通过重新使用现有服务，服务请求不会中断。 12.2、部署CoreDNS123456789101112[root@k8s-master-128 coredns]# chmod +x deploy.sh[root@k8s-master-128 coredns]# ./deploy.sh -i 10.0.0.2 &gt;coredns.yaml tips: 少了个jq命令：yum install -y jq[root@k8s-master-128 coredns]# kubectl create -f coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created 查看生成的文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182apiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - &quot;&quot; resources: - endpoints - services - pods - namespaces verbs: - list - watch- apiGroups: - &quot;&quot; resources: - nodes verbs: - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance &#125;---apiVersion: apps/v1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: &quot;CoreDNS&quot;spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: &quot;CriticalAddonsOnly&quot; operator: &quot;Exists&quot; nodeSelector: beta.kubernetes.io/os: linux containers: - name: coredns image: coredns/coredns:1.5.0 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ &quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot; ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: &quot;9153&quot; prometheus.io/scrape: &quot;true&quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: &quot;true&quot; kubernetes.io/name: &quot;CoreDNS&quot;spec: selector: k8s-app: kube-dns clusterIP: 10.0.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 早在部署kubelet的时候就通过--cluster-dns=10.0.0.2 --cluster-domain=cluster.local参数指定了dns地址和解析的域，因此直接部署即可使用，如果kubelet启动参数里没有配置这两个dns参数的话，上面部署coredns之后还需要将所有kubelet重新添加配置并重启进程。 12.3、查看CoreDNS状态1234567891011121314151617181920212223[root@k8s-master-128 coredns]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-55f46dd959-q47j5 1/1 Running 0 3m59scoredns-55f46dd959-vcj4w 1/1 Running 0 3m59skubernetes-dashboard-7d5f7c58f5-xqvmh 1/1 Running 1 94m[root@k8s-master-128 coredns]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 4m12skubernetes-dashboard NodePort 10.0.0.47 &lt;none&gt; 443:34756/TCP 14d[root@k8s-master-128 coredns]# kubectl get deploy -n kube-systemNAME READY UP-TO-DATE AVAILABLE AGEcoredns 2/2 2 2 5m32skubernetes-dashboard 1/1 1 1 14d[root@k8s-master-128 coredns]# kubectl get ep -n kube-system kube-dnsNAME ENDPOINTS AGEkube-dns 172.17.17.4:53,172.17.50.2:53,172.17.17.4:53 + 3 more... 10m[root@k8s-master-128 coredns]# kubectl -n kube-system get configmap corednsNAME DATA AGEcoredns 1 12m CoreDNS所创建的服务均已正常启动。 12.4、测试CoreDNS在安装完Kubernetes cluster环境后，如何验证coreDNS是否在正常工作?这是一项很重要的工作，将会影响将来在容器中部署的服务能否被正常调用。 我们可以通过创建一个busybox 的pod，再在busybox里去解析服务名的方式来验证coreDNS是否正常工作。 具体可参考kubernetes官方文档《Debugging DNS Resolution》 busybox的yaml文件：123456789101112131415[root@k8s-master-128 coredns]# cat busybox.yamlapiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - name: busybox image: busybox:1.28 command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent restartPolicy: Always 创建Buxybox pod：12345[root@k8s-master-128 coredns]# kubectl create -f busybox.yamlpod/busybox created[root@k8s-master-128 coredns]# kubectl get pods busyboxNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 0 26s busybox的resolv.conf内容：1234[root@k8s-master-128 coredns]# kubectl exec busybox cat /etc/resolv.confnameserver 10.0.0.2search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 在busybox 的pod里解析不同名字空间的服务：解析规则是：my-svc.my-namespace.svc.cluster.local因此每个空间的服务都需要指明自己所在的名字空间才可进行访问123456789101112[root@k8s-master-128 coredns]# kubectl exec -it busybox nslookup kubernetes.defaultServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local[root@k8s-master-128 coredns]# kubectl exec -it busybox nslookup kube-dns.kube-systemServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kube-dns.kube-systemAddress 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.local 实验证明，用 my-svc.my-namespace.svc.cluster.local方法即可访问服务。 在busybox 的pod里解析公网：1234567891011121314[root@k8s-master-128 coredns]# kubectl exec -it busybox nslookup qq.comServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: qq.comAddress 1: 111.161.64.48 dns48.online.tj.cnAddress 2: 111.161.64.40 dns40.online.tj.cn[root@k8s-master-128 coredns]# kubectl exec -it busybox nslookup www.baidu.comServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: www.baidu.comAddress 1: 220.181.38.150Address 2: 220.181.38.149 在busybox 的pod里解析外部IP地址 ，按照前文CoreDNS的配置，是通过pod所在node上的/etc/resolv.conf 来代理解析的。通过以上例子可见，coredns工作正常。coredns既可以管理新生成的service的域名，又可以解析出外部域名。 参考文章： Kubernetes-基于flannel的集群网络 Kubernetes-docker垃圾清理 Kubernetes-DashBoard用户界面]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>etcd</tag>
        <tag>flanneld</tag>
        <tag>SSL</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《集群部署（上）》.md]]></title>
    <url>%2F2019%2F06%2F11%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%EF%BC%88%E4%B8%8A%EF%BC%89%E3%80%8B.html</url>
    <content type="text"><![CDATA[从本文开始，将进入Kubernetes实战阶段，后续大部分文章将以实战为基础，讲解各个知识点本文将实现： 完整的ETCD集群 自颁发SSL证书 Docker的安装 Flanneld网络从安装到到原理深入理解 一、官方提供的三种部署方式1.1、minikubeMinikube是一个工具，可以在本地快速运行一个单点的kubernetes，仅用于尝试kubernetes或日常开发的用户使用。 官方地址：https://kubernetes.io/docs/setup/minikube/ 1.2、kubeadmKubeadm也是一个工具，提供kubeadm init和kubeadm join，用于快速部署kubernetes集群。 官网地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 1.3、二进制包推荐的生产环境部署方式，从官网下载发行版的二进制源码包，手动部署每个组件，组成kubernetes集群，也有助于理解kubernetes整个集群的构成。 下载地址：https://github.com/kubernetes/kubernetes/releases 二、kubernetes平台环境规划2.1、软件和服务器环境规划你需要准备一台或多台服务器，这里我准备了三台虚拟机，分别如下： OS：CentOS Linux release 7.6.1810 (Core) 软件版本： Kubernetes 1.14 Etcd 3.3 Docker version 18.09.5 Fannel 0.10 Load Balancer我复用了两个master（因为机器不够），生产环境应该是两台负载均衡服务器； 2.2、平台架构图2.2.1、单Master节点适用于做实验环境，能节省机器开销，不建议上生产。本次实验先部署单Master节点的架构图，如果需要做多Master，后面再横向扩展也很容易 2.2.2、多Master节点整个集群一般需要8台以上服务器，适用于生产环境，每个功能有对应的服务器支撑，保障线上k8s集群的健壮性，也是为业务保驾护航。 生产环境的Etcd集群应当是一组高可用服务器，为k8s提供稳定的服务； Load Balancer为k8s-master的api-server提供负载均衡服务； Master节点一般是两个以上； Node节点根据业务需求,可横向扩展至N个; 2.3、禁用SElinux并关闭防火墙12345678$ vim /etc/selinux/configSELINUX=disabled# 临时禁用selinuxsetenforce 0$ systemctl stop firewalld$ systemctl disable firewalld 2.4、关闭SWAP分区k8s集群不允许使用swap分区，否则会出问题，详细信息自行了解。 12$ swapoff -a$ swapon -s # 没有分区信息即可。 2.5、配置host解析12345vim /etc/hosts172.16.194.127 k8s-master-127172.16.194.128 k8s-master-128172.16.194.129 k8s-node-129172.16.194.130 k8s-node-130 自己顺便把主机名也根据规划配置好哈 2.6、配置时间同步123$ crontab -e# 时间同步*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt;/dev/null 2&gt;&amp;1 2.7、配置主机密钥通讯信任做这个密钥认证是为了后面分发k8s集群私钥时方便。1234567891011121314151617181920212223242526272829[root@k8s-master-128 ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory &apos;/root/.ssh&apos;.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:CxqCw48dTEwAUEIjBzEax5v9xz+2XLQVKSjJ5OVaj78 root@k8s-master-128The key&apos;s randomart image is:+---[RSA 2048]----+|&amp;O=. . . ||o*= + + . . ||. * = + . o ||..= . + o . . ||o..o....S . o . || .+..o..o. o o || . o. ... + || .+. . || .ooE |+----[SHA256]-----+[root@k8s-master-128 ~]# ls -lh /root/.ssh/总用量 8.0K-rw------- 1 root root 1.7K 4月 30 14:28 id_rsa-rw-r--r-- 1 root root 401 4月 30 14:28 id_rsa.pub[root@k8s-master-128 ~]# ssh-copy-id k8s-node-129[root@k8s-master-128 ~]# ssh-copy-id k8s-node-130[root@k8s-master-128 ~]# ssh &apos;k8s-node-130&apos; # 测试登录下 上面做的是k8s-master单向信任认证，如果需要做多向信任认证的话，把私钥cp到其他两台节点上即可。 三、自签SSL证书参考： Kubernetes安装之证书验证 创建CA证书和秘钥 3.1、安装证书生成工具cfssl123456789[root@k8s-master-128 ~]# mkdir ssl[root@k8s-master-128 ~]# cd ssl/wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo 3.2、生成Etcd证书Etcd集群需要使用两个证书，ca和server证书，生成方法如下：（在Master节点生成即可）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@k8s-master-128 ~]# mkdir /root/ssl/etcd-cret[root@k8s-master-128 ~]# cd /root/ssl/etcd-cret/[root@k8s-master-128 etcd-cret]# cat etcd-cret.sh#!/bin/bash# 创建Etcd CA证书cat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;www&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;Etcd CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot; &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca -#-----------------------# 创建Etcd Server证书cat &gt; server-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;Etcd&quot;, &quot;hosts&quot;: [ &quot;172.16.194.128&quot;, &quot;172.16.194.129&quot;, &quot;172.16.194.130&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot; &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server[root@k8s-master-128 etcd-cret]# chmod +x etcd-cret.sh[root@k8s-master-128 etcd-cret]# sh etcd-cret.sh # 警告忽略即可 将证书放到它该存在的位置： 12345678910[root@k8s-master-128 etcd-cret]# mkdir -p /opt/etcd/&#123;cfg,bin,ssl&#125;[root@k8s-master-128 etcd-cret]# cp /root/ssl/etcd-cret/&#123;ca*,server*&#125; /opt/etcd/ssl/[root@k8s-master-128 etcd-cret]# ls -lh /opt/etcd/ssl/总用量 24K-rw-r--r-- 1 root root 956 4月 30 17:33 ca.csr-rw------- 1 root root 1.7K 4月 30 17:33 ca-key.pem-rw-r--r-- 1 root root 1.3K 4月 30 17:33 ca.pem-rw-r--r-- 1 root root 1013 4月 30 17:33 server.csr-rw------- 1 root root 1.7K 4月 30 17:33 server-key.pem-rw-r--r-- 1 root root 1.4K 4月 30 17:33 server.pem 3.3、生成K8s证书这里先k8s需要用到的证书也一并生成出来，方便后面使用。(当前的操作如果不懂没关系，先跟着做，后面慢慢就明白)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[root@k8s-master-128 ~]# mkdir /root/ssl/k8s-cret[root@k8s-master-128 ~]# cd /root/ssl/k8s-cret/[root@k8s-master-128 k8s-cret]# vim certificate.sh#!/bin/bash# 创建Kubernetes CA证书cat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca -#-----------------------# 创建Kubernetes Server证书cat &gt; server-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.10.10.1&quot;, &quot;172.16.194.111&quot;, &quot;172.16.194.127&quot;, &quot;172.16.194.128&quot;, &quot;172.16.194.129&quot;, &quot;172.16.194.130&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server#-----------------------# 创建admin证书cat &gt; admin-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin#-----------------------# 创建Kube Proxy证书cat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy[root@k8s-master-128 k8s-cret]# chmod +x certificate.sh[root@k8s-master-128 k8s-cret]# sh certificate.sh # 警告忽略即可 注意：在server-csr.json文件里的hosts字段，你需要将上面的IP更改为你自己规划的服务IP，主要替换172.16.194.x这个网段即可，用于允许集群之间证书许可认证。 将证书放到它该存在的位置： 123456789101112131415161718[root@k8s-master-128 k8s-cret]# mkdir -p /opt/kubernetes/&#123;cfg,bin,ssl,logs&#125;[root@k8s-master-128 k8s-cret]# cp -a &#123;ca*,admin*,server*,kube-proxy*&#125; /opt/kubernetes/ssl/[root@k8s-master-128 k8s-cret]# rm -f /opt/kubernetes/ssl/*.json[root@k8s-master-128 k8s-cret]# ls -lh /opt/kubernetes/ssl/总用量 56K-rw-r--r-- 1 root root 1009 5月 6 18:22 admin.csr-rw------- 1 root root 1.7K 5月 6 18:22 admin-key.pem-rw-r--r-- 1 root root 1.4K 5月 6 18:22 admin.pem-rw-r--r-- 1 root root 1001 5月 6 18:22 ca.csr-rw------- 1 root root 1.7K 5月 6 18:22 ca-key.pem-rw-r--r-- 1 root root 1.4K 5月 6 18:22 ca.pem-rw-r--r-- 1 root root 1009 5月 6 18:22 kube-proxy.csr-rw------- 1 root root 1.7K 5月 6 18:22 kube-proxy-key.pem-rw------- 1 root root 6.2K 5月 6 18:37 kube-proxy.kubeconfig-rw-r--r-- 1 root root 1.4K 5月 6 18:22 kube-proxy.pem-rw-r--r-- 1 root root 1.3K 5月 6 19:25 server.csr-rw------- 1 root root 1.7K 5月 6 19:25 server-key.pem-rw-r--r-- 1 root root 1.6K 5月 6 19:25 server.pem 四、部署高可用Etcd集群4.1、下载二进制包etcd官网下载地址：https://github.com/coreos/etcd/releases1234567891011[root@k8s-master-128 ~]# mkdir soft[root@k8s-master-128 ~]# cd soft/[root@k8s-master-128 soft]# [root@k8s-master-128 soft]# wget -c https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz[root@k8s-master-128 soft]# tar zxf etcd-v3.3.12-linux-amd64.tar.gz[root@k8s-master-128 soft]# cp -a etcd-v3.3.12-linux-amd64/&#123;etcd,etcdctl&#125; /opt/etcd/bin/[root@k8s-master-128 soft]# /opt/etcd/bin/etcd --versionetcd Version: 3.3.12Git SHA: d57e8b8Go Version: go1.10.8Go OS/Arch: linux/amd64 4.2、部署Etcd的配置文件部署etcd，并配置systemd方式启动etcd进程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@k8s-master-128 ~]# cat etcd.sh#!/bin/bash# example: ./etcd.sh etcd01 172.16.194.128 etcd01=https//172.16.194.128:2380,etcd02=https://172.16.194.129:2380,etcd03=https://172.16.194.130:2380ETCD_NAME=$&#123;1:-&quot;etcd01&quot;&#125; #etcd当前节点的名称ETCD_IP=$&#123;2:-&quot;127.0.0.1&quot;&#125; # etcd当前节点的ipETCD_CLUSTER=$&#123;3:-&quot;etcd01=http://127.0.0.1:2379&quot;&#125; # 填写etcd集群的所有节点WORK_DIR=/opt/etcdcat &lt;&lt;EOF &gt;$&#123;WORK_DIR&#125;/cfg/etcd#[Member]ETCD_NAME=&quot;$&#123;ETCD_NAME&#125;&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://$&#123;ETCD_IP&#125;:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://$&#123;ETCD_IP&#125;:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://$&#123;ETCD_IP&#125;:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://$&#123;ETCD_IP&#125;:2379&quot;ETCD_INITIAL_CLUSTER=&quot;$&#123;ETCD_CLUSTER&#125;&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=-$&#123;WORK_DIR&#125;/cfg/etcdExecStart=$&#123;WORK_DIR&#125;/bin/etcd \\--name=\$&#123;ETCD_NAME&#125; \\--data-dir=\$&#123;ETCD_DATA_DIR&#125; \\--listen-peer-urls=\$&#123;ETCD_LISTEN_PEER_URLS&#125; \\--listen-client-urls=\$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \\--advertise-client-urls=\$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \\--initial-advertise-peer-urls=\$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \\--initial-cluster=\$&#123;ETCD_INITIAL_CLUSTER&#125; \\--initial-cluster-token=\$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \\--initial-cluster-state=new \\--cert-file=$&#123;WORK_DIR&#125;/ssl/server.pem \\--key-file=$&#123;WORK_DIR&#125;/ssl/server-key.pem \\--peer-cert-file=$&#123;WORK_DIR&#125;/ssl/server.pem \\--peer-key-file=$&#123;WORK_DIR&#125;/ssl/server-key.pem \\--trusted-ca-file=$&#123;WORK_DIR&#125;/ssl/ca.pem \\--peer-trusted-ca-file=$&#123;WORK_DIR&#125;/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 关于systemd的知识，自己补充哈。 4.3.2、检查配置文件启动过程中，有错误去看/var/log/message日志，或者看以下两个文件是否配置正确12[root@k8s-master-128 ~]# chmod +x etcd.sh[root@k8s-master-128 ~]# ./etcd.sh etcd01 172.16.194.128 etcd01=https://172.16.194.128:2380,etcd02=https://172.16.194.129:2380,etcd03=https://172.16.194.130:2380 #因为etcd集群通过证书通讯，所以这里需要写成https通讯协议。 4.3.2、检查配置文件启动过程中，有错误去看/var/log/message日志，或者看以下两个文件是否配置正确12345678910111213141516171819202122232425262728293031323334353637383940414243# Etcd 主配置文件[root@k8s-master-128 ~]# cat /opt/etcd/cfg/etcd#[Member]ETCD_NAME=&quot;etcd01&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://172.16.194.128:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://172.16.194.128:2379&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.194.128:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://172.16.194.128:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd01=https://172.16.194.128:2380,etcd02=https://172.16.194.129:2380,etcd03=https://172.16.194.130:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;# Etcd启动配置文件[root@k8s-master-128 ~]# cat /usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=-/opt/etcd/cfg/etcdExecStart=/opt/etcd/bin/etcd \--name=$&#123;ETCD_NAME&#125; \--data-dir=$&#123;ETCD_DATA_DIR&#125; \--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \--initial-cluster-state=new \--cert-file=/opt/etcd/ssl/server.pem \--key-file=/opt/etcd/ssl/server-key.pem \--peer-cert-file=/opt/etcd/ssl/server.pem \--peer-key-file=/opt/etcd/ssl/server-key.pem \--trusted-ca-file=/opt/etcd/ssl/ca.pem \--peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 以上两个文件都是通过etcd.sh生成的。 4.3.3、启动主进程12345[root@k8s-master-128 ~]# systemctl daemon-reload[root@k8s-master-128 ~]# systemctl enable etcd[root@k8s-master-128 ~]# systemctl start etcd # 会启动失败，而且等待启动时间长，忽略即可，其他节点起来后这个节点就会正常了。Job for etcd.service failed because a timeout was exceeded. See &quot;systemctl status etcd.service&quot; and &quot;journalctl -xe&quot; for details.[root@k8s-master-128 ~]# ps -ef|grep etcd # 查看etcd进程，是启动状态即可 4.4 部署其他Etcd节点第一个节点启动的状态就是这样，接下来需要配置第二个和第三个节点，步骤如上，需要的key自己cp到机器上即可。 记录下配置其他两个节点的命令和过程1234567891011121314151617[root@k8s-master-128 ~]# ssh k8s-node-129 mkdir -p /opt/etcd/&#123;cfg,bin,ssl&#125;[root@k8s-master-128 ~]# ssh k8s-node-130 mkdir -p /opt/etcd/&#123;cfg,bin,ssl&#125;[root@k8s-master-128 ~]# scp -r /opt/etcd/* k8s-node-129:/opt/etcd/[root@k8s-master-128 ~]# scp -r /opt/etcd/* k8s-node-130:/opt/etcd/[root@k8s-master-128 ~]# scp -r /root/etcd.sh k8s-node-129:/root/[root@k8s-master-128 ~]# scp -r /root/etcd.sh k8s-node-130:/root/# 启动etcd节点[root@k8s-node-129 ~]# ./etcd.sh etcd02 172.16.194.129 etcd01=https://172.16.194.128:2380,etcd02=https://172.16.194.129:2380,etcd03=https://172.16.194.130:2380[root@k8s-node-129 ~]# systemctl daemon-reload[root@k8s-node-129 ~]# systemctl enable etcd[root@k8s-node-129 ~]# systemctl start etcd[root@k8s-node-130 ~]# ./etcd.sh etcd03 172.16.194.130 etcd01=https://172.16.194.128:2380,etcd02=https://172.16.194.129:2380,etcd03=https://172.16.194.130:2380[root@k8s-node-130 ~]# systemctl daemon-reload[root@k8s-node-130 ~]# systemctl enable etcd[root@k8s-node-130 ~]# systemctl start etcd 当三个节点部署完成后，随即查看一个节点，启动正常啦！ 4.5、查看集群健康状态123456[root@k8s-master-128 ~]# cd /opt/etcd/ssl/[root@k8s-master-128 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; cluster-healthmember 12a876ebdf52404b is healthy: got healthy result from https://172.16.194.130:2379member 570d74cd507eba51 is healthy: got healthy result from https://172.16.194.129:2379member f3e89385516e234d is healthy: got healthy result from https://172.16.194.128:2379cluster is healthy 当然，你可以停掉一个节点再查看集群状态，看看会有什么变化。 Tips： 2379用于客户端通信 2380用于节点通信 五、Node节点安装Docker其他版本安装请移步官网：https://docs.docker.com/install123456789101112131415161718192021222324yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engineyum install -y yum-utils \ device-mapper-persistent-data \ lvm2# 官网的repo源在中国用不了，咱们还是乖乖使用马爸爸提供的源好了yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum makecache fastyum install docker-ce -ymkdir /etc/docker/cat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;EOFsystemctl enable docker &amp;&amp; systemctl start docker 六、部署Flannel网络Overlay Network： 覆盖网络，在基础网络上叠加的一种虚拟网络技术模式，该网络中的主机通过虚拟链路连接起来。 VXLAN：将源数据包封装到UDP中，并使用基础网络的IP/MAC作为外层报文头进行封装，然后在以太网上传输，到达目的地后有隧道端点解封装并将数据发送给目标地址。 Flannel：是Overlay网络的一种，也是将源数据包封装在另一种网络包里进行路由转发和通信，目前已经支持UDP、VXLAN、Host-GW、AWS VPC和GCE路由等数据转发方式。 多主机容器网络通信其他主流方案：隧道方案（Weave、OpenSwitch），路由方案（Calico）等。 6.1、Flannel网络架构图 比较详细的可以参考下面这张： 6.2、注册Flannel信息到Etcd集群注意：Flannel网络是部署在k8s的Node节点上 123456789# 写入分配的子网段到etcd，供flanneld使用[root@k8s-node-129 ~]# cd /opt/etcd/ssl/[root@k8s-node-129 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; set /coreos.com/network/config &apos;&#123; &quot;Network&quot;: &quot;172.17.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;&#123; &quot;Network&quot;: &quot;172.17.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;# GET 查看设置[root@k8s-node-129 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; get /coreos.com/network/config&#123; &quot;Network&quot;: &quot;172.17.0.0/16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125; # 能get到我们写入etcd集群的信息，即表明写入成功 6.3、下载二进制包下载地址：https://github.com/coreos/flannel/releases 1234[root@k8s-node-129 ~]# wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz[root@k8s-node-129 ~]# tar zxf flannel-v0.11.0-linux-amd64.tar.gz[root@k8s-node-129 ~]# mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl,logs&#125; # 创建k8s相关的工作目录 [root@k8s-node-129 ~]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ # 将flanneld相关文件放到k8s工作目录 6.4、部署Flannel的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@k8s-node-129 ~]# cat flanneld.sh#!/bin/bashETCD_ENDPOINTS=$&#123;1:-&quot;http://127.0.0.1:2379&quot;&#125; #接收ETCD的参数 cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/flanneld # 生成Flanneld配置文件FLANNEL_OPTIONS=&quot;--etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \-etcd-cafile=/opt/etcd/ssl/ca.pem \-etcd-certfile=/opt/etcd/ssl/server.pem \-etcd-keyfile=/opt/etcd/ssl/server-key.pem&quot;EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/flanneld.service # 生成Flanneld启动文件，使Systemd管理Flannel[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld --ip-masq \$FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/docker.service # 配置Docker启动指定子网段[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.env # 添加了这行（使Docker在启动时指定子网段，这个子网段是Flanneld分配给它的）ExecStart=/usr/bin/dockerd \$DOCKER_NETWORK_OPTIONS # 添加了DOCKER_NETWORK_OPTIONS 选项ExecReload=/bin/kill -s HUP \$MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.targetEOF 6.5、启动Flanneld节点6.5.1、生成配置文件12[root@k8s-node-129 ~]# chmod +x flanneld.sh[root@k8s-node-129 ~]# ./flanneld.sh https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379 6.5.2、检查配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# flanneld配置文件[root@k8s-node-129 ~]# cat /opt/kubernetes/cfg/flanneldFLANNEL_OPTIONS=&quot;--etcd-endpoints=https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379 -etcd-cafile=/opt/etcd/ssl/ca.pem -etcd-certfile=/opt/etcd/ssl/server.pem -etcd-keyfile=/opt/etcd/ssl/server-key.pem&quot;# Flanneld启动文件[root@k8s-node-129 ~]# cat /usr/lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]Type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.target# Docker启动文件[root@k8s-node-129 ~]# cat /usr/lib/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notifyEnvironmentFile=/run/flannel/subnet.env #指定了Flanneld网络配置文件ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTimeoutStartSec=0Delegate=yesKillMode=processRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 6.5.3、启动Flanneld进程12345# 启动flanneld和Dockersystemctl daemon-reloadsystemctl enable flanneldsystemctl start flanneldsystemctl restart docker 6.6、查看Flannel部署结果1234567891011121314151617181920# Flanneld的网络配置文件（Docker会根据这个网络配置文件信息生成子网段）[root@k8s-node-129 ~]# cat /run/flannel/subnet.envDOCKER_OPT_BIP=&quot;--bip=172.17.73.1/24&quot;DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;DOCKER_OPT_MTU=&quot;--mtu=1450&quot;DOCKER_NETWORK_OPTIONS=&quot; --bip=172.17.73.1/24 --ip-masq=false --mtu=1450&quot;[root@k8s-node-129 ~]# ip a···略···3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:1a:42:15:83 brd ff:ff:ff:ff:ff:ff inet 172.17.73.1/24 brd 172.17.73.255 scope global docker0 valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 3a:d5:fa:a7:57:c6 brd ff:ff:ff:ff:ff:ff inet 172.17.73.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::38d5:faff:fea7:57c6/64 scope link valid_lft forever preferred_lft forever 能看到docker0网卡的IP地址段已经变更为我们配置的地址段。该节点新启动了flannel.1网卡。 6.7、其他Node节点部署Flannal网络根据前期规划，k8s有两个Node节点，上面部署了129这个节点，剩下的130节点按照上面的部署步骤来就行，多个节点同理。 12345678910111213141516171819# 查看一台Node的部署状态，如下部署正常。[root@k8s-node-130 ~]# cat /run/flannel/subnet.envDOCKER_OPT_BIP=&quot;--bip=172.17.16.1/24&quot;DOCKER_OPT_IPMASQ=&quot;--ip-masq=false&quot;DOCKER_OPT_MTU=&quot;--mtu=1450&quot;DOCKER_NETWORK_OPTIONS=&quot; --bip=172.17.16.1/24 --ip-masq=false --mtu=1450&quot;[root@k8s-node-130 ~]# ip a···略···3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:f4:21:f9:94 brd ff:ff:ff:ff:ff:ff inet 172.17.16.1/24 brd 172.17.16.255 scope global docker0 valid_lft forever preferred_lft forever4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether e2:cf:ca:a9:0f:98 brd ff:ff:ff:ff:ff:ff inet 172.17.16.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::e0cf:caff:fea9:f98/64 scope link valid_lft forever preferred_lft forever 6.8、查看Etcd集群记录的通讯信息每个节点在部署Flanneld网络时，相关信息都记录在Etcd集群当中，我们来一下1234567891011# ls /coreos.com/network/ 查看etcd存储的网络配置信息[root@k8s-node-129 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; ls /coreos.com/network/# ls /coreos.com/network/subnets 查看Flanneld分配的子网信息[root@k8s-node-129 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; ls /coreos.com/network/subnets/coreos.com/network/subnets/172.17.73.0-24/coreos.com/network/subnets/172.17.16.0-24# get /coreos.com/network/subnets/172.17.73.0-24 查看该子网的详细信息[root@k8s-node-129 ssl]# /opt/etcd/bin/etcdctl --ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem --endpoints=&quot;https://172.16.194.128:2379,https://172.16.194.129:2379,https://172.16.194.130:2379&quot; get /coreos.com/network/subnets/172.17.73.0-24&#123;&quot;PublicIP&quot;:&quot;172.16.194.129&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:&#123;&quot;VtepMAC&quot;:&quot;3a:d5:fa:a7:57:c6&quot;&#125;&#125; 解读： 子网信息里记录了公共IP172.16.194.129，也就是说172.17.73.0这个网段的通讯，会通过公网IP转发出去与其他节点的网段进行通讯。 所以，只要是加入了Flanneld网络的主机节点，都可以通过etcd记录的信息，与其他子网进行通讯。 6.9、测试Flanneld网络通讯上面我们部署了两个节点的网络，这两个节点的Flannal网段信息分别为： k8s-node-129 flannel.1: 172.17.73.0/32 docker0: 172.17.73.1/24 k8s-node-130 flannel.1: 172.17.16.0/32 docker0: 172.17.16.1/24 通过ping检测他们之间的连通性，发现他们是不同的子网，为什么能连通呢？123456789[root@k8s-node-129 ~]# ping -c 2 172.17.16.1PING 172.17.16.1 (172.17.16.1) 56(84) bytes of data.64 bytes from 172.17.16.1: icmp_seq=1 ttl=64 time=0.503 ms64 bytes from 172.17.16.1: icmp_seq=2 ttl=64 time=0.537 ms[root@k8s-node-130 ~]# ping -c 2 172.17.73.1PING 172.17.73.1 (172.17.73.1) 56(84) bytes of data.64 bytes from 172.17.73.1: icmp_seq=1 ttl=64 time=0.382 ms64 bytes from 172.17.73.1: icmp_seq=2 ttl=64 time=0.621 ms 带着这个思考，我们来看一下节点上的路由情况1234567[root@k8s-node-129 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.16.194.2 0.0.0.0 UG 100 0 0 eth0172.16.194.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0172.17.16.0 172.17.16.0 255.255.255.0 UG 0 0 0 flannel.1172.17.73.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 解读： docker0网卡是172.17.73.0网段，也就是节点自身的网段； 规则：172.17.16.0网段走flannel.1网卡出口，而flannel.1能与其他flannel节点网段直接通讯； 规则：172.16.194.0是宿主机的网段，出口是物理网卡eth0； 根据上面解读的路由规则，我尝试通过一个案例来理解网络请求走向：12# (k8s-node-129)172.17.73.0网段向(k8s-node-130)172.17.16.0网段发起一个ping请求[root@k8s-node-129 ~]# ping -c 2 172.17.16.1 其网络请求过程如下： k8s-node-129节点匹配到本机路由规则：去往172.17.16.0 网段，走 flannel.1出口 ； 通过 /opt/kubernetes/bin/etcdctl get到172.17.16.0-24的公网IP是172.16.194.130，报文交由172.16.194.0网段处理； k8s-node-129节点再次匹配到本机路由规则：去往172.16.194.0网段，走eth0（物理网卡）出口; 公网172.16.194.129通过eth0网卡将报文发送给172.16.194.130（etcd里注册了172.17.16.0网段的公网IP是172.16.194.130）; 目标地址172.16.194.130接收到请求报文后转发flannel.1网卡， flannel.1网卡再转发到docker0网卡，等待报文被响应后，再原路返回。完成整个通讯。 Flanneld网络部署完毕。 Kubernetes集群部署的上篇完结，请关注文章《集群部署（下）》]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>etcd</tag>
        <tag>flanneld</tag>
        <tag>SSL</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes系列之《基础入门介绍》.md]]></title>
    <url>%2F2019%2F06%2F11%2Fkubernetes%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D%E3%80%8B.html</url>
    <content type="text"><![CDATA[一、Kubernetes简介 Kubernetes是Google在2014年6月开源的一个容器集群管理系统，该系统使用Google研发的Golang语言开发。Kubernetes也被叫做k8s，因为k和s之前有8个字母。 k8s原本是Google内部一个叫Borg的容器集群管理系统衍生出来的，Borg已经在Google大规模生产运行十年之久。 k8s主要用于自动化部署、扩展和管理容器应用，它提供了资源调度、部署管理、服务发现、扩容缩容、监控等一整套功能。 Kubernetes的目标是让部署容器化应用简单高效。官网地址：https://www.kubernetes.io 二、Kubernetes主要功能1、数据卷 在Pod中，容器之前共享数据，可以使用数据卷。这个功能与docker里volume功能相等。 2、应用程序健康检查 检查容器内的服务、进程是否异常；可以设置监控检测策略来保证应用的健壮性。 3、复制应用程序实例 控制维护Pod副本数，保证一个Pod或一组同类的Pod数始终可用。 4、弹性缩容 根据设定的指标（CPU利用率）自动缩放Pod副本数。 5、服务发现 使用环境变量或DOS服务插件保证容器中程序发现Pod入口访问地址。 6、负载均衡 一组Pod副本分配一个私有的集群IP地址，负载均衡转发请求到后端容器。在集群内部，其他Pod可通过这个ClusterIP访问应用。 7、滚动更新 更新服务不中断，异常更新一个Pod，而不是同时删除整个服务。 8、服务编排 通过文件描述部署服务，使得应用程序部署变得更高效。 9、资源监控 Node节点组件继承CAdvisor资源收集工具，可通过Heapster汇总整个集群节点资源数据，然后存储到InfluxDB时序数据库中，再由Grafana展示。 10、提供认证和授权 支持角色访问控制（RBAC）认证授权等策略。 三、基本概念 在k8s中，它的概念非常的多！而且一定要去了解这些概念，知道它的作用是必须的，你需要知道这些概念，才能更好的应用它。 1、Pod Pod是k8s最小部署单元，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，在同一个Docker主机上运行。 官网解读：https://www.kubernetes.org.cn/kubernetes-pod 2、Service Service一个应用服务的抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。 官网解读：https://www.kubernetes.org.cn/kubernetes-services 3、Volume 数据卷，共享Pod中容器使用的数据。 官网解读：https://www.kubernetes.org.cn/kubernetes-volumes 4、Namespace 命名空间将对象逻辑上分配到不同Namespace，可以是不同的项目、用户等分区管理，并设定控制策略，从而实现多租户。 官网解读：https://www.kubernetes.org.cn/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%EF%BC%9Anamespace 5、Lable 标签用于区分对象（比如Pod、Service），键/值对存在；每个对象可以有多个标签，通过标签关联对象。 官网解读：https://www.kubernetes.org.cn/kubernetes-labels 四、更高层次的抽象（Controllers）1、ReplicaSet（RS） ReplicaSet是下一代复本控制器。 官网解读：https://www.kubernetes.org.cn/replicasets 2、Deployment（无状态应用部署） Deployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。 官方建议使用Department管理ReplicaSets,而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象。 咱们在实际应用当中，基本上都是去创建一个Deployment，然后由Deployment去创建RS和Pod等。 官网解读：https://www.kubernetes.org.cn/deployment 3、StatefulSet（有状态应用部署） StatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。 官网解读：https://www.kubernetes.org.cn/statefulset 4、DaemonSet DaemonSet确保所有（或一些）节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。 官网解读：https://www.kubernetes.org.cn/daemonset 5、Job 一次性任务，运行完成后Pod销毁，不在重新启动新容器。还可以任务定时运行。 官网解读：https://www.kubernetes.org.cn/job 6、CronJob 定时任务 官网解读： https://www.kubernetes.org.cn/cronjob 五、Kubernetes系统架构拓扑图官网架构图： 自己画的图： 六、组件功能介绍6.1、Master组件 kube-apiserverKubernetes API，集群的统一入口，各组件协调者。以HTTP API提供接口服务，所有对象的增删改查和监听操作都交给API Server处理后再提交给Etcd存储。 kube-controller-manager处理集群中常规后台任务，一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。 kube-scheduler根据调度算法为新创建的Pod选择一个Node节点，可以任意部署，可以部署在同一个节点上，也可以部署在不同的节点上。 6.2、Node组件 kubeletkubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。 kube-proxy在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。 docker或rocket/rkt容器引擎，k8s可选的容器运行底层支持技术，一般我们都使用docker。 6.3、第三方服务 etcd分布式键值存储系统。用于保持集群状态数据，比如Pod、Service等对象信息。 参考文章： kubernetes-整体概述和架构 Kubernetes组件之kube-controller-manager Kubernetes基础概念及架构概述 Kubernetes 之APIServer组件简介 聊聊kube-apiserver]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-撰写前端功能》]]></title>
    <url>%2F2018%2F08%2F20%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E6%92%B0%E5%86%99%E5%89%8D%E7%AB%AF%E5%8A%9F%E8%83%BD%E3%80%8B.html</url>
    <content type="text"><![CDATA[Django使用前后端分离后，后端使用DRF，而前端则使用VUE来实现，本文学习下前端框架vueAdmin-template的使用。 GitHub：https://github.com/PanJiaChen/vueAdmin-templatedemo地址：https://panjiachen.github.io/vueAdmin-template/#/dashboard 编译安装123456# Clone project$ git clone https://github.com/PanJiaChen/vueAdmin-template.git# Install dependencies$ cd vueAdmin-template$ npm install # 会下载项目所有所需要的依赖包 运行vueAdmin1$ npm run dev 一、获取资源1. 显示侧边栏vim router/index.js123456789101112131415161718192021222324252627282930313233343536373839export const constantRouterMap = [ &#123; path: &apos;/login&apos;, component: () =&gt; import(&apos;@/views/login/index&apos;), hidden: true &#125;, &#123; path: &apos;/404&apos;, component: () =&gt; import(&apos;@/views/404&apos;), hidden: true &#125;, &#123; path: &apos;/&apos;, # 这是网址根路由 component: Layout, # 调用模板 redirect: &apos;/dashboard&apos;, # 路由URI名称 name: &apos;Dashboard&apos;, # 取一个名字 hidden: false, # 默认是否隐藏，否 children: [&#123; path: &apos;dashboard&apos;, component: () =&gt; import(&apos;@/views/dashboard/index&apos;), # 指定访问的文件 meta: &#123; title: &apos;Dashboard&apos;, icon: &apos;example&apos; &#125; &#125;] &#125;, &#123; path: &apos;/users&apos;, # 这是一级路由地址， component: Layout, # 同样调用模板 # 没有redirect字段，因为users是多级标签 name: &apos;用户管理&apos;, # 给它一个名字 meta: &#123; title: &apos;用户管理&apos;, icon: &apos;example&apos; &#125;, # 来一个meta信息 children: [ 子分类 &#123; path: &apos;list&apos;, # 二级路由地址，会与一级地址拼接成：/users/list name: &apos;用户列表&apos;, # 给它一个名字，用于显示 component: () =&gt; import(&apos;@/views/user/index&apos;), # 指向user list所在的页面文件 meta: &#123; title: &apos;用户列表&apos; &#125; &#125;, &#123; path: &apos;group&apos;, # 二级路由地址，与上面同理 name: &apos;用户组列表&apos;, component: () =&gt; import(&apos;@/views/user/group&apos;), meta: &#123; title: &apos;用户组列表&apos; &#125; &#125; ] &#125;, &#123; path: &apos;*&apos;, redirect: &apos;/404&apos;, hidden: true &#125;] 完事，来看看页面效果： 2. Api接口GET方法vim api/users/group.js12345678910import request from &apos;@/utils/request&apos;// 获取用户组信息export function getGroupList(params) &#123; return request(&#123; url: &apos;/groups/&apos;, method: &apos;get&apos;, params &#125;)&#125; 3. 创建view页面文件前端页面使用饿了么开源的Element-ui框架，项目地址：http://element-cn.eleme.io/#/zh-CN/component/installation vim views/user/group.vue1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;template&gt; &lt;div class=&quot;user-group-container&quot; style=&quot;padding: 10px;&quot;&gt; &lt;div&gt; &lt;!-- 用户列表搜索 --&gt; &lt;el-col :span=&quot;8&quot;&gt; &lt;el-input placeholder=&quot;请输入内容&quot; size=&quot;small&quot; prefix-icon=&quot;el-icon-search&quot; v-model=&quot;params.name&quot; @input=&quot;searchClick&quot;&gt; &lt;/el-input&gt; &lt;/el-col&gt; &lt;/div&gt; &lt;!-- 显示用户列表 --&gt; &lt;el-table :data=&quot;groupList&quot; border style=&quot;width: 100%&quot;&gt; &lt;el-table-column prop=&quot;id&quot; label=&quot;ID&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;name&quot; label=&quot;用户组&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;users&quot; label=&quot;组成员&quot;&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;!-- 分页 --&gt; &lt;center&gt; &lt;el-pagination background layout=&quot;total, prev, pager, next, jumper&quot; @current-change=&quot;handleCurrentChange&quot; :total=&quot;totalNum&quot;&gt; &lt;/el-pagination&gt; &lt;/center&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import &#123; getGroupList &#125; from &apos;@/api/users/group&apos; export default &#123; data() &#123; return &#123; groupList: [], totalNum: 0, params: &#123; page: 1, name: &apos;&apos; &#125;, &#125; &#125;, created() &#123; this.fetchData() &#125;, methods: &#123; fetchData() &#123; getGroupList(this.params).then(res =&gt; &#123; this.groupList = res.results this.totalNum = res.count &#125;) &#125;, handleCurrentChange(val) &#123; this.params.page = val this.fetchData() &#125;, searchClick() &#123; this.params.page = 1 this.fetchData() &#125; &#125; &#125;&lt;/script&gt; 4. 查看页面效果 二、创建资源1. Api接口POST方法vim api/users/group.js12345678910111213import request from &apos;@/utils/request&apos;// 获取用户组信息······// 创建用户组export function createGroup(data) &#123; return request(&#123; url: &apos;/groups/&apos;, method: &apos;post&apos;, data &#125;)&#125; 2. Dialog表单+表单验证vim views/user/group.vue1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 &lt;div&gt; &lt;!-- 用户列表搜索 --&gt; ······ &lt;!-- 添加用户组按钮 --&gt; &lt;el-col :span=&quot;16&quot; style=&quot;text-alige: right&quot;&gt; &lt;el-button round type=&quot;primary&quot; @click=&quot;handleAddGroupBtn&quot; &gt; 添加用户组 &lt;/el-button&gt; &lt;/el-col&gt; &lt;/div&gt; &lt;!-- 显示用户列表 --&gt; ····· &lt;!-- 添加用户组Dialog --&gt; &lt;el-dialog title=&quot;添加用户组&quot; :visible.sync=&quot;addGroupFormVisible&quot;&gt; &lt;el-form :model=&quot;addGroupForm&quot; :rules=&quot;addGroupFormRules&quot; ref=&quot;addGroupForm&quot; label-width=&quot;100px&quot;&gt; &lt;el-form-item label=&quot;用户组名称&quot; prop=&quot;name&quot; :label-width=&quot;addGroupformLabelWidth&quot;&gt; &lt;el-input v-model=&quot;addGroupForm.name&quot; style=&quot;width: 300px;&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;div slot=&quot;footer&quot; class=&quot;dialog-footer&quot;&gt; &lt;el-button @click=&quot;addGroupFormVisible = false&quot;&gt;取 消&lt;/el-button&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;addGroupFormVisible = false&quot;&gt;确 定&lt;/el-button&gt; &lt;/div&gt; &lt;/el-dialog&gt;&lt;script&gt; import &#123; getGroupList &#125; from &apos;@/api/users/group&apos; export default &#123; data() &#123; return &#123; ······ addGroupFormVisible: false, addGroupformLabelWidth: &apos;200px&apos;, addGroupForm: &#123; name: &apos;&apos; &#125;, addGroupFormRules: &#123; name: [ &#123; required: true, message: &apos;请输入用户组名称&apos;, trigger: &apos;blur&apos; &#125;, &#123; min: 2, max: 8, message: &apos;长度在 2 到 8 个字符&apos;, trigger: &apos;blur&apos; &#125; ] &#125; &#125; &#125;, ······ methods: &#123; ······ handleAddGroupBtn() &#123; this.addGroupFormVisible = true &#125; &#125; &#125;&lt;/script&gt; 3. 查看页面效果 4. 提交添加动作12345678910111213141516&lt;el-button type=&quot;primary&quot; @click=&quot;handleAddGroupSubmit&quot;&gt;确 定&lt;/el-button&gt;import &#123; getGroupList, createGroup &#125; from &apos;@/api/users/group&apos;handleAddGroupSubmit() &#123; createGroup(this.addGroupForm).then(res =&gt; &#123; this.$message(&#123; message: &apos;用户组创建成功.&apos;, type: &apos;success&apos; &#125;) this.addGroupFormVisible = false //隐藏表单 this.$refs[&apos;addGroupForm&apos;].resetFields() // 重置Form表单 this.fetchData() //刷新页面（其实就是重新获取数据） // console.log(this.addGroupForm) // 有个BUG，表单验证后，没有拦截，就直接添加，等于没有验证效果。 &#125;)&#125; 增加用户组完成。 三、更新资源1. Api接口PATCH方法vim api/users/group.js12345678// 修改用户组export function updateGroup(id, data) &#123; return(&#123; url: &apos;/groups/&apos; + id + &apos;/&apos;, //一定要加上ID method: &apos;patch&apos;, data &#125;)&#125; 2. Dialog表单+表单验证123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;!-- 显示用户列表 --&gt;······ &lt;el-table-column label=&quot;操作&quot;&gt; &lt;template slot-scope=&quot;scope&quot;&gt; &lt;el-button type=&quot;warning&quot; icon=&quot;el-icon-edit&quot; circle @click=&quot;handleModifyGroupClick(scope.row)&quot; &gt;&lt;/el-button&gt; &lt;/template&gt; &lt;/el-table-column&gt;&lt;!-- 更新用户组Dialog --&gt;&lt;el-dialog title=&quot;更新用户组&quot; :visible.sync=&quot;modifGroupFormVisible&quot;&gt; &lt;el-form :model=&quot;modifGroupForm&quot; :rules=&quot;GroupFormRules&quot; ref=&quot;modifGroupForm&quot; label-width=&quot;100px&quot;&gt; &lt;el-form-item label=&quot;用户组名称&quot; prop=&quot;name&quot; :label-width=&quot;modifGroupformLabelWidth&quot;&gt; &lt;el-input v-model=&quot;modifGroupForm.name&quot; style=&quot;width: 300px;&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;div slot=&quot;footer&quot; class=&quot;dialog-footer&quot;&gt; &lt;el-button @click=&quot;modifGroupFormVisible = false&quot;&gt;取 消&lt;/el-button&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;handleModifGroupSubmit&quot;&gt;确 定&lt;/el-button&gt; &lt;/div&gt;&lt;/el-dialog&gt;&lt;script&gt; import &#123; getGroupList, createGroup, updateGroup &#125; from &apos;@/api/users/group&apos; export default &#123; data() &#123; return &#123; ······ GroupFormRules: &#123; name: [ &#123; required: true, message: &apos;请输入用户组名称&apos;, trigger: &apos;blur&apos; &#125;, &#123; min: 2, max: 8, message: &apos;长度在 2 到 8 个字符&apos;, trigger: &apos;blur&apos; &#125; ] &#125;, modifGroupFormVisible: false, modifGroupformLabelWidth: &apos;200px&apos;, modifGroupForm: &#123; name: &apos;&apos; &#125; &#125; &#125;, ······ methods: &#123; ······ handleModifyGroupClick(obj) &#123; this.modifGroupFormVisible = true this.modifGroupForm = obj &#125;, handleModifGroupSubmit() &#123; updateGroup(this.modifGroupForm.id, this.modifGroupForm).then(res =&gt; &#123; this.$message(&#123; message: &apos;用户组更新成功.&apos;, type: &apos;success&apos; &#125;) this.modifGroupFormVisible = false this.$refs[&apos;modifGroupForm&apos;].resetFields() this.fetchData() &#125;) &#125; &#125; &#125;&lt;/script&gt; 3. 查看页面效果 四、删除资源1. Api接口DELETE方法vim api/users/group.js1234567// 删除用户组export function deleteGroup(id) &#123; return request(&#123; url: &apos;/groups/&apos; + id + &apos;/&apos;, method: &apos;delete&apos;, &#125;)&#125; 2. 绑定删除动作vim views/uesr/group.vue1234567891011121314151617181920212223242526272829303132333435363738&lt;!-- 显示用户列表 --&gt;······ &lt;el-table-column label=&quot;操作&quot;&gt; &lt;template slot-scope=&quot;scope&quot;&gt; &lt;el-button type=&quot;warning&quot; icon=&quot;el-icon-edit&quot; circle @click=&quot;handleModifyGroupClick(scope.row)&quot; &gt;&lt;/el-button&gt; &lt;el-button type=&quot;danger&quot; icon=&quot;el-icon-delete&quot; circle @click=&quot;handleDeleteGroupClick(scope.row.id,scope.row.name)&quot;&gt;&lt;/el-button&gt; &lt;/template&gt; &lt;/el-table-column&gt;&lt;script&gt; import &#123; getGroupList, createGroup, updateGroup, deleteGroup &#125; from &apos;@/api/users/group&apos; export default &#123; ······ methods: &#123; ······ handleDeleteGroupClick(id, name) &#123; this.$confirm(&apos;确定要删除 &quot;&apos; + name + &apos;&quot; 用户组吗？&apos;, &apos;提示&apos;, &#123; confirmButtonText: &apos;确定&apos;, cancelButtonText: &apos;取消&apos;, type: &apos;warning&apos; &#125;).then(() =&gt; &#123; deleteGroup(id).then(() =&gt; &#123; this.$message(&#123; type: &apos;success&apos;, message: &apos;删除成功!&apos; &#125;) this.fetchData() &#125;) &#125;).catch(() =&gt; &#123; this.$message(&#123; type: &apos;info&apos;, message: &apos;已取消删除&apos; &#125;) &#125;) &#125; &#125; &#125;&lt;/script&gt; 3. 查看页面效果 五、遇到的问题1. 验证问题现象：在添加或更新资源时，设置验证提示2-8个字符，尽管校验错误，但此时点击确定，还是能添加进去；还是能添加成功 问题解决：在调用create、update方法之前，先判断一下是否有验证通过，没有验证通过就提示一下；具体解决代码：123456789101112131415161718192021222301x 目前还没解决，解决后贴上02x 已经解决啦，代码如下：&lt;el-button type=&quot;primary&quot; @click=&quot;handleAddGroupSubmit(&apos;addGroupForm&apos;)&quot;&gt;确 定&lt;/el-button&gt;handleAddGroupSubmit(addGroupForm) &#123; this.$refs[addGroupForm].validate((valid) =&gt; &#123; //添加认证判断即可 if (valid) &#123; createGroup(this.addGroupForm).then(res =&gt; &#123; this.$message(&#123; message: &apos;用户组创建成功.&apos;, type: &apos;success&apos; &#125;) this.addGroupFormVisible = false this.$refs[&apos;addGroupForm&apos;].resetFields() this.fetchData() &#125;) &#125; else &#123; return false &#125; &#125;)&#125;, 2. 字段信息同步我也不太确定这算不算是个问题，在模态框上输入类容，页面上的数据会动态监听并且随之变化； 3. 验证信息不重置第一步：在input框输入信息如下，出发验证错误 第二步：取消模态框，再点击其他更新按钮，会发现上一次的验证信息还在？ 解决方法：123456789101101x 设置这个参数不起作用，目前还没解决这个问题this.$refs[&apos;modifGroupForm&apos;].resetFields()02x 解决啦，如下：(在打开模态框后重置From即可)handleModifyGroupClick(obj) &#123; this.modifyGroupFormVisible = true this.modifyGroupForm = obj if (this.$refs[&apos;modifyGroupForm&apos;] !== undefined) &#123; this.$refs[&apos;modifyGroupForm&apos;].resetFields() &#125;&#125;, 六、整个案例全部代码1. API接口12345678910111213141516171819202122232425262728293031323334353637import request from &apos;@/utils/request&apos;// 获取用户组信息export function getGroupList(params) &#123; return request(&#123; url: &apos;/groups/&apos;, method: &apos;get&apos;, params &#125;)&#125;// 创建用户组export function createGroup(data) &#123; return request(&#123; url: &apos;/groups/&apos;, method: &apos;post&apos;, data &#125;)&#125;// 修改用户组export function updateGroup(id, data) &#123; return request(&#123; url: &apos;/groups/&apos; + id + &apos;/&apos;, method: &apos;patch&apos;, data &#125;)&#125;// 删除用户组export function deleteGroup(id) &#123; return request(&#123; url: &apos;/groups/&apos; + id + &apos;/&apos;, method: &apos;delete&apos;, id &#125;)&#125; 2. 前端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184&lt;template&gt; &lt;div class=&quot;user-group-container&quot; style=&quot;padding: 10px;&quot;&gt; &lt;div&gt; &lt;!-- 用户列表搜索 --&gt; &lt;el-col :span=&quot;8&quot;&gt; &lt;el-input placeholder=&quot;请输入内容&quot; size=&quot;small&quot; prefix-icon=&quot;el-icon-search&quot; v-model=&quot;params.name&quot; @input=&quot;searchClick&quot;&gt; &lt;/el-input&gt; &lt;/el-col&gt; &lt;!-- 添加用户组按钮 --&gt; &lt;el-col :span=&quot;16&quot; style=&quot;text-alige: right&quot;&gt; &lt;el-button plain type=&quot;primary&quot; size=&quot;small&quot; @click=&quot;handleAddGroupBtn&quot; &gt; 添加 &lt;/el-button&gt; &lt;/el-col&gt; &lt;/div&gt; &lt;!-- 显示用户列表 --&gt; &lt;el-table :data=&quot;groupList&quot; border style=&quot;width: 100%&quot;&gt; &lt;el-table-column prop=&quot;id&quot; label=&quot;ID&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;name&quot; label=&quot;用户组&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;users&quot; label=&quot;组成员&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column label=&quot;操作&quot;&gt; &lt;template slot-scope=&quot;scope&quot;&gt; &lt;el-button size=&quot;mini&quot; @click=&quot;handleModifyGroupClick(scope.row)&quot; plain&gt;编辑&lt;/el-button&gt; &lt;el-button size=&quot;mini&quot; type=&quot;danger&quot; @click=&quot;handleDeleteGroupClick(scope.row.id,scope.row.name)&quot; plain&gt;删除&lt;/el-button&gt; &lt;/template&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;!-- 添加用户组Dialog --&gt; &lt;el-dialog title=&quot;添加用户组&quot; :visible.sync=&quot;addGroupFormVisible&quot;&gt; &lt;el-form :model=&quot;addGroupForm&quot; :rules=&quot;GroupFormRules&quot; ref=&quot;addGroupForm&quot; label-width=&quot;100px&quot;&gt; &lt;el-form-item label=&quot;用户组名称&quot; prop=&quot;name&quot; :label-width=&quot;addGroupformLabelWidth&quot;&gt; &lt;el-input v-model=&quot;addGroupForm.name&quot; style=&quot;width: 300px;&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;div slot=&quot;footer&quot; class=&quot;dialog-footer&quot;&gt; &lt;el-button @click=&quot;addGroupFormVisible = false&quot;&gt;取 消&lt;/el-button&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;handleAddGroupSubmit(&apos;addGroupForm&apos;)&quot;&gt;确 定&lt;/el-button&gt; &lt;/div&gt; &lt;/el-dialog&gt; &lt;!-- 更新用户组Dialog --&gt; &lt;el-dialog title=&quot;更新用户组&quot; :visible.sync=&quot;modifyGroupFormVisible&quot;&gt; &lt;el-form :model=&quot;modifyGroupForm&quot; :rules=&quot;GroupFormRules&quot; ref=&quot;modifyGroupForm&quot; label-width=&quot;100px&quot;&gt; &lt;el-form-item label=&quot;用户组名称&quot; prop=&quot;name&quot; :label-width=&quot;modifyGroupformLabelWidth&quot;&gt; &lt;el-input v-model=&quot;modifyGroupForm.name&quot; style=&quot;width: 300px;&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;div slot=&quot;footer&quot; class=&quot;dialog-footer&quot;&gt; &lt;el-button @click=&quot;modifyGroupFormVisible = false&quot;&gt;取 消&lt;/el-button&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;handleModifyGroupSubmit(&apos;modifyGroupForm&apos;)&quot;&gt;确 定&lt;/el-button&gt; &lt;/div&gt; &lt;/el-dialog&gt; &lt;!-- 分页 --&gt; &lt;center&gt; &lt;el-pagination background layout=&quot;total, prev, pager, next, jumper&quot; @current-change=&quot;handleCurrentChange&quot; :total=&quot;totalNum&quot;&gt; &lt;/el-pagination&gt; &lt;/center&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import &#123; getGroupList, createGroup, updateGroup, deleteGroup &#125; from &apos;@/api/users/group&apos; export default &#123; data() &#123; return &#123; groupList: [], totalNum: 0, params: &#123; page: 1, name: &apos;&apos; &#125;, addGroupFormVisible: false, addGroupformLabelWidth: &apos;200px&apos;, addGroupForm: &#123; name: &apos;&apos; &#125;, GroupFormRules: &#123; name: [ &#123; required: true, message: &apos;请输入用户组名称&apos;, trigger: &apos;blur&apos; &#125;, &#123; min: 2, max: 8, message: &apos;长度在 2 到 8 个字符&apos;, trigger: &apos;blur&apos; &#125; ] &#125;, modifyGroupFormVisible: false, modifyGroupformLabelWidth: &apos;200px&apos;, modifyGroupForm: &#123; name: &apos;&apos; &#125; &#125; &#125;, created() &#123; this.fetchData() &#125;, methods: &#123; fetchData() &#123; getGroupList(this.params).then(res =&gt; &#123; this.groupList = res.results this.totalNum = res.count &#125;) &#125;, handleCurrentChange(val) &#123; this.params.page = val this.fetchData() &#125;, searchClick() &#123; this.params.page = 1 this.fetchData() &#125;, handleAddGroupBtn() &#123; this.addGroupFormVisible = true if (this.$refs[&apos;addGroupForm&apos;] !== undefined) &#123; this.$refs[&apos;addGroupForm&apos;].resetFields() &#125; &#125;, handleAddGroupSubmit(addGroupForm) &#123; this.$refs[addGroupForm].validate((valid) =&gt; &#123; if (valid) &#123; createGroup(this.addGroupForm).then(res =&gt; &#123; this.$message(&#123; message: &apos;用户组创建成功.&apos;, type: &apos;success&apos; &#125;) this.addGroupFormVisible = false this.$refs[&apos;addGroupForm&apos;].resetFields() this.fetchData() &#125;) &#125; else &#123; return false &#125; &#125;) &#125;, handleModifyGroupClick(obj) &#123; this.modifyGroupFormVisible = true this.modifyGroupForm = obj if (this.$refs[&apos;modifyGroupForm&apos;] !== undefined) &#123; this.$refs[&apos;modifyGroupForm&apos;].resetFields() &#125; &#125;, handleModifyGroupSubmit(modifyGroupForm) &#123; this.$refs[modifyGroupForm].validate((valid) =&gt; &#123; if (valid) &#123; updateGroup(this.modifyGroupForm.id, this.modifyGroupForm).then(res =&gt; &#123; this.$message(&#123; message: &apos;用户组更新成功.&apos;, type: &apos;success&apos; &#125;) this.modifyGroupFormVisible = false this.fetchData() &#125;) &#125; else &#123; return false &#125; &#125;) &#125;, handleDeleteGroupClick(id, name) &#123; this.$confirm(&apos;确定要删除 &quot;&apos; + name + &apos;&quot; 用户组吗？&apos;, &apos;提示&apos;, &#123; confirmButtonText: &apos;确定&apos;, cancelButtonText: &apos;取消&apos;, type: &apos;warning&apos; &#125;).then(() =&gt; &#123; deleteGroup(id).then(() =&gt; &#123; this.$message(&#123; type: &apos;success&apos;, message: &apos;删除成功!&apos; &#125;) this.fetchData() &#125;) &#125;).catch(() =&gt; &#123; this.$message(&#123; type: &apos;info&apos;, message: &apos;已取消删除&apos; &#125;) &#125;) &#125; &#125; &#125;&lt;/script&gt; 3. 页面展示 本文主要讲解前端如何调用后面的接口，以及数据的增删改查，并没有将vueAdmin-template这个模板如何应用交代清楚，还望各位自行研究，或加入我们开源群，一起学习。 群已满，加我好友拉入群：（需备注：来至开源知识库）]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-JWT用户认证》]]></title>
    <url>%2F2018%2F08%2F20%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-JWT%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E3%80%8B.html</url>
    <content type="text"><![CDATA[一起来学习下前后端分离之JWT用户认证 一、JWT理论https://www.jianshu.com/p/180a870a308a 二、实践使用GitHub：https://github.com/GetBlimp/django-rest-framework-jwt使用文档：http://getblimp.github.io/django-rest-framework-jwt/ 1. 安装JWT使用pip来安装 1$ pip install djangorestframework-jwt 2. 配置使用在你的setting.py文件中配置如下：12345678REST_FRAMEWORK = &#123; ······ &apos;DEFAULT_AUTHENTICATION_CLASSES&apos;: ( &apos;rest_framework_jwt.authentication.JSONWebTokenAuthentication&apos;, ······ ), ······&#125; 3. URL配置认证接口在你的urls.py文件中配置如下:123456from rest_framework_jwt.views import obtain_jwt_tokenurlpatterns = [ ······ url(r&apos;^api-token-auth/&apos;, obtain_jwt_token),] 4. 获取Token下面两种方式均可1234$ curl -X POST -d &quot;username=admin&amp;password=password123&quot; http://localhost:8000/api-token-auth/# 如果是使用Json请求，需要指定请求头部信息$ curl -X POST -H &quot;Content-Type: application/json&quot; -d &apos;&#123;&quot;username&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;password123&quot;&#125;&apos; http://localhost:8000/api-token-auth/ 5. 使用Token请求数据使用方法：1curl -H &quot;Authorization: JWT &lt;your_token&gt;&quot; http://localhost:8000/protected-url/ 6. 设置Token过期时间在你的setting.py文件中配置如下：123JWT_AUTH = &#123;&apos;JWT_EXPIRATION_DELTA&apos;: datetime.timedelta(seconds=3600),&#125;]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之《03.生产与消费》]]></title>
    <url>%2F2018%2F08%2F16%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8B%E3%80%8A03-%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E3%80%8B.html</url>
    <content type="text"><![CDATA[本文思考准备使用Python来做客户端（据了解使用go做客户端的很多），生产message 到NSQ消息队列，然后再消费掉，目的是想研究Python如何使用NSQ，以便我后面在开发项目中使用它。 启动nsqd相关服务具体配置启动方法请看上一篇文章，启动服务如下：12345$ supervisorctl statusnsqadmin RUNNING pid 15252, uptime 1 day, 0:25:41nsqd01 RUNNING pid 14720, uptime 1 day, 0:37:21nsqd02 RUNNING pid 14787, uptime 1 day, 0:36:05nsqlookupd RUNNING pid 3566, uptime 0:09:01 安装依赖123$ pip install -r requirements.txtpynsq==0.8.2tornado==4.5.3 生产者1234567891011121314$ vim nsq_create.pyimport nsqimport tornado.ioloopimport timedef pub_message(): writer.pub(&apos;test_topic&apos;, time.strftime(&apos;%H:%M:%S&apos;), finish_pub)def finish_pub(conn, data): print datawriter = nsq.Writer([&apos;127.0.0.1:4150&apos;])tornado.ioloop.PeriodicCallback(pub_message, 1000).start()nsq.run() 消费者12345678910111213$ vim nsq_consume.pyimport nsqdef handler(message): #print message print message.body return True# lookupd不能使用时，也可以直接使用nsqd来进行消费r = nsq.Reader(message_handler=handler, nsqd_tcp_addresses=[&apos;127.0.0.1:4150&apos;], topic=&apos;test_topic&apos;, channel=&apos;asdfxx&apos;, lookupd_poll_interval=15)#r = nsq.Reader(message_handler=handler, lookupd_http_addresses=[&apos;http://127.0.0.1:4161&apos;], topic=&apos;test_topic&apos;, channel=&apos;asdfxx&apos;, lookupd_poll_interval=15)nsq.run() 生产和消费数据运行生产脚本123456$ python nsq_create.pyOKOKOKOKOK 运行消费脚本123456$ python nsq_consume.py17:33:3317:33:3417:33:3517:33:3617:33:37 以上，通过python编写的生产和消费功能完成，大多数代码来源于官网，本文仅供学习理解使用。 NSQAdmin带大家认识一下NSQ的web页面 1、首页，会显示有多少个Topics 2、点击一个Topic，会显示详细的信息以及操作：可以清空、删除、暂停一个Topic Topic Message Queue - 显示这个topic的所有NSQd Host节点 Channel Message Queues - 显示所有消费通道 3、Node页，显示有多少个NSQd节点，以及版本和端口信息 4、点击Adderss地址，显示每个Node节点的Topics详情，有多少数据，以及数据积压情况 Depth - 数据积压数量 Memory+Disk - 占用的内存和磁盘空间 Messages - 总共处理的message数量 Client Host - 连接Topic的客户端 User-Agent - 客户端的UA Finshed - 完成处理多少message Connected - 连接时长 5、总共处理了多少message数量 6、NSQAdmin如果连接了多个nslookupd程序，将会在这里显示 这个界面还可以手动创建Topic和Channel。 参考文章：https://pynsq.readthedocs.io/en/latest/index.html]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>NSQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之《02.NSQ实践篇》]]></title>
    <url>%2F2018%2F08%2F15%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8B%E3%80%8A02-NSQ%E5%AE%9E%E8%B7%B5%E7%AF%87%E3%80%8B.html</url>
    <content type="text"><![CDATA[这是消息队列NSQ的姊妹篇，不足之处请指出！ 安装NSQ方法一首先搭建golang开发环境，这里就不详细讲了 注意一下，搭建golang环境时最好将bin目录添加到系统环境(path)里，省去了每次都要去bin目录里执行的麻烦 安装包管理器godep1go get github.com/tools/godep 执行完后检查godep是否已经安装在bin目录下，一般都会自动安装，如果没有，用go install手动安装下 安装依赖包assert 1go get github.com/bmizerany/assert 安装NSQ 1godep get github.com/bitly/nsq/... 如果安装成功，bin目录里就会出现一大堆nsq_…开头的可执行文件 PS：如果安装失败，采用方法二安装 方法二二进制包安装， 直接去 https://github.com/nsqio/nsq/releases 下载编译好的执行文件，将里面的可执行文件复制到bin目录下就可以使用了。 123wget https://github.com/nsqio/nsq/releases/download/v1.0.0-compat/nsq-1.0.0-compat.linux-amd64.go1.8.tar.gztar zxf nsq-1.0.0-compat.linux-amd64.go1.8.tar.gzmv nsq-1.0.0-compat.linux-amd64.go1.8 /usr/local/nsqd 运行NSQ运行单机nsqd服务nsqd是一个独立的服务，启动一个nsqd就可以完成message的收发，启动一个单机的nsqd很简单：12345678910111213141516171819202122232425262728293031/usr/local/nsqd/bin/nsqd#当然，我习惯使用supervisor来管理启动的程序：[root@tanshuai supervisor]# cat nsqd.conf [program:nsqd]command=/usr/local/nsqd/bin/nsqddirectory=/usr/local/nsqdautostart=trueautorestart=truestartsecs=10startretries=3user=rootlog_stdout=truelog_stderr=trueredirect_stderr=truestdout_logfile=/var/log/supervisor/nsqd.logstdout_logfile_maxbytes=100MBstdout_logfile_backups=10numprocs=1[root@tanshuai supervisor]# supervisorctl rereadnsqd: available[root@tanshuai supervisor]# supervisorctl add nsqdnsqd: added process group[root@tanshuai supervisor]# supervisorctl statusnsqd RUNNING pid 13703, uptime 0:00:11[root@tanshuai supervisor]# cat /var/log/supervisor/nsqd.log [nsqd] 2018/08/15 16:29:01.661329 nsqd v1.0.0-compat (built w/go1.8)[nsqd] 2018/08/15 16:29:01.661362 ID: 831[nsqd] 2018/08/15 16:29:01.661378 NSQ: persisting topic/channel metadata to nsqd.dat[nsqd] 2018/08/15 16:29:01.663372 TCP: listening on [::]:4150[nsqd] 2018/08/15 16:29:01.663411 HTTP: listening on [::]:4151 客户端可以使用http，也可以使用tcp来发送message 运行NSQ服务集群 首先启动nsqlookupd 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@tanshuai supervisor]# /usr/local/nsqd/bin/nsqlookupd --helpUsage of nsqlookupd: -broadcast-address string address of this lookupd node, (default to the OS hostname) (default &quot;tanshuai&quot;) -config string path to config file -http-address string &lt;addr&gt;:&lt;port&gt; to listen on for HTTP clients (default &quot;0.0.0.0:4161&quot;) -inactive-producer-timeout duration duration of time a producer will remain in the active list since its last ping (default 5m0s) -log-prefix string log message prefix (default &quot;[nsqlookupd] &quot;) -tcp-address string &lt;addr&gt;:&lt;port&gt; to listen on for TCP clients (default &quot;0.0.0.0:4160&quot;) -tombstone-lifetime duration duration of time a producer will remain tombstoned if registration remains (default 45s) -verbose enable verbose logging -version print version string[root@tanshuai supervisor]# /usr/local/nsqd/bin/nsqlookupd -broadcast-address=0.0.0.0 -http-address=0.0.0.0:4161 -tcp-address=0.0.0.0:4160[nsqlookupd] 2018/08/15 16:43:04.879524 nsqlookupd v1.0.0-compat (built w/go1.8)[nsqlookupd] 2018/08/15 16:43:04.879884 TCP: listening on [::]:4160[nsqlookupd] 2018/08/15 16:43:04.879925 HTTP: listening on [::]:4161^C[nsqlookupd] 2018/08/15 16:43:09.132779 HTTP: closing [::]:4161[nsqlookupd] 2018/08/15 16:43:09.132802 TCP: closing [::]:4160[root@tanshuai supervisor]# # 使用supervisord启动[root@tanshuai supervisor]# cat nsqlookupd.conf [program:nsqlookupd]command=/usr/local/nsqd/bin/nsqlookupd -broadcast-address=0.0.0.0 -http-address=0.0.0.0:4161 -tcp-address=0.0.0.0:4160directory=/usr/local/nsqdautostart=trueautorestart=truestartsecs=10startretries=3user=rootlog_stdout=truelog_stderr=trueredirect_stderr=truestdout_logfile=/var/log/supervisor/nsqlookupd.logstdout_logfile_maxbytes=100MBstdout_logfile_backups=10numprocs=1[root@tanshuai supervisor]# cat /var/log/supervisor/nsqlookupd.log [nsqlookupd] 2018/08/15 16:46:46.534278 nsqlookupd v1.0.0-compat (built w/go1.8)[nsqlookupd] 2018/08/15 16:46:46.534869 TCP: listening on [::]:4160[nsqlookupd] 2018/08/15 16:46:46.534902 HTTP: listening on [::]:4161 启动nsqd，并接入刚刚启动的nsqlookupd。为了方便测试，启动两个nsqd。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@tanshuai supervisor]# cat nsqd01.conf [program:nsqd01]command=/usr/local/nsqd/bin/nsqd -lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4150 -http-address=0.0.0.0:4151directory=/usr/local/nsqdautostart=trueautorestart=truestartsecs=10startretries=3user=rootlog_stdout=truelog_stderr=trueredirect_stderr=truestdout_logfile=/var/log/supervisor/nsqd01.logstdout_logfile_maxbytes=100MBstdout_logfile_backups=10numprocs=1[root@tanshuai supervisor]# cat nsqd02.conf [program:nsqd02]command=/usr/local/nsqd/bin/nsqd -lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4152 -http-address=0.0.0.0:4153# directory=/usr/local/nsqd #supervisor不允许相同的进程使用相同的directory，所以禁用掉，没影响。autostart=trueautorestart=truestartsecs=10startretries=3user=rootlog_stdout=truelog_stderr=trueredirect_stderr=truestdout_logfile=/var/log/supervisor/nsqd02.logstdout_logfile_maxbytes=100MBstdout_logfile_backups=10numprocs=1[root@tanshuai supervisor]# cat /var/log/supervisor/nsqd01.log [nsqd] 2018/08/15 16:52:43.646357 nsqd v1.0.0-compat (built w/go1.8)[nsqd] 2018/08/15 16:52:43.646391 ID: 831[nsqd] 2018/08/15 16:52:43.646459 NSQ: persisting topic/channel metadata to nsqd.dat[nsqd] 2018/08/15 16:52:43.647727 TCP: listening on [::]:4150[nsqd] 2018/08/15 16:52:43.647765 HTTP: listening on [::]:4151[nsqd] 2018/08/15 16:52:43.647833 LOOKUP(127.0.0.1:4160): adding peer[nsqd] 2018/08/15 16:52:43.647836 LOOKUP connecting to 127.0.0.1:4160[nsqd] 2018/08/15 16:52:43.649061 LOOKUPD(127.0.0.1:4160): peer info &#123;TCPPort:4160 HTTPPort:4161 Version:1.0.0-compat BroadcastAddress:0.0.0.0&#125;[nsqd] 2018/08/15 16:52:58.647932 LOOKUPD(127.0.0.1:4160): sending heartbeat[nsqd] 2018/08/15 16:53:13.647894 LOOKUPD(127.0.0.1:4160): sending heartbeat[nsqd] 2018/08/15 16:53:28.647928 LOOKUPD(127.0.0.1:4160): sending heartbeat 启动nsqadmin 1234567891011121314151617181920[root@tanshuai supervisor]# cat nsqadmin.conf [program:nsqadmin]command=/usr/local/nsqd/bin/nsqadmin -lookupd-http-address=127.0.0.1:4161directory=/usr/local/nsqdautostart=trueautorestart=truestartsecs=10startretries=3user=rootlog_stdout=truelog_stderr=trueredirect_stderr=truestdout_logfile=/var/log/supervisor/nsqadmin.logstdout_logfile_maxbytes=100MBstdout_logfile_backups=10numprocs=1[root@tanshuai supervisor]# cat /var/log/supervisor/nsqadmin.log [nsqadmin] 2018/08/15 17:04:23.622502 nsqadmin v1.0.0-compat (built w/go1.8)[nsqadmin] 2018/08/15 17:04:23.623083 HTTP: listening on [::]:4171 所有启动的进程 12345[root@tanshuai supervisor]# supervisorctl statusnsqadmin RUNNING pid 15252, uptime 0:01:05nsqd01 RUNNING pid 14720, uptime 0:12:45nsqd02 RUNNING pid 14787, uptime 0:11:29nsqlookupd RUNNING pid 14456, uptime 0:18:42 访问nsqadmin：http://127.0.0.1:4171/ 使用NSQ1、创建一个topic并发布一条消息接口：POST /pub1curl -d &quot;hello world&quot; http://127.0.0.1:4151/pub?topic=name 2、发送多条消息接口：POST /mpub1curl -d &quot;&lt;message01&gt;\n&lt;message02&gt;\n&lt;message03&gt;&quot; http://127.0.0.1:4151/mpub?topic=name 3、创建一个topic接口：POST /topic/create1curl -X POST http://127.0.0.1:4151/topic/create?topic=name 4、删除一个topic接口：POST /topic/delete1curl -X POST http://127.0.0.1:4151/topic/delete?topic=test 5、为现有的topic创建一个channel接口：POST /channel/create1curl -X POST &quot;http://127.0.0.1:4151/channel/create?topic=name&amp;channel=name&quot; 6、删除topic里的一个channel接口：POST /channel/delete1curl -X POST &quot;http://127.0.0.1:4151/channel/delete?topic=name&amp;channel=name&quot; 7、清空topic接口：POST /1curl -X POST http://127.0.0.1:4151/topic/empty?topic=name 8、清空channel接口：POST /channel/empty1curl -X POST http://127.0.0.1:4151/channel/empty?topic=name&amp;channel=name 9、统计数据接口：POST /stats1curl http://127.0.0.1:4151/stats 10、集群信息接口：POST /info1curl http://127.0.0.1:4151/info 以上为常用接口，还有其他接口看官网即可。 下一篇将介绍python如何使用NSQD队列进行message收发。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>NSQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之《01.NSQ理论篇》]]></title>
    <url>%2F2018%2F08%2F15%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8B%E3%80%8A01-NSQ%E7%90%86%E8%AE%BA%E7%AF%87%E3%80%8B.html</url>
    <content type="text"><![CDATA[最近一直在做开源项目，该项目需要使用消息中间件来转储下日志、消息、监控数据、服务器上报等数据，因此调研了下业界开源的消息队列，大致有：Kafka、Redis、Zeromq、Rebbitmq、NSQ等等； 前言考虑到开发周期的快速迭代，没办法把以上所有队列都通透研究一遍，初步跟开源团队讨论了下，且我司也有大规模使用NSQ的场景，于是我们就决定先将NSQ吃透，NSQ在分布式和实时上确实有一定的优势，也许其他队列简单能满足我们的需求，但“杀鸡用牛刀”也是我们想去尝试的，毕竟我们的开源项目都是尊崇所用到的技术都是业界最牛逼的开源解决方案。 NSQ与其他队列对比图 (图片来之golang2017开发者大会) NSQ介绍 NSQ是一个基于Go语言的分布式实时平台，代码托管在GitHub。 NSQ可用于大规模系统中的实时消息服务，并且每天能够处理数亿级别的消息。 NSQ具有分布式、去中心化的拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保障消息的可靠传输的特征。 NSQ非常容易配置和部署，且具有最大的灵活性，支持众多消息协议。 NSQ服务端介绍在使用NSQ服务之前，还是有必要了解下NSQ的几个核心组件，整个NSQ服务包含三个主要部分 nsqlookupd先看看官网的介绍： nsqdlookup是守护进程负责管理拓扑信息，客户端通过查询nsqlookupd来发现指定话题(topic)的生产者，并且nsqd节点广播话题(topic)和通道(channel)信息 简单的理解：nsqlookupd就是中心管理服务，它使用tc(默认度那可4160)管理nsqd服务，使用http(默认端口4161)管理nsqadmin服务，同事为客户端提供查询服务。 总的来说，nsqlookupd具有以下功能或特性： 唯一性 - 在一个NSQ服务中只有一个nsqlookupd服务。当然也可以在集群中部署多个nsqlookupd，但它们之间是没有关联的。 去中心化 - 及时nsqlookupd崩溃，也不会影响正在运行的nsqd服务 充当nsqd和nsqadmin信息交互的中间件 提供一个http查询服务，给客户端定时更新nsqd的地址目录 nsqadmin官网原话：是一套WEB UI，用来户汇聚集群的实时统计，并执行不同的管理任务 总的来说，nsqadmin具有以下功能或特性： 提供一个topic和channel统一管理的操作界面以及各种实时监控数据的展示，界面设计很简介、操作也很简单 展示所有message的数量，这个是装X神器 能够在后台创建topic和channel，不常用 nsqadmin的所有功能都必须依赖于nsqlookupd，nsqadmin只是向nsqlookupd传递用户操作并展示来至nsqlookupd的数据 nsqadmin默认的访问地址是:http://127.0.0.1:4171/ nsqd官方原话：nsqd是一个守护进程，负责接收、排队、投递消息给客户端 真正干活的就是这个服务，它主要负责message的收发，队列的维护；nsqd会默认监听一个tcp端口(4150)和一个http端口(4151)以及一个可选的https端口 总的来说，nsqd具有以下功能或特性： 对订阅了的同一个topic、同一个channel的消费者使用负载均衡策略(不是轮询) 只要channel存在，即使没有该channel的消费者，也会讲生产者的message缓存到队列汇总(注意消息的过时处理) 保证队列中的message至少会被消费一次，及时nsqd退出，也会讲队列中的消息暂存磁盘(结束进程等意外情况除外) 限定内存使用，能够配置nsqd中的每个channel队列在内存中缓存的message数量，一旦超出，message建会被缓存到磁盘中 topic，channel一旦建立将会一直存在，要及时在管理台或者代码清除无效的topic和channel，避免资源浪费 下面是官方的图，第一个channel(meteics)因为有多个消费者，所以触发了负载均衡机制。后面两个channel由于没有消费者，所有的message均会被缓存在相应的队列里，直到消费者出现。 这里想到一个问题是：如果一个channel只有生产者不停的在投递message，会不会导致服务器资源被耗尽？也许nsqd内部做了相应处理，但还是要避免这种情况出现。 NSQ生产与消费了解nsqlookupd，nsqd与客户端中消费者和生产者的关系 消费者消费者有良好总方式与nsqd建立连接： 消费者直连nsqd，这是最简单的方式，缺点是nsqd服务无法实现动态伸缩了(当然，自己去实现一个亦可以) 消费者通过http查询nsqlookupd获取所有nsqd的连接地址，然后再分别和折现nsqd建立连接(官网推荐的做法，也是我司在使用的方式)，但是客户端会不停的向nsqlookupd查询最新的nsqd地址目录。 官方的消费者模型： 生产者生产者必须连接nsqd去投递message(当然，可以连接到nsqlookupd，让nsqlookupd自动选择一个nsqd去完成投递) 如果生产者所连接的nsqd挂了，那么message就会投递失败，所以在客户端必须自己实现相应的备用方案。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>NSQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-权限》]]></title>
    <url>%2F2018%2F07%2F25%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E6%9D%83%E9%99%90%E3%80%8B.html</url>
    <content type="text"><![CDATA[DRF系列的姊妹篇，了解一下. 一、认证1. 启用DRF登录接口在drf源码文件rest_framework.urls.py中能看到如下信息：1234urlpatterns = [ url(r&apos;^login/$&apos;, login, login_kwargs, name=&apos;login&apos;), url(r&apos;^logout/$&apos;, logout, name=&apos;logout&apos;),] 会发现DRF在这里将登录、登出的接口都已经写好了，直接使用即可。 全局urls.py文件里增加如下配置，来启用登录接口：1234urlpatterns = [ ······ url(r&apos;^api-auth&apos;, include(&quot;rest_framework.urls&quot;, namespace=&quot;rest_framework&quot;)),] 然后，去页面登录： 输入账号登录，查看浏览器保存的Session： 你在数据库中，也能看到Session的存储信息. 2. User接口使用Session登录认证这里来个使用案例，了解下如何在App里使用认证；vim users/views.py123456789101112131415from rest_framework.authentication import SessionAuthenticationfrom rest_framework.permissions import IsAuthenticatedclass UserViewset(viewsets.ReadOnlyModelViewSet): &quot;&quot;&quot; retrieve: 返回指定用户信息 list: 返回用户列表 &quot;&quot;&quot; queryset = User.objects.all() serializer_class = UserSerializer authentication_classes = (SessionAuthentication, ) # 将认证的key, 存放在数据库里;[局部使用方法/可以全局配置] permission_classes = (IsAuthenticated, ) # 启用登录认证,即:登录就可以访问. 把上面的登录用户退出，然后访问User接口： 用户没有登录，不让访问信息，认证成功。只有在用户登录时，才可访问用户数据。 3. Session认证全局配置vim setting.py12345678REST_FRAMEWORK = &#123; ······ &apos;DEFAULT_AUTHENTICATION_CLASSES&apos;: ( &apos;rest_framework_jwt.authentication.JSONWebTokenAuthentication&apos;, &apos;rest_framework.authentication.SessionAuthentication&apos;, &apos;rest_framework.authentication.BasicAuthentication&apos; ),&#125; 当然，除了SessionAuthentication认证以外，还有其他认证方式，也可以配置多个，取决于你使用到什么就配置什么好了。 那现在就可以去将User里的Session配置去掉，访问效果一样。 本文只讲了SessionAuthentication的认证方式和使用，其他认证方式在后面文章中会陆续讲解。 那认证讲完了，接下来看看权限. 二、权限1. 简单了解权限其实在上面已经使用过权限了，你注意到了吗？ 上面users/views.py文件里写到：1permission_classes = (IsAuthenticated, ) # 启用登录认证,即:登录就可以访问. 这个配置是指定登录认证，其实还有很多种认证方式，如下： IsAuthenticated - 登录权限 IsAdminUser - 管理员权限 IsAuthenticatedOrReadOnly - 登录或只读权限 DjangoModelPermissions - 模型权限验证权限 DjangoModelPermissionsOrAnonReadOnly - 模型权限只读认证 DjangoObjectPermissions - 对象权限认证 当然，这些认证都基于BasePermission权限，源码内容如下：12345678910111213141516class BasePermission(object): &quot;&quot;&quot; A base class from which all permission classes should inherit. &quot;&quot;&quot; def has_permission(self, request, view): &quot;&quot;&quot; Return `True` if permission is granted, `False` otherwise. &quot;&quot;&quot; return True def has_object_permission(self, request, view, obj): &quot;&quot;&quot; Return `True` if permission is granted, `False` otherwise. &quot;&quot;&quot; return True has_permission，检查权限 has_object_permission，检查对象权限 我们也可以继承BasePermission权限，进行自定义权限。 2. DjangoModelPermissionsDjango模型权限，是我们用的最多的，这里也准备对它的使用展开讲解下。 1234567891011class DjangoModelPermissions(BasePermission): perms_map = &#123; &apos;GET&apos;: [], &apos;OPTIONS&apos;: [], &apos;HEAD&apos;: [], &apos;POST&apos;: [&apos;%(app_label)s.add_%(model_name)s&apos;], &apos;PUT&apos;: [&apos;%(app_label)s.change_%(model_name)s&apos;], &apos;PATCH&apos;: [&apos;%(app_label)s.change_%(model_name)s&apos;], &apos;DELETE&apos;: [&apos;%(app_label)s.delete_%(model_name)s&apos;], &#125; 从它的源码中能看出，这个权限给我们设置好了很多方法的权限认证，增删改查的接口操作都给做了，我们只需要使用它即可。不得不感叹DRF的强大，以前在使用MVC框架模式开发中，权限得一个个去控制，其过程可以用恶心来形容。 DRF的安全请求方法:在rest_framework.permissions中配置了如下安全方法，1SAFE_METHODS = (&apos;GET&apos;, &apos;HEAD&apos;, &apos;OPTIONS&apos;) 也就是只允许这些方法在没有权限的情况下，能访问资源。 2.1 配置DjangoModelPermissions权限vim setting.py123456REST_FRAMEWORK = &#123; ······ &apos;DEFAULT_PERMISSION_CLASSES&apos;: ( &apos;rest_framework.permissions.DjangoModelPermissions&apos;, ),&#125; 直接在全局配置文件里进行配置， 优点： 不用在每个视图里去写了，缺点： 如果你一但写一个viewset没有queryset的话，将会出错，解决：使用permission_classes = (IsAuthenticated, ) 写在你的viewset里，自己定义其他认证方式，覆盖全局的配置即可。 2.2 权限验证使用普通用户登录，查看idc的一个资源： 可以看到只允许安全方法中 GET方法 来访问数据，而曾，删，改方法，都被禁止。 给nicksors-1授权idc的删除权限：1234567891011121314151617In [1]: from django.contrib.auth.models import UserIn [2]: from django.contrib.auth.models import PermissionIn [3]: user = User.objects.get(username=&apos;nicksors-1&apos;)In [4]: Permission.objects.get(pk=21)Out[4]: &lt;Permission: idcs | idc | Can delete idc&gt;In [5]: del_idc = Permission.objects.get(pk=21)In [6]: user.user_permissions = [del_idc]manage.py:1: RemovedInDjango20Warning: Direct assignment to the forward side of a many-to-many set is deprecated due to the implicit save() that happens. Use user_permissions.set() instead. #!/usr/bin/env pythonIn [7]: user.user_permissions.all()Out[7]: &lt;QuerySet [&lt;Permission: idcs | idc | Can delete idc&gt;]&gt; 再次刷新页面，你能看到DELETE权限已经有了，其他权限就不挨个测试了哈，原理一样。 2.3 自定义GET权限因为GET方法在DjangoModelPermissions权限认证中，属于安全权限， 意思是可以直接查看。 试想一下，如果我们有接口不想被GET查看，那如何控制呢？这就需要我们自己来定义实现了。 动刀：在 setting.py同级目录下创建permissions.py文件：12345678910111213141516171819202122232425from rest_framework.permissions import DjangoModelPermissionsclass ModelPermissions(DjangoModelPermissions): def get_custom_perms(self, view, method): # 接收请求的视图，以及请求方法 if hasattr(view, &apos;extra_perm_map&apos;): # 判断视图是否定义了extra_perm_map if isinstance(view.extra_perm_map, dict): # 判断extra_perm_map属性必须是一个dict return view.extra_perm_map.get(method, []) # 返回extra_perm_map里定义的参数，默认为空list return [] def has_permission(self, request, view): # Workaround to ensure DjangoModelPermissions are not applied # to the root view when using DefaultRouter. if getattr(view, &apos;_ignore_model_permissions&apos;, False): return True if not request.user or ( not request.user.is_authenticated and self.authenticated_users_only): return False queryset = self._queryset(view) perms = self.get_required_permissions(request.method, queryset.model) perms.extend(self.get_custom_perms(view, request.method)) # 两个列表相加，将请求方法加入DjangoModelPermissions里的perms_map中. return request.user.has_perms(perms) 自定义了ModelPermissions权限，继承DjangoModelPermissions权限类； 上面这一串代码其实就是在更改DjangoModelPermissions里的perms_map列表，将perms_map列表里原有GET = [] 改写成我们自定义的权限认证信息. 且看下面如何使用。 2.4 配置全局使用在全局配置文件中将DjangoModelPermissions注释，换成我们上面自定义的ModelPermissions权限类1234567REST_FRAMEWORK = &#123; ······ &apos;DEFAULT_PERMISSION_CLASSES&apos;: ( # &apos;rest_framework.permissions.DjangoModelPermissions&apos;, &apos;ops.permissions.ModelPermissions&apos;, ),&#125; 2.5 使用自定义GET权限还是拿User作为示例：12345678910111213class UserViewset(viewsets.ReadOnlyModelViewSet): &quot;&quot;&quot; retrieve: 返回指定用户信息 list: 返回用户列表 &quot;&quot;&quot; queryset = User.objects.all() serializer_class = UserSerializer ······ extra_perm_map = &#123; &quot;GET&quot;: [&apos;auth.view_user&apos;, ] &#125; 这里extra_perm_map字段指定了GET方法，意思是User接口的GET方法得有auth.view_user权限才能访问。 访问User接口页面： 嗯，显示没有GET权限，满足我们的需求，哈哈。 当然，auth.view_user权限默认是没有的，你需要添加权限，然后赋权给登录的用户，才能使用这个权限。 3 视图接口控制权限DjangoModelPermissions的perms_map变量里记录着增删改查的权限，那viewset视图如何控制这些权限呢？ 先介绍下rest_framework.viewsets里的模型类： 视同中常用的模型是ModelViewSet，因为它帮我们实现了增删改查操作，那有些接口只能有增加权限，这里如何控制？ 答案很简单：通过在视图里指定模型方法来控制权限，例如：123class UserViewset(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.UpdateModelMixin,): 相信你很容易能看出，这个接口只允许GET和PUT请求，我们来访问看看 获取用户列表是没问题的，这是mixins.ListModelMixin定义的功能； 获取和更新单个用户： 提示GET方法不被允许，而PUT可以，什么原因呢？如果有好好看文章，相信你知道是什么原因！这里就不细说了。 视图的接口权限，就是通过以上方式来控制的。 写到这里，作者将自己常用的权限使用方法分享给大家啦，当然肯定有些信息介绍的没那么详细，也适当锻炼下大家看看官网或者其他文章，哈哈。 文完。]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
        <tag>Permission</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-搜索》]]></title>
    <url>%2F2018%2F07%2F24%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E6%90%9C%E7%B4%A2%E3%80%8B.html</url>
    <content type="text"><![CDATA[在GenericAPIView类里有个get_queryset()方法，重写此方法允许以多种不同方式自定义视图返回的查询集。 例如：搜索用户12345678910111213141516class UserViewset(viewsets.ReadOnlyModelViewSet): &quot;&quot;&quot; retrieve: 返回指定用户信息 list: 返回用户列表 &quot;&quot;&quot; queryset = User.objects.all() serializer_class = UserSerializer def get_queryset(self): queryset = super(UserViewset, self).get_queryset() username = self.request.query_params.get(&quot;username&quot;, &quot;&quot;) if username: queryset = queryset.filter(username__icontains=username) return queryset 请求：1GET http://127.0.0.1:8000/users/?username=admin 返回：1234567891011121314151617HTTP 200 OKAllow: GET, HEAD, OPTIONSContent-Type: application/jsonVary: Accept&#123; &quot;count&quot;: 1, &quot;next&quot;: null, &quot;previous&quot;: null, &quot;results&quot;: [ &#123; &quot;id&quot;: 96, &quot;username&quot;: &quot;admin&quot;, &quot;email&quot;: &quot;tanshuai@nicksors.cc&quot; &#125; ]&#125; 而直接更改get_queryset这个方法来实现搜索功能，是比较原始的。项目中一般不会使用这种方法，而是使用Django-Filter来实现。 GitHub项目地址：https://github.com/carltongibson/django-filter 安装使用pip安装即可1pip install django-filter 在setting.py文件里的INSTALLED_APPS配置下添加django_filters1234INSTALLED_APPS = [ ... &apos;django_filters&apos;,] 使用创建过滤器如果你有多个App，需要在每个App目录下创建filters文件，然后写好过滤器，提供该App使用。这里以User App为例，创建过滤器内容如下： vim users/filters.py1234567891011121314from django.contrib.auth import get_user_modelUser = get_user_model()import django_filtersclass UserFilter(django_filters.FilterSet): &quot;&quot;&quot; 用户过滤器类 &quot;&quot;&quot; username = django_filters.CharFilter(name=&quot;username&quot;, lookup_expr=&apos;icontains&apos;) class Meta: model = User fields = [&apos;username&apos;, ] 技术点说明： 定义一个用户过滤器类，该类继承django_filters的FilterSet子类； django_filters这里使用了CharFilte，其实还有很多其他Filte，看看源码不难发现； 参数：name 指定搜索字段，lookup_expr 指定匹配规则（强制匹配或是模糊匹配等） Meta 里的fields字段，是指定通过什么字段来进行匹配搜索结果。 使用搜索功能1）views视图12345678910111213141516from django_filters.rest_framework import DjangoFilterBackendfrom django.contrib.auth import get_user_modelfrom .filters import UserFilterUser = get_user_model()class UserViewset(viewsets.ReadOnlyModelViewSet): &quot;&quot;&quot; retrieve: 返回指定用户信息 list: 返回用户列表 &quot;&quot;&quot; serializer_class = UserSerializer filter_backends = (DjangoFilterBackend, ) filter_class = UserFilter filter_fields = (&quot;username&quot;,) 导入DjangoFilterBackend，作为filter_backends的参数，是固定写法； filter_class 调用我们上面写的用户过滤器类，使用我们自己写的规则来进行过滤； filter_fields 指定哪个字段进行搜索。 2）页面搜索 在API页面上首先会出现 “过滤器” 点击按钮，点击后可以通过我们定义好的字段进行搜索，返回相应结果。 配置全局搜索配置全局搜索的好处不言而喻，本着不希望在viewset里写过多的代码（而且是重复的），那可以在setting.py文件配置如下： 123REST_FRAMEWORK = &#123; &apos;DEFAULT_FILTER_BACKENDS&apos;: (&apos;django_filters.rest_framework.DjangoFilterBackend&apos;,),&#125; 搜索效果一样，不一样的是你不用再每个viewset里写filter_backends = (DjangoFilterBackend, )这个配置了。 高级搜索: Q使用Django自带的Q来进行规则匹配，达到高级搜索需求1234567891011121314151617import django_filtersfrom django.db.models import Qfrom .models import Idcclass IdcFilter(django_filters.FilterSet): &quot;&quot;&quot; IDC机房过滤器类 &quot;&quot;&quot; name = django_filters.CharFilter(method=&apos;search_idc&apos;) def search_idc(self, queryset, name, value): return queryset.filter(Q(letter__icontains=value) | Q(name__icontains=value)) class Meta: model = Idc fields = [&apos;name&apos;, ] 可以按照 letter 字段和 name 字段来进行搜索，都能搜出想要的结果。 到这里，搜索功能也就做完了，不足之处多多指教，感谢关注。]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
        <tag>Filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-分页》]]></title>
    <url>%2F2018%2F07%2F23%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E5%88%86%E9%A1%B5%E3%80%8B.html</url>
    <content type="text"><![CDATA[DRF 分页了解一下 ^_^. 一、PageNumberPagination此分页样式在请求查询参数中接受单个数字页码。 请求：1GET http://127.0.0.1:8000/users/?page=3 返回：12345678910111213HTTP 200 OK&#123; &quot;count&quot;: 197, &quot;next&quot;: &quot;http://127.0.0.1:8000/users/?page=4&quot;, &quot;previous&quot;: &quot;http://127.0.0.1:8000/users/?page=2&quot;, &quot;results&quot;: [ &#123; &quot;id&quot;: 21, &quot;username&quot;: &quot;pwd-2&quot;, &quot;email&quot;: &quot;pwd-2@nicksors.cc&quot; &#125;, ]&#125; 1. 配置方法要全局启用PageNumberPagination分页样式，请使用以下配置，并根据需要设置PAGE_SIZE：1234REST_FRAMEWORK = &#123; &apos;DEFAULT_PAGINATION_CLASS&apos;: &apos;rest_framework.pagination.PageNumberPagination&apos;, &apos;PAGE_SIZE&apos;: 10 # 每页显示十条数据&#125; PageNumberPagination类允许你重写属性来修改分页的样式，其中有部分常用属性如下： page_size - 表示页面大小的数值。如果设置，则会覆盖PAGE_SIZE设置。默认为与PAGE_SIZE设置键相同的值。 其他的参数，官网介绍比较详细。 2. 基本使用2.1 配置setting.py123456$ vim setting.pyREST_FRAMEWORK = &#123; #分页 &apos;PAGE_SIZE&apos;:10 #每页显示多少条数据&#125; 2.2 App视图使用1234567891011121314$ vim users/views.pyfrom rest_framework.pagination import PageNumberPaginationclass UserViewset(viewsets.ReadOnlyModelViewSet): &quot;&quot;&quot; retrieve: 返回指定用户信息 list: 返回用户列表 &quot;&quot;&quot; queryset = User.objects.all() serializer_class = UserSerializer pagination_class = PageNumberPagination # viewsets.ReadOnlyModelViewSet 里提供的 pagination_class 方法，可以从源码里看出，这里直接指定PageNumberPagination使用即可。 2.3 访问显示 很简单，基本就完成了分页需求。 2.4 全局配置如果我们有非常多个非常多的App，非常多的viewset，那是不是需写很多次呢？答案是否定的。其实我们可以实现配置一次，全局生效. setting.py 1234REST_FRAMEWORK = &#123; &apos;DEFAULT_PAGINATION_CLASS&apos;: &apos;rest_framework.pagination.PageNumberPagination&apos;, &apos;PAGE_SIZE&apos;: 10,&#125; views.py去除viewset 视图里的pagination_class = PageNumberPagination 访问结果与上面一样。 2.5 自定义分页类page_size是分页的一个BUG，如下面请求： 1GET http://127.0.0.1:8000/users/?page=5&amp;page_size=5 这里指定page_size显示5条数据，但结果依旧是默认的10条，如下： 这是什么原因导致的呢？通过调试源码发现问题： 12345678910111213141516171819202122class PageNumberPagination(BasePagination): ···略··· page_size_query_param = None ···略··· def get_page_size(self, request): if self.page_size_query_param: try: return _positive_int( request.query_params[self.page_size_query_param], strict=True, cutoff=self.max_page_size ) except (KeyError, ValueError): pass return self.page_size ···略··· 原因：上面get_page_size函数调用page_size_query_param有问题，我们在前设置page_size_query_param = ‘page_size’ 不生效。所以导致get_page_size在判断的时候直接返回了self.page_size 而这个值就是默认的10条。 解决：通过分析我们找到了原因，解决办法也就清晰了，直接将get_page_size重写即可，来看我这边如何处理 1）编写自定义分页类 vim opt/pagination.py1234567891011121314from rest_framework.pagination import PageNumberPaginationclass Pagination(PageNumberPagination): def get_page_size(self, request): try: page_size = int(request.query_params.get(&apos;page_size&apos;, 0)) if page_size &lt; 0: return self.page_size return page_size except: pass return self.page_size 2）配置全局调用vim setting.py1234REST_FRAMEWORK = &#123; &apos;DEFAULT_PAGINATION_CLASS&apos;: &apos;ops.pagination.Pagination&apos;, # 这里调用我们写的自定义分页类 &apos;PAGE_SIZE&apos;: 10,&#125; 3）查看效果 没问题，使用DRF分页就到这里。 二、后言DRF除了提供PageNumberPagination分页以外，还提供了LimitOffsetPagination和CursorPagination，那这两个分页大家可以参考下官网的介绍，作者使用后两个分页不是太多，那这里就不做过多介绍了。 下一篇将介绍DRF的搜索及使用，尽请关注。]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
        <tag>Pagination</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus系列之《HTTP-API的使用》]]></title>
    <url>%2F2018%2F07%2F20%2FPrometheus%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AHTTP-API%E7%9A%84%E4%BD%BF%E7%94%A8%E3%80%8B.html</url>
    <content type="text"><![CDATA[非常惭愧，有两周多没更新文档了。一方面是工作较忙原因，另一方面是技术也需要实践，积累。今天接着咱们的技术文章分享，最近在研究Prometheus，后续也会有这方面的技术文件与大家分享哈。 在Prometheus里，是通过/api/v1接口来访问数据的。 一、格式介绍1、API响应的格式是JSON，每个成功的API请求都会返回2xx状态代码。2、API处理程序的无效请求将返回JSON错误对象以及以下HTTP响应代码之一： 400 Bad Request 参数丢失或不正确； 422 Unprocessable Entity 当表达式不能执行时（RCFC49）； 503 Service Unavailable 当查询超时或中止时。。 3、JSON响应格式如下：123456789&#123; &quot;status&quot;: &quot;success&quot; | &quot;error&quot;, &quot;data&quot;: &lt;data&gt;, // Only set if status is &quot;error&quot;. The data field may still hold // additional data. &quot;errorType&quot;: &quot;&lt;string&gt;&quot;, &quot;error&quot;: &quot;&lt;string&gt;&quot;&#125; 二、表达式查询1. 立刻查询1GET /api/v1/query URL 查询参数： query=&lt;string&gt;: Prometheus表达式查询字符串. time=&lt;rfc3339 | unix_timestamp&gt;: 指定查询的时间戳。可选的. timeout=&lt;duration&gt;: 超时时间，可选的. 如果省略time参数，则使用当前服务器时间。 查询结果的数据部分具有以下格式：1234&#123; &quot;resultType&quot;: &quot;matrix&quot; | &quot;vector&quot; | &quot;scalar&quot; | &quot;string&quot;, &quot;result&quot;: &lt;value&gt;&#125; 是指查询结果数据，具有不同的格式，具体取决于resultType。 下面使用时间表达式2018-07-18T02:10:51.781Z演示示例：123456789101112131415161718192021222324252627282930313233343536373839404142[root@tanshuai ~]# curl -s &apos;http://localhost:9090/api/v1/query?query=up&amp;time=2018-07-18T02:10:51.781Z&apos;|python -mjson.tool&#123; &quot;data&quot;: &#123; &quot;result&quot;: [ &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;114.67.154.180:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;value&quot;: [ 1531879851.781, &quot;1&quot; ] &#125;, &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;mysql&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &quot;value&quot;: [ 1531879851.781, &quot;1&quot; ] &#125;, &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;node_exporter&quot;, &quot;job&quot;: &quot;centos_linux&quot; &#125;, &quot;value&quot;: [ 1531879851.781, &quot;1&quot; ] &#125; ], &quot;resultType&quot;: &quot;vector&quot; &#125;, &quot;status&quot;: &quot;success&quot;&#125; 2. 范围查询1GET /api/v1/query_range URL 查询参数： query=&lt;string&gt;: Prometheus表达式查询字符串. start=&lt;rfc3339 | unix_timestamp&gt;: 开始时间戳. end=&lt;rfc3339 | unix_timestamp&gt;: 结束时间戳. step=&lt;duration&gt;: 查询平率步长. timeout=&lt;duration&gt;: 超时时间，可选的. 查询结果的数据部分具有以下格式：1234&#123; &quot;resultType&quot;: &quot;matrix&quot;, &quot;result&quot;: &lt;value&gt;&#125; 以下示例设置表达式在30秒范围内，查询分辨率为15秒：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@tanshuai ~]# curl -s &apos;http://localhost:9090/api/v1/query_range?query=up&amp;start=2018-07-18T02:10:30.781Z&amp;end=2018-07-18T02:11:00.781Z&amp;step=15s&apos;|python -mjson.tool&#123; &quot;data&quot;: &#123; &quot;result&quot;: [ &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;114.67.154.180:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;values&quot;: [ [ 1531879830.781, &quot;1&quot; ], [ 1531879845.781, &quot;1&quot; ], [ 1531879860.781, &quot;1&quot; ] ] &#125;, &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;mysql&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &quot;values&quot;: [ [ 1531879830.781, &quot;1&quot; ], [ 1531879845.781, &quot;1&quot; ], [ 1531879860.781, &quot;1&quot; ] ] &#125;, &#123; &quot;metric&quot;: &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;node_exporter&quot;, &quot;job&quot;: &quot;centos_linux&quot; &#125;, &quot;values&quot;: [ [ 1531879830.781, &quot;1&quot; ], [ 1531879845.781, &quot;1&quot; ], [ 1531879860.781, &quot;1&quot; ] ] &#125; ], &quot;resultType&quot;: &quot;matrix&quot; &#125;, &quot;status&quot;: &quot;success&quot;&#125; 根据平率的次数返回相对应次数的结果。 三、查询原数据按标签匹配器查找系列 1GET /api/v1/series URL 查询参数： match[]=&lt;series_selector&gt;: 重复的系列选择器参数，用于选择要返回的系列。必须至少提供一个match []参数. start=&lt;rfc3339 | unix_timestamp&gt;: 开始时间戳. end=&lt;rfc3339 | unix_timestamp&gt;: 结束时间戳. 查询结果的数据部分包含一个对象列表，这些对象包含标识每个系列的标签名称/值对。 以下示例返回匹配到up或process_start_time_seconds{job=&quot;prometheus&quot;}的系列结果：1234567891011121314151617181920212223242526272829303132333435363738394041[root@tanshuai ~]# curl -s -g &apos;http://localhost:9090/api/v1/series?match[]=up&amp;match[]=process_start_time_seconds&#123;job=&quot;prometheus&quot;&#125;&apos;|python -mjson.tool&#123; &quot;data&quot;: [ &#123; &quot;__name__&quot;: &quot;process_start_time_seconds&quot;, &quot;instance&quot;: &quot;114.67.154.180:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &#123; &quot;__name__&quot;: &quot;process_start_time_seconds&quot;, &quot;instance&quot;: &quot;127.0.0.1:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;114.67.154.180:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;127.0.0.1:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;db-127.0.0.1&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;mysql&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &#123; &quot;__name__&quot;: &quot;up&quot;, &quot;instance&quot;: &quot;node_exporter&quot;, &quot;job&quot;: &quot;centos_linux&quot; &#125; ], &quot;status&quot;: &quot;success&quot;&#125; 1. 查询标签值1GET /api/v1/label/&lt;label_name&gt;/values JSON响应的数据部分是字符串标签名称的列表。 此示例查询job标签的所有标签数据：1234567891011121314151617181920[root@tanshuai ~]# curl -s http://localhost:9090/api/v1/label/job/values|python -mjson.tool&#123; &quot;data&quot;: [ &quot;centos_linux&quot;, &quot;mysql&quot;, &quot;prometheus&quot; ], &quot;status&quot;: &quot;success&quot;&#125;[root@tanshuai ~]# curl -s http://localhost:9090/api/v1/label/instance/values|python -mjson.tool&#123; &quot;data&quot;: [ &quot;114.67.154.180:9090&quot;, &quot;127.0.0.1:9090&quot;, &quot;db-127.0.0.1&quot;, &quot;mysql&quot;, &quot;node_exporter&quot; ], &quot;status&quot;: &quot;success&quot;&#125; 四、Targets1GET /api/v1/targets 只返回当前活跃的targets：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@tanshuai ~]# curl -s http://localhost:9090/api/v1/targets|python -mjson.tool&#123; &quot;data&quot;: &#123; &quot;activeTargets&quot;: [ &#123; &quot;discoveredLabels&quot;: &#123; &quot;__address__&quot;: &quot;114.67.154.180:9104&quot;, &quot;__metrics_path__&quot;: &quot;/metrics&quot;, &quot;__scheme__&quot;: &quot;http&quot;, &quot;instance&quot;: &quot;mysql&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &quot;health&quot;: &quot;up&quot;, &quot;labels&quot;: &#123; &quot;instance&quot;: &quot;mysql&quot;, &quot;job&quot;: &quot;mysql&quot; &#125;, &quot;lastError&quot;: &quot;&quot;, &quot;lastScrape&quot;: &quot;2018-07-18T11:24:03.865575164+08:00&quot;, &quot;scrapeUrl&quot;: &quot;http://114.67.154.180:9104/metrics&quot; &#125;, &#123; &quot;discoveredLabels&quot;: &#123; &quot;__address__&quot;: &quot;114.67.154.180:9090&quot;, &quot;__metrics_path__&quot;: &quot;/metrics&quot;, &quot;__scheme__&quot;: &quot;http&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;health&quot;: &quot;up&quot;, &quot;labels&quot;: &#123; &quot;instance&quot;: &quot;114.67.154.180:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;lastError&quot;: &quot;&quot;, &quot;lastScrape&quot;: &quot;2018-07-18T11:23:58.310157271+08:00&quot;, &quot;scrapeUrl&quot;: &quot;http://114.67.154.180:9090/metrics&quot; &#125;, &#123; &quot;discoveredLabels&quot;: &#123; &quot;__address__&quot;: &quot;114.67.154.180:9100&quot;, &quot;__metrics_path__&quot;: &quot;/metrics&quot;, &quot;__scheme__&quot;: &quot;http&quot;, &quot;instance&quot;: &quot;node_exporter&quot;, &quot;job&quot;: &quot;centos_linux&quot; &#125;, &quot;health&quot;: &quot;up&quot;, &quot;labels&quot;: &#123; &quot;instance&quot;: &quot;node_exporter&quot;, &quot;job&quot;: &quot;centos_linux&quot; &#125;, &quot;lastError&quot;: &quot;&quot;, &quot;lastScrape&quot;: &quot;2018-07-18T11:23:57.005890999+08:00&quot;, &quot;scrapeUrl&quot;: &quot;http://114.67.154.180:9100/metrics&quot; &#125; ] &#125;, &quot;status&quot;: &quot;success&quot;&#125; 五、告警管理1GET /api/v1/alertmanagers 获取当前活跃的告警信息：1234567[root@tanshuai ~]# curl -s http://localhost:9090/api/v1/alertmanagers|python -mjson.tool&#123; &quot;data&quot;: &#123; &quot;activeAlertmanagers&quot;: [] &#125;, &quot;status&quot;: &quot;success&quot;&#125; 六、TSDB Admin APIs这些是为高级用户公开数据库功能的API。除非设置了–web.enable-admin-api，否则不会启用这些API。 1. 配置在启动时加上如下参数，即可启动TSDB功能。1/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml --storage.tsdb.path=/data/tsdb/ --web.enable-admin-api 2. 快照快照会将所有当前数据的快照创建到TSDB数据目录下的快照/ - 中，并将该目录作为响应返回。1POST /api/v1/admin/tsdb/snapshot 123456789101112131415161718192021[root@tanshuai ~]# curl -s -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot|python -mjson.tool&#123; &quot;data&quot;: &#123; &quot;name&quot;: &quot;20180718T034116Z-3b894be8bf313d55&quot; &#125;, &quot;status&quot;: &quot;success&quot;&#125;[root@tanshuai ~]# ls -lh /data/tsdb/总用量 4.0K-rw------- 1 root root 6 7月 18 11:39 lockdrwxr-xr-x 7 root root 209 7月 18 11:41 snapshotsdrwxr-xr-x 2 root root 20 7月 18 11:39 wal[root@tanshuai ~]# ls -lh /data/tsdb/snapshots/总用量 0drwxr-xr-x 3 root root 40 7月 18 11:40 20180718T034030Z-612639dc1ae4d372drwxr-xr-x 3 root root 40 7月 18 11:40 20180718T034055Z-d813bcea67bd979drwxr-xr-x 3 root root 40 7月 18 11:40 20180718T034059Z-54e2ebca2b8fcde6drwxr-xr-x 3 root root 40 7月 18 11:41 20180718T034106Z-3f6e502443f56c1drwxr-xr-x 3 root root 40 7月 18 11:41 20180718T034116Z-3b894be8bf313d55 snapshots目录存储着每次快照的版本,以及快照日期。 3. Delete Seriesv2.1中的新功能 如果成功，则返回2041POST /api/v1/admin/tsdb/delete_series URL 查询参数： match[]=&lt;series_selector&gt;: 选择要删除的系列的重复标签匹配器参数。必须至少提供一个match []参数。. start=&lt;rfc3339 | unix_timestamp&gt;: 开始时间戳. end=&lt;rfc3339 | unix_timestamp&gt;: 结束时间戳. 注意，如果不提供开始和结束时间，将会删除match[]在数据库中匹配到的所有数据！1curl -XPOST -g &apos;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=up&amp;match[]=process_start_time_seconds&#123;job=&quot;prometheus&quot;&#125;&apos; 4. Clean TombstonesCleanTombstones从磁盘中删除已删除的数据并清理现有的逻辑删除。这可以在删除系列后使用以释放空间。 如果成功，则返回204。1POST /api/v1/admin/tsdb/clean_tombstones 这不需要参数或正文。1$ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones v2.1中的新功能. 文完。]]></content>
      <categories>
        <category>Prometheus</category>
      </categories>
      <tags>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《Python操作ZabbixAPI了解一下》]]></title>
    <url>%2F2018%2F07%2F04%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8APython%E6%93%8D%E4%BD%9CZabbixAPI%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言，本文讲解Zabbix API的使用及经验技巧。 一、Zabbix API介绍Zabbix api为批量操作，第三方软件集成以及其他可编程的操作提供可编程接口。所有的zabbix的移动客户端都是基于api开发的，甚至原生的web前端也是基于他之上的。它允许你通过json RPC协议来创建，获取，更新zabbix的对象，当然了你可以通过它做任何你想做的操作。 zabbix API提供的主要的功能 远程管理zabbix的配置； 远程检索配置和历史信息。 API的请求 Zabbix 采用的是Json-RPC实现的。这也就是说，zabbix请求和响应都是Json类型，传的参数的类型也应该是Json类型。 二、API请求方式Zabbix API是通过api_jsonrpc.php文件来接收一个HTTP POST请求进行执行指令的，例如，你安装好的Zabbix地址为：http://you_ip/zabbix，那么请求方式如下：12345# 请求头部为：&#123;&quot;Content-Type&quot;: &quot;application/json-rpc&quot;&#125;# 请求的URL为：http://you_ip/zabbix/api_jsonrpc.php 三、用法案例接下来以Python调用API为例，进行演示。 3.1、测试连接是否成功1234567import requestsimport jsonurl = "http://172.16.194.128/zabbix/api_jsonrpc.php" header = &#123;"Content-Type": "application/json-rpc"&#125;data = &#123;"jsonrpc":"2.0","method":"apiinfo.version","id":1,"auth":None,"params":&#123;&#125;&#125;request = requests.post(url=url, headers=header, data=json.dumps(data))print request.content 上面的data用来测试是否可连接成功，官方文档中auth的值为null，但在python中null用None表示，所以需要改成None。 成功后返回值如下，会显示zabbix版本1&#123;&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:&quot;3.4.9&quot;,&quot;id&quot;:1&#125; 3.2、获取Token1234567import requestsimport jsonurl = &quot;http://172.16.194.128/zabbix/api_jsonrpc.php&quot; header = &#123;&quot;Content-Type&quot;: &quot;application/json-rpc&quot;&#125;data = &#123;&quot;jsonrpc&quot;: &quot;2.0&quot;,&quot;method&quot;: &quot;user.login&quot;,&quot;params&quot;: &#123;&quot;user&quot;: &quot;Admin&quot;,&quot;password&quot;: &quot;zabbix&quot;&#125;,&quot;id&quot;: 1,&quot;auth&quot;: None&#125;request = requests.post(url=url, headers=header, data=json.dumps(data))print request.content 执行成功后返回值如下：result字段就是Token，你可以拿着它去zabbix系统里干你想干的事情！1&#123;&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:&quot;9fc0b8d5508a4d18ac34279ddce78667&quot;,&quot;id&quot;:1&#125; 3.3、获取所有主机信息这里使用一个完整的Python脚本通过API来获取；123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/usr/bin/python#coding:utf:8import requestsimport jsonclass Zabbix: def __init__(self, url, header, username, password): self.url = url self.header = header self.username = username self.password = password def getToken(self): #获取Token并返回字符Token字符串 data = &#123;&quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;user.login&quot;, &quot;params&quot;: &#123; &quot;user&quot;: self.username, &quot;password&quot;: self.password &#125;, &quot;id&quot;: 1, &quot;auth&quot;: None &#125; token = requests.post(url=self.url, headers=self.header, data=json.dumps(data)) return json.loads(token.content)[&quot;result&quot;] def getAllHost(self): #获取所有主机信息 data = &#123;&quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;host.get&quot;, &quot;params&quot;: &#123; &quot;output&quot;: [ &quot;hostid&quot;, &quot;host&quot;, ], &quot;selectGroups&quot;: &quot;extend&quot;, &quot;selectInterfaces&quot;: [ &quot;interfaceid&quot;, &quot;ip&quot; ] &#125;, &quot;id&quot;: 2, &quot;auth&quot;: self.getToken() &#125; hosts = requests.post(url=self.url, headers=self.header, data=json.dumps(data)) return json.loads(hosts.content)[&quot;result&quot;]if __name__ == &quot;__main__&quot;: header = &#123;&quot;Content-Type&quot;: &quot;application/json-rpc&quot;&#125; url=&quot;http://172.16.194.128/zabbix/api_jsonrpc.php&quot; test = Zabbix(url=url, header=header, username=&quot;Admin&quot;, password=&quot;zabbix&quot;) print(test.getAllHost()) 通过以上的学习相信大家对Zabbix API已经有一个初步的认识，后面将会通过API去完成主机的自动添加，自动化报表等使用]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>ZabbixApi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Virtualenv使Python2与3项目并存》]]></title>
    <url>%2F2018%2F06%2F29%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AVirtualenv%E4%BD%BFPython2%E4%B8%8E3%E9%A1%B9%E7%9B%AE%E5%B9%B6%E5%AD%98%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：我在开发公司的运维cmdb平台时，使用的环境是python2.7，但有些项目我需要用到python3的环境来做，因此一个环境不能满足我需求，当然我想过很多方法，比如：虚拟机、docker？；但在我了解virtualenv过后，发现这个东西能实现我的需求，而且非常便捷。 ps : python2系统默认的环境，所以本文我们主要讲解python3如何使用Virtualenv来创建我们想要的项目环境。这是我在mac环境下配置的过程。 python 环境说明mac自带有python2.7，我在mac上又安装了python3.6，安装方法就是在python官网下载一个Mac的包，然后以Mac安装包方式安装即可，安装好后会在这里出现：12345tanshuai@tanshuaideMacBook-Pro # ls -lh /Library/Frameworks/Python.framework/Versions/total 8drwxrwxr-x 10 root admin 340B 7 29 12:17 2.7 &lt;==系统自带drwxrwxr-x 10 root admin 340B 9 26 12:34 3.6 &lt;==官网下载安装的lrwxr-xr-x 1 root wheel 3B 7 29 12:17 Current -&gt; 2.7 安装好python3后，记得做环境变量：12tanshuai@tanshuaideMacBook-Pro # vim .bash_profilePATH="/Library/Frameworks/Python.framework/Versions/3.6/bin:$&#123;PATH&#125;" 首先，安装virtualenv第一步：查看pip是哪个版本123456tanshuai@tanshuaideMacBook-Pro # which pip/usr/local/bin/piptanshuai@tanshuaideMacBook-Pro # ls -lh /usr/local/bin/piplrwxrwxr-x 1 root admin 65B 7 29 12:17 /usr/local/bin/pip -&gt; ../../../Library/Frameworks/Python.framework/Versions/2.7/bin/piptanshuai@tanshuaideMacBook-Pro # which pip3/Library/Frameworks/Python.framework/Versions/3.6/bin/pip3 从上能看到pip默认是2.7版本的，因为python3.6版本叫pip3 第二步：安装virtualenv12345tanshuai@tanshuaideMacBook-Pro # pip3 install virtualenv # 使用php3版本安装virtualenvtanshuai@tanshuaideMacBook-Pro # pip3 list|grep virtualenvvirtualenv 15.1.0tanshuai@tanshuaideMacBook-Pro # which virtualenv /Library/Frameworks/Python.framework/Versions/3.6/bin/virtualenv &lt;==注意，一定要使用python3.6环境下的virtualenv，创建出来才会是python3.6环境（python2同理） 使用virtualenv第一步：创建目录12tanshuai@tanshuaideMacBook-Pro # mkdir -p ~/Projects/Python/3.6/VirtualSourcetanshuai@tanshuaideMacBook-Pro # cd ~/Projects/Python/3.6/VirtualSource 第二步：创建一个独立的Python运行环境，命名为mxcmdb：12345678tanshuai@tanshuaideMacBook-Pro # virtualenv --no-site-packages mxcmdbUsing base prefix '/Library/Frameworks/Python.framework/Versions/3.6'New python executable in /Users/tanshuai/Projects/Python/3.6/VirtualSource/venv/bin/python3.6Also creating executable in /Users/tanshuai/Projects/Python/3.6/VirtualSource/venv/bin/pythonInstalling setuptools, pip, wheel...done.tanshuai@tanshuaideMacBook-Pro # ls -lhtotal 0drwxr-xr-x 7 tanshuai staff 238B 9 26 13:39 mxcmdb 使用命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数–no-site-packages，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。 新建的Python环境被放到当前目录下的mxcmdb目录。有了mxcmdb这个Python环境，可以用source进入该环境：12tanshuai@tanshuaideMacBook-Pro # source mxcmdb/bin/activate(mxcmdb) tanshuai@tanshuaideMacBook-Pro # 注意到命令提示符变了，有了个(mxcmdb)前缀，表示当前环境下是一个名为mxcmdb的环境。 在这个环境下可以安装你想要的环境，如：123(mxcmdb) tanshuai@tanshuaideMacBook-Pro # pip install jinja2(mxcmdb) tanshuai@tanshuaideMacBook-Pro # pip list|grep Jinja2Jinja2 2.9.6 在mxcmdb环境下，用pip安装的包都被安装到mxcmdb这个环境下，系统Python环境不受任何影响。 退出当前的mxcmdb环境，使用deactivate命令：12(mxcmdb) tanshuai@tanshuaideMacBook-Pro # deactivatetanshuai@tanshuaideMacBook-Pro # 此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。 virtualenv原理virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。小结：virtualenv为应用提供了隔离的Python运行环境，解决了不同应用间多版本的冲突问题。 参考博文：Python–Virtualenv简明教程：http://www.jianshu.com/p/08c657bd34f1virtualenv - 廖雪峰的官方网站]]></content>
      <categories>
        <category>Python开发</category>
        <category>日常技巧</category>
      </categories>
      <tags>
        <tag>Virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《监控MySQL主从同步》]]></title>
    <url>%2F2018%2F06%2F22%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E7%9B%91%E6%8E%A7MySQL%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：通过zabbix自定义key来监控mysql主从的同步状态，本文主要是学习在实战过程中的技巧和自定义key的原理，希望对你有所启发。 一、监控效果展示io进程的生命状态 其他两个进程，包括能看到延时是0毫秒。 在监控主机的最新数据里能看到监控的数据，本次设计只监控了slave同步最关注的三个数值：SQL进程，IO进程，延时等状态。 二、编写监控脚本12345678910111213141516171819202122232425262728293031323334$ cat /var/lib/zabbix/scripts/mysql/check_mysql_slave.sh#!/bin/bashUSER=rootPASS=xxxxxxio_status()&#123; IoStatus=`mysql -u$&#123;USER&#125; -p$&#123;PASS&#125; -e "show slave status\G;" |grep -i running|sed -n 1p|awk '&#123;print $NF&#125;'` if [ $IoStatus == "Yes" ];then IoStatus=1 else IoStatus=0 fi echo $IoStatus&#125;sql_status()&#123; SqlStatus=`mysql -u$&#123;USER&#125; -p$&#123;PASS&#125; -e "show slave status\G;" |grep -i running|sed -n 2p|awk '&#123;print $NF&#125;'` if [ $SqlStatus == "Yes" ];then SqlStatus=1 else SqlStatus=0 fi echo $SqlStatus&#125;lag_status()&#123; DelayStatus=`mysql -u$&#123;USER&#125; -p$&#123;PASS&#125; -e "show slave status\G;" |grep "Seconds_Behind_Master"|awk '&#123;print $NF&#125;'` echo $DelayStatus&#125;$1# 一定要给可执行权限，不然zabbix调用会失败哈。$ chmod +x /var/lib/zabbix/scripts/mysql/check_mysql_slave.sh 脚本里将Yes转换成了1或0状态，1表示正常，0表示异常。这样设计的目的是为了zabbix添加触发器的时候方便判断。 当然你可以根据需求自己改动哈。脚本也还有许多可以优化的地方，自定义性比较强。 测试脚本可用性（写好了一定要先测试）：123456$ /var/lib/zabbix/scripts/mysql/check_mysql_slave.sh io_status1$ /var/lib/zabbix/scripts/mysql/check_mysql_slave.sh sql_status1$ /var/lib/zabbix/scripts/mysql/check_mysql_slave.sh lag_status0 脚本测试没问题，接着继续… 三、配置zabbix自定义key在zabbix_agentd配置文件里写入自定义key1234$ vim /etc/zabbix/zabbix_agentd.conf#mysql slave监控UserParameter=mysql.slave[*],/var/lib/zabbix/scripts/mysql/check_mysql_slave.sh $1 技术点说明： UserParameter，是声明自定义key的标识，zabbix agent会识别。 mysql.slave，是zabbix自定义key的名称，可以为任意字符串； mysql.slave[*]，zabbix支持模板传参，星号（*）可以传入任意值，传入的值供后面使用（看到那个$1了吗，$1就是用于接受参数的） key与自定义脚本之间，使用逗号分隔，逗号后面是我们写的自定义脚本 通过zabbix agent测试自定义key是否能获取到值：12345678910111213# 配置zabbix运行通过127.0.0.1获取值$ vim /etc/zabbix/zabbix_agentd.conf Server=baidu.com,127.0.0.1# 重启生效（自定义key和修改配置必须要重启）$ /etc/init.d/zabbix_agentd restartzw-r710-133 ~ # zabbix_get -s 127.0.0.1 -p 10050 -k "mysql.slave[io_status]"1zw-r710-133 ~ # zabbix_get -s 127.0.0.1 -p 10050 -k "mysql.slave[sql_status]"1zw-r710-133 ~ # zabbix_get -s 127.0.0.1 -p 10050 -k "mysql.slave[lag_status]"0 zabbix_get工具的用途是模拟zabbix server来获取数据，如果zabbix_get能获取到数据，那说明你的agent端的配置没问题了。剩下的就是去zabbix平台上配置监控项即可。 自定义key后，一定要记得使用zabbix_get测试，这一步测试尤为重要，很多人都忽略这一点。 四、创建监控模板在Zabbix系列之《监控TCP连接状态》一文中，我有详细说明如果创建zabbix监控模板，请参考！ 创建好模板后，一次创建项目： 创建好的三个项目： 触发器： 五、主机添加模板这一步就不用我说了吧。 略。 老规矩，你可以下载我的模板：mysql_slave.xml 看似简单的东西，你学会了，会变通吗？比如在工作中老大各种奇葩要求，需要监控交换机，动态流量等。如果学会了自定义key，你甚至可以自己写脚本，不用zabbix。来达到监控的目的都不成问题。 运维路漫长，且行且珍惜。欢迎加入QQ群：32330026，我们一起成长。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker系列之《Dockerfile构建SSH服务镜像》]]></title>
    <url>%2F2018%2F06%2F20%2FDocker%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADockerfile%E6%9E%84%E5%BB%BASSH%E6%9C%8D%E5%8A%A1%E9%95%9C%E5%83%8F%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：这又是一篇对Dockerfile实例应用的文章，如果你将本站的几篇Dockerfile文章学完，相信你对docker的理解又提升了不少。 基于commit命令的方式先运行一个centos或ubuntu镜像，然后安装ssh，然后commit镜像，然后运行即可；大概步骤：123$ docker run -it centos $ yum install openssh-server openssh -y$ docker commit 容器ID ssh:commit 上面就已经将容器配置好啦，接下来是使用容器1$ docker run -d -p 52113:22 ssh:commit /root/run.sh 基于Dockerfile的方式（推荐）1、准备工作创建一个存放Dockerfile相关文件的工作目录：1$ mkdir /root/Dockerfile/ 在该目录下，需要创建3个文件：Dockerfile、run.sh和authorized_keys，Dockerfile用于构建镜像，run.sh是启动SSH服务的脚本文件，authorized_keys则是包含需要远程登录的用户公钥。run.sh脚本文件的内容如下：12#!/bin/bash/usr/sbin/sshd -D authorized_keys文件内容如下：在宿主机上生成一对ssh密钥12$ ssh-keygen -t rsa #一路回车即可$ cat ~/.ssh/id_rsa.pub &gt;authorized_keys 2、编写Dockerfile1234567891011121314151617181920212223242526272829#使用的基础镜像FROM centos#添加作者信息MAINTAINER tanyanhong&lt;tanyanhong@moxiu.net&gt;#安装阿里epel源RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm#安装SSH服务RUN yum install openssh openssh-server -y RUN mkdir -p /var/run/sshdRUN mkdir -p /root/.ssh#取消pam登录限制RUN sed -ri 's/session required pam_loginuid.so/#session required pam_loginuid.so/g' /etc/pam.d/sshdRUN sed -i 's/HostKey \/etc\/ssh\/ssh_host_rsa_key/#HostKey \/etc\/ssh\/ssh_host_rsa_key/g' /etc/ssh/sshd_configRUN sed -i 's/HostKey \/etc\/ssh\/ssh_host_ecdsa_key/#HostKey \/etc\/ssh\/ssh_host_ecdsa_key/g' /etc/ssh/sshd_configRUN sed -i 's/HostKey \/etc\/ssh\/ssh_host_ed25519_key/#HostKey \/etc\/ssh\/ssh_host_ed25519_key/g' /etc/ssh/sshd_config#添加认证文件和启动脚本ADD authorized_keys /root/.ssh/authorized_keysADD run.sh /root/run.shRUN chmod u+x /root/run.sh#暴露端口EXPOSE 22#设置默认的启动命令CMD ["/root/run.sh"] 3、创建镜像运行docker build目录，生成目标镜像：12345678910111213141516171819202122232425262728293031323334353637383940$ docker build -t docker.io/nicksors/ssh:latest /root/Dockerfile/# -t 指定tag，我打了自己的仓库标签Sending build context to Docker daemon 4.608 kBStep 1 : FROM centos ---&gt; 0584b3d2cf6dStep 2 : MAINTAINER tanyanhong&lt;tanyanhong@moxiu.net&gt; ---&gt; Running in 0364d111928e ---&gt; c4adaaf68ddbRemoving intermediate container 0364d111928eStep 3 : RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm ---&gt; Running in d351669fb7b4warning: /var/tmp/rpm-tmp.cIXtaq: Header V3 RSA/SHA256 Signature, key ID 352c64e5: NOKEYRetrieving http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpmPreparing... ########################################Updating / installing...epel-release-7-8 ######################################## ---&gt; fdbef0ca575eRemoving intermediate container d351669fb7b4Step 4 : RUN yum install openssh openssh-server -y···略···Removing intermediate container d6b4d4ece952Step 11 : ADD authorized_keys /root/.ssh/authorized_keys ---&gt; a9a0f7224e8dRemoving intermediate container 6133f320df7aStep 12 : ADD run.sh /root/run.sh ---&gt; 3c9ce2d1a03cRemoving intermediate container 2124700e28ebStep 13 : RUN chmod u+x /root/run.sh ---&gt; Running in f787f85cc2b4 ---&gt; 5e81e48e260fRemoving intermediate container f787f85cc2b4Step 14 : EXPOSE 22 ---&gt; Running in d78bd239eeaf ---&gt; 5598dc68af4dRemoving intermediate container d78bd239eeafStep 15 : CMD /root/run.sh ---&gt; Running in e1bb50e4f1c7 ---&gt; 7ea2a68e47dbRemoving intermediate container e1bb50e4f1c7Successfully built 7ea2a68e47db 看到最后的Successfully built，就表明镜像成功生成了，其中7ea2a68e47db是镜像ID。使用docker images命令查看本地镜像，可以看到我们刚刚生成的镜像，相关代码如下：1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/nicksors/ssh latest 7ea2a68e47db 25 minutes ago 354.8 MBdocker.io/centos latest 0584b3d2cf6d 2 weeks ago 196.5 MB 4、启动并连接容器1234$ docker run -d -p 52113:22 --name=ssh docker.io/nicksors/ssh /root/run.sh# 说明-p 指定宿主机与容器的端口映射镜像后面的/root/run.sh是启动镜像是需要执行的命令 然后再打开一个属主机终端，并ssh连接到容器里：1$ ssh 10.0.10.181 -p 52113]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker系列之《Dockerfile构建MySQL镜像》]]></title>
    <url>%2F2018%2F06%2F19%2FDocker%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADockerfile%E6%9E%84%E5%BB%BAMySQL%E9%95%9C%E5%83%8F%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言: 学会使用Dockerfile制作自己需要的镜像是必备的技能，本文就通过实战的方式展示如何通过Dockerfile来制作MySQL镜像。 1、编写镜像Dockerfile（1）创建Dockerfile文件，并在文件开始位置添加使用#注释的描述信息：123# 名称：容器化的MySQL# 用途：用作后端数据库持久化服务# 创建时间：2018.06.11 （2）定义基础镜像1FROM centos （3）声明维护者信息1MAINTAINER tanshuai 1432753451@qq.com （4）定义工作目录1WORKDIR /root/ （5）安装相关软件123RUN yum -y install wgetRUN wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmRUN rpm -ivh /root/mysql-community-release-el7-5.noarch.rpm （6）使用yum安装MySQL服务123RUN yum install mysql-server -y# 安装好MySQL后，默认并没有建立数据库，需要使用musql_install_db创建一个数据库：RUN mysql_install_db --user=mysql （7）通过环境变量指定MySQL使用的用户名和密码，MySQL拥有一个默认的用户root，但root用户默认只能在本地访问，所以这里定一了一个额外的用户test：12ENV MYSQL_USER testENV MYSQL_PASS =mypassword （8）让容器支持中文，centos容器默认是不支持中文的1ENV LC_ALL en_US.UTF-8 （9）导出3306端口（这里是MYSQL使用的端口），以后外部使用可以访问它1EXPOSE 3306 （10）定义默认的启动命令，这里使用一个脚本来启动MySQL123ADD run.sh /root/run.shRUN chmod u+x /root/run.shCMD /root/run.sh 2、完整的Dockerfile：123456789101112131415161718# 名称：容器化的MySQL# 用途：用作后端数据库持久化服务# 创建时间：2018.06.11FROM centosMAINTAINER tanshuai 1432753451@qq.comWORKDIR /root/RUN yum -y install wgetRUN wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmRUN rpm -ivh /root/mysql-community-release-el7-5.noarch.rpmRUN yum install mysql-server -yRUN mysql_install_db --user=mysqlENV MYSQL_USER testENV MYSQL_PASS =mypasswordENV LC_ALL en_US.UTF-8EXPOSE 3306ADD run.sh /root/run.shRUN chmod u+x /root/run.shCMD /root/run.sh （1）run.sh定义了容器的默认启动行为，这里只是拉起MySQL，其内容为：12#!/bin/bashmysqld_safe 3、构建和上传镜像Dockerfile和必要的文件都准备好了，接下来就可以使用docker build 命令来后街镜像了：1234567891011121314151617$ docker build -t docker.io/nicksors/mysql:latest /root/build_images/build_mysqlSending build context to Docker daemon 3.584 kBStep 1 : FROM centos ---&gt; 0584b3d2cf6dStep 2 : MAINTAINER tanshuai 1432753451@qq.com ---&gt; Running in ebbb7a8b003a ---&gt; 19316ea8b554Removing intermediate container ebbb7a8b003aStep 3 : WORKDIR /root/ ---&gt; Running in 83e6594d48db ---&gt; dd4a9e06196c~~~中间太长自动略过~~~Step 15 : CMD /root/run.sh ---&gt; Running in 98e4b96d656e ---&gt; b340df2bc01fRemoving intermediate container 98e4b96d656eSuccessfully built b340df2bc01f 4、启动MySQL容器12345$ docker run -d -p 3306:3306 --name=mysql docker.io/nicksors/mysql /root/run.shabc07ad4db8eef4d38d7b7abda99d6609f9108704aa8f45e0499459610ccef2f$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESabc07ad4db8e docker.io/nicksors/mysql "/root/run.sh" 52 seconds ago Up 51 seconds 0.0.0.0:3306-&gt;3306/tcp mysql 构建好镜像之后，通过docker push命令将镜像提交到Docker Hub： 123456789101112$ docker push docker.io/nicksors/mysqla2db66ccbd56: Pushed 157fa65fbf3b: Pushed 8ed28c549a06: Pushed 4ded80e382c6: Pushing [=====&gt; ] 49.42 MB/423.7 MBdee17d2fbb38: Pushed 0074ce8e9d38: Pushed d2193e3db81f: Pushing [=====================&gt; ] 46.14 MB/109.5 MB97ca462ad9ee: Mounted from library/centos 97ca462ad9ee: Preparing 97ca462ad9ee: Mounted from library/centos latest: digest: sha256:d0f8c127884437a8aa8798340be021b8dd56929cfab51bf4822afd435f8ce0a6 size: 1975 现在MySQL镜像就创建好了，并且可以在任何可以上网的机器上从Docker Hub拉取。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker系列之《Dockerfile介绍与实战应用》]]></title>
    <url>%2F2018%2F06%2F19%2FDocker%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADockerfile%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8%E3%80%8B.html</url>
    <content type="text"><![CDATA[Dockerfile是docker构建镜像的基础，也是docker区别于其他容器的重要特征，正是有了Dockerfile，docker的自动化和可移植性才成为可能。 不论是开发还是运维，学会编写Dockerfile几乎是必备的，这有助于你理解整个容器的运行。 FROM，从一个基础镜像构建新的镜像1FROM ubuntu MAINTAINER，维护者信息1MAINTAINER tanyanhong&lt;tanyanhong@moxiu.net&gt; ENV，设置环境变量1ENV TEST 1 RUN，非交互式运行shell命令12RUN apt-get -y updateRUN apt-get -y install nginx ADD，将外部文件拷贝到镜像里src可以为url1ADD http://www.moxiu.com/test.tar /data/test.tar WORKDIR /path/to/workdir，设置工作目录1WORKDIR /var/www USER，设置用户ID1USER nginx VOLUME，设置volume1VOLUME ['/data'] EXPOSE，暴露哪些端口1EXPOSE 80 442 ENTRYPOINT [‘executable’, ‘param1’,’param2’]执行命令1ENTRYPOINT ["/usr/sbin/nginx"] CMD [“param1”,”param2”]1CMD ["start"] docker创建、启动container时执行的命令，如果设置了ENTRYPOINT，则CMD将作为参数. 两个实战案例Dockerfile构建MongoDB镜像1、完整的Dockerfile 12345678910# 名称：容器化的MongoDB# 用途：用作后端数据持久化服务# 创建时间：2018.06.11FROM centosMAINTAINER tanshuai 1432753451@qq.comRUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpmRUN yum -y install mongodb mongodb-serverRUN mkdir -p /data/dbEXPOSE 27017ENTRYPOINT ["/usr/bin/mongod"] 2、构建并上传镜像 1234567891011121314151617181920212223242526$ docker build -t docker.io/nicksors/mongodb:latest /root/build_images/build_mongodb/Sending build context to Docker daemon 2.048 kBStep 1 : FROM centos ---&gt; 0584b3d2cf6dStep 2 : MAINTAINER tanshuai 1432753451@qq.com ---&gt; Using cache ---&gt; 19316ea8b554Step 3 : RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm ---&gt; Using cache ---&gt; 502f4a9df04dStep 4 : RUN yum -y install mongodb mongodb-server ---&gt; Using cache ---&gt; 3b3e3fa7b714Step 5 : RUN mkdir -p /data/db ---&gt; Running in 8a13c217fa0d ---&gt; b143715ca281Removing intermediate container 8a13c217fa0dStep 6 : EXPOSE 27017 ---&gt; Running in 38082f2ca4e1 ---&gt; f435f91790f0Removing intermediate container 38082f2ca4e1Step 7 : ENTRYPOINT /usr/bin/mongod ---&gt; Running in 5f208c460f9c ---&gt; 5550b91f642cRemoving intermediate container 5f208c460f9cSuccessfully built 5550b91f642c 3、构建好镜像之后，通过docker push命令将镜像提交到Docker Hub： 1234567$ docker push docker.io/nicksors/mongodbThe push refers to a repository [docker.io/nicksors/mongodb]9f89394eb43a: Pushed 8d78ca8ae7c5: Pushed 6a5a8df2ecbc: Pushed 97ca462ad9ee: Mounted from library/centos latest: digest: sha256:cdb622243fa2e292b857c70855b87f347e76d968d1ce04f16ee8f8359fa2c687 size: 1138 4、启动并连接MongoDB 12$ docker run -d -p 27017:27017 --name=mongodb docker.io/nicksors/mongodb$ mongo 10.0.10.181:27017 Dockerfile构建Nginx镜像1、编写Dockerfile文件 12345678910111213[root@docker nginx]# mkdir /opt/dockerfille/nginx -p[root@docker nginx]# cat Dockerfile # 名称：容器化的Nginx# 用途：Web服务# 创建时间：2018.06.11FROM centosMAINTAINER tanshuai 1432753451@qq.comRUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpmRUN yum install -y nginxADD index.html /usr/share/nginx/html/index.htmlRUN echo "daemon off;" &gt;&gt;/etc/nginx/nginx.confEXPOSE 80CMD ["nginx"] 2、使用docker build 执行dockerfile构建镜像 12345678910111213141516171819202122232425262728293031[root@docker nginx]# docker build -t docker.io/nicksors/nginx:latest /opt/dockerfille/nginx/Sending build context to Docker daemon 5.12 kBStep 0 : FROM centos ---&gt; 60e65a8e4030Step 1 : MAINTAINER tanshuai 1432753451@qq.com ---&gt; Running in 0fd1cef13b99 ---&gt; 43b217395619Removing intermediate container 0fd1cef13b99Step 2 : RUN rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm ---&gt; Running in 2679fe3aa727····Removing intermediate container 87666c3126dfStep 4 : ADD index.html /usr/share/nginx/html/index.html ---&gt; 64d3f3fc81a7Removing intermediate container 38ac7440e293Step 5 : RUN echo "daemon off;" &gt;&gt;/etc/nginx/nginx.conf ---&gt; Running in 96ed8d13dc48 ---&gt; 6455c0d9c3f4Removing intermediate container 96ed8d13dc48Step 6 : EXPOSE 80 ---&gt; Running in 5fefac8445b8 ---&gt; 9992a1075d2fRemoving intermediate container 5fefac8445b8Step 7 : CMD nginx ---&gt; Running in 12b47409d774 ---&gt; 6dc5ae1d99dfRemoving intermediate container 12b47409d774Successfully built 6dc5ae1d99df 3、查看使用dockerfile构建的镜像 123[root@docker nginx]# docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEdocker.io/nicksors/nginx latest d57cda7dac3c 11 minutes ago 347.7 MB 4、运行构建的docker容器 12[root@docker nginx]# docker run -d --name nginx -p 80:80 docker.io/nicksors/nginx b988f45e5847e1942219cf03783d19615f39c57d92a956c2acfc2a6c4cf14b89 5、访问]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Dockerfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-视图的演变（二）》]]></title>
    <url>%2F2018%2F06%2F16%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E8%A7%86%E5%9B%BE%E7%9A%84%E6%BC%94%E5%8F%98%EF%BC%88%E4%BA%8C%EF%BC%89%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：本文是“Python系列之《Django-DRF-序列化模型（一）》”的姊妹篇，其中示例所用的内容也继承了它，这篇文章是比较高阶的内容，适合有一定基础的同学查阅。 视图的演变版本一（底层方法）这种方法只需要看得懂即可，因为太底层了，后面写代码不会用这种方式 1、撰写视图： 写一个视图，支持：GET all、GET one、POST、PUT、DELETE这五个操作。 $ vim idcs/views.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from django.http import HttpResponsefrom rest_framework.renderers import JSONRendererfrom rest_framework.parsers import JSONParserfrom .models import Idcfrom .serializers import IdcSerializerclass JSONResponse(HttpResponse): def __init__(self, data, **kwargs): kwargs.setdefault('content_type', 'application/json') content = JSONRenderer().render(data) super(JSONResponse, self).__init__(content=content, **kwargs)def idc_list(request, *args, **kwargs): if request.method == 'GET': quertset = Idc.objects.all() serializer = IdcSerializer(quertset,many=True) return JSONResponse(serializer.data) # content = JSONRenderer().render(serializer.data) # return HttpResponse(content=content,content_type="application/json") elif request.method == 'POST': content = JSONParser().parse(request) serializer = IdcSerializer(data=content) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data) return HttpResponse('')def idc_detail(request, pk, *args, **kwargs): try: idc = Idc.objects.get(pk=pk) except Idc.DoesNotExist: return HttpResponse(status=404) if request.method == 'GET': serializer = IdcSerializer(idc) return JSONResponse(serializer.data) elif request.method == 'PUT': content = JSONParser().parse(request) serializer = IdcSerializer(idc, data=content) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data) return JSONResponse(serializer.errors,status=400) elif request.method == 'DELETE': idc.delete() return HttpResponse(status=204)# 代码说明：1. JSONResponse改写了Django里默认的，目的是处理POST提交的数据，将数据转换成JSON后方便处理。2. idc_list可以通过HttpResponse和改写的JSONResponse两种方法来返回给前端。3. idc_detail函数设计接受一个pk，这个pk是一个id，通过查询到ID后，进行GET、PUT、DELETE操作。 2、路由规则：$ vim idc/urls.py12345678from django.conf.urls import urlfrom idcs.views import idc_list,idc_detail########################### 版本一 ##############################urlpatterns = [ url('^idcs/$', idc_list, name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', idc_detail, name='idc_detail')] 3、GET请求（获取所有数据）： 本文使用Postman工具来进行Http请求的提交操作，每一次提交请求后，该工具都会有Status字段显示请求的状态码，比较方便。 4、POST请求（创建一个数据）： 5、GET请求（使用pk指定获取一个资源信息）： 6、PUT更新（更新一个资源的信息）： 7、DELETE删除（删除一个资源）： 版本二（基于函数视图的@api_view装饰器）1、撰写视图：$ vim idcs/views.py1234567891011121314151617181920212223242526272829303132333435363738394041424344########################### 版本二 ##############################from rest_framework.decorators import api_view #导入api_viewfrom rest_framework import status # 导入status，返回状态使用这个模块from rest_framework.response import Response # DRF封装好的方法Response@api_view(["GET","POST"])def idc_list_v2(request, *args, **kwargs): if request.method == 'GET': queryset = Idc.objects.all() serializer = IdcSerializer(queryset, many=True) return Response(serializer.data) elif request.method == 'POST': serializer = IdcSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.data, status=status.HTTP_404_NOT_FOUND)@api_view(["GET", "PUT", "DELETE"])def idc_detail_v2(request, pk, *args, **kwargs): try: idc = Idc.objects.get(pk=pk) except Idc.DoesNotExist: return HttpResponse(status=status.HTTP_404_NOT_FOUND) if request.method == 'GET': serializer = IdcSerializer(idc) return Response(serializer.data) elif request.method == 'PUT': serializer = IdcSerializer(idc, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_404_NOT_FOUND) elif request.method == 'DELETE': idc.delete() return HttpResponse(status=status.HTTP_204_NO_CONTENT)# 代码说明：1. @api_view()，传入一个列表作为参数，这个列表里写入具体的方法，如["GET", "PUT", "DELETE"]。api_view之会允许你写入的Http方法进行交互。没有写的就会禁止交互。2. status.HTTP_404_NOT_FOUND，status模块可以返回你想要给前端的状态。3. Response，这是drf给我们封装好的方法，它会将模板和数据一并返回给前端，所以你在前端能看见drf的页面了。 2、路由规则：$ vim idc/urls.py12345678910from django.conf.urls import urlfrom idcs.views import idc_list,idc_detailfrom . import views ########################### 版本二 ##############################urlpatterns = [ url('^idcs/$', views.idc_list_v2, name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', views.idc_detail_v2)] 3、GET请求（请求所有列表）： 4、POST请求（提交数据进行创建操作）：注意：需要指定POST请求的数据类型为JSON！！！ 5、GET请求（通过id获取数据）：直接通过浏览器访问的话，就能看到Response渲染给前端的页面。 6、PUT请求（更新数据）： 7、DELETE操作 第二版本完事儿~ Api Root截至目前，你在访问跟站点的时候，应该是会出错的，因为我们没有定义访问跟站点需要显示哪些资源。 因此，我们这里介绍下Api Root，它是干什么的呢？先简单理解下：在访问跟站点的时候，为我们列出当前有哪些资源。如果你还是不理解，那且看下面的操作。 1、撰写视图：$ vim idc/views.py123456789101112from rest_framework.reverse import reverse@api_view(["GET"])def api_root(request, format=None, *args, **kwargs): return Response(&#123; "idcs": reverse("idc_list", request=request, format=format) &#125;)代码说明：1. reverse，drf封装好的方法，跟Django里的reverse功能一样；第一个参数："idc_list"是路由里的Namespace名称，使用它的好处我想不用再说了。2. 通过Response返回一个字典类型。 2、路由规则：$ vim idc/urls.py123456789101112########################### 版本二 ##############################from . import viewsfrom rest_framework.urlpatterns import format_suffix_patternsurlpatterns = [ url("^$",views.api_root), url('^idcs/$', views.idc_list_v2, name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', views.idc_detail_v2, name='idc_detail')]urlpatterns = format_suffix_patterns(urlpatterns)代码说明：1. 所有的urlpatterns通过drf里的方法format_suffix_patterns实例化后，再付给urlpatterns，达到能渲染跟站点路由的效果。 3、首页访问显示： 具体的效果如下 那有个问题，如果这个平台的app非常多，项目非常大，导致url也会非常多，这时候这个列表该如何维护呢？ 只能一条条增加，且后期维护成本较大，这算是一个缺点。 版本三（基于类视图APIView类）1、撰写视图：$ vim idcs/views.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546########################### 版本三 ##############################from rest_framework.views import APIViewfrom django.http import Http404class IdcList(APIView): def get(self,request, format=None): queryset = Idc.objects.all() serializer = IdcSerializer(queryset, many=True) return Response(serializer.data) def post(self,request, format=None): serializer = IdcSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.data, status=status.HTTP_404_NOT_FOUND)class IdcDetail(APIView): def get_object(self, pk): try: return Idc.objects.get(pk=pk) except Idc.DoesNotExist: raise Http404 def get(self, request, pk, format=None): idc = self.get_object(pk) serializer = IdcSerializer(idc) return Response(serializer.data) def put(self,request, pk, format=None): idc = self.get_object(pk) serializer = IdcSerializer(idc, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_404_NOT_FOUND) def delete(self, request, pk, format=None): idc = self.get_object(pk) idc.delete() return HttpResponse(status=status.HTTP_204_NO_CONTENT)代码说明：1. 使用类视图，定义两个类并继承APIView类视图；2. 在类视图里编写：增删改查方法，通过HttpResponse返回状态。 通过源码能看出，APIView是继承的Django View视图的。 2、路由规则：$ vim idc/urls.py123456789########################### 版本三 ##############################from . import viewsfrom rest_framework.urlpatterns import format_suffix_patternsurlpatterns = [ url("^$",views.api_root), url('^idcs/$', views.IdcList.as_view(), name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', views.IdcDetail.as_view(), name='idc_detail') #调用类视图]urlpatterns = format_suffix_patterns(urlpatterns) 从这个版本往后，我就不一个个截图了，接口操作上面已经说得很详细。 版本四（使用混合 mixins）这一版本的功能更为高级，使用mixins来实现，往下看！ 1、撰写视图：$ vim idcs/views.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647########################### 版本四（混合:mixins） ##############################from rest_framework import mixins, generics # 导入相应模块class IdcList_V4(generics.GenericAPIView, mixins.ListModelMixin, mixins.CreateModelMixin): # 继承generics和mixins里的方法，称之为“混合” queryset = Idc.objects.all() # 继承了generics，通过通过成员属性的形式传入参数 serializer_class = IdcSerializer # 继承了generics，通过通过成员属性的形式传入参数 def get(self, request, *args, **kwargs): # 第四版的 get和post方法还得自己写 return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs)class IdcDetail_V4(generics.GenericAPIView, mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin): # detail类，继承mixins的检索、更新、删除等类方法 queryset = Idc.objects.all() serializer_class = IdcSerializer def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def put(self, request, *args, **kwargs): return self.update(request, *args, **kwargs) def delete(self, request, *args, **kwargs): # 写好调用的方法 return self.destroy(request, *args, **kwargs)代码说明：1. 为什么要继承mixins.RetrieveModelMixin,mixins.UpdateModelMixin,mixins.DestroyModelMixin等这一堆方法呢？答：比如我们自定义写了delete方法，而这个方法会自动、只能的去调用mixins.DestroyModelMixin这个类里面的动作，这里给你看看DestroyModelMixin的源码就明白了。class DestroyModelMixin(object): """ Destroy a model instance. """ def destroy(self, request, *args, **kwargs): instance = self.get_object() self.perform_destroy(instance) return Response(status=status.HTTP_204_NO_CONTENT) def perform_destroy(self, instance): instance.delete() &lt;==在这里，DestroyModelMixin指定执行了一个delete动作，这也就达到了，前端调用我们自定义写的delete方法，就相当于在调用这里。 在版本三中，queryset和serializer属性都是通过自己声明去使用的；版本四继承使用混合继承了generics，通过查看generics源码发现如下：12345class GenericAPIView(views.APIView):··· queryset = None serializer_class = None··· 由上可看出，generics将queryset和serializer通过成员属性的形式抽离出来了，那我们只需要将这两个成员属性声明即可。 2、路由规则：$ vim idc/urls.py123456789########################### 版本四 ##############################from . import viewsfrom rest_framework.urlpatterns import format_suffix_patternsurlpatterns = [ url("^$",views.api_root), url('^idcs/$', views.IdcList_V4.as_view(), name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', views.IdcDetail_V4.as_view(), name='idc_detail')]urlpatterns = format_suffix_patterns(urlpatterns) 版本五（使用混合高级版）1、撰写视图：$ vim idcs/views.py123456789########################### 版本五(使用混合高级版) ##############################class IdcList_V5(generics.ListCreateAPIView): queryset = Idc.objects.all() serializer_class = IdcSerializerclass IdcDetail_V5(generics.RetrieveUpdateDestroyAPIView): queryset = Idc.objects.all() serializer_class = IdcSerializer 这几行代码搞定上面所有功能，只需要传入queryset和serializer即可，这什么原理呢？且看我来解释： 1、第四版本中，我们继承了generics.GenericAPIView类视图，自己写了两个get和post方法对吧，那generics的另一个方法把这两个事情也干了，我们只需要继承即可。2、对的，这个方法就是generics.ListCreateAPIView，我们且看看它的源码，你就明白了1234567891011class ListCreateAPIView(mixins.ListModelMixin, mixins.CreateModelMixin, GenericAPIView): """ Concrete view for listing a queryset or creating a model instance. """ def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) 在上面源码里我们看到，ListCreateAPIView直接继承了mixins.ListModelMixin,mixins.CreateModelMixin和GenericAPIView视图，那我们直接用它就好了呀，哈哈，这就是第五版本的进化点。 3、继承generics.RetrieveUpdateDestroyAPIView的方法类似，因为它也封装好了get、put、patch（更新）和delete操作。 真是简单便捷，卧凑，下面还有更简单的，咱们一步步往下看。 2、路由规则：$ vim idc/urls.py1234567########################### 版本五 ##############################urlpatterns = [ url("^$",views.api_root), url('^idcs/$', views.IdcList_V5.as_view(), name='idc_list'), url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', views.IdcDetail_V5.as_view(), name='idc_detail')]urlpatterns = format_suffix_patterns(urlpatterns) 版本六（视图集 ViewSet）1、撰写视图：$ vim idcs/views.py1234567891011########################### 版本六（视图集 ViewSet） ##############################from rest_framework import viewsetsclass IdcListViewset(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.DestroyModelMixin, mixins.UpdateModelMixin,): queryset = Idc.objects.all() serializer_class = IdcSerializer 第六版的视图进化点很明显就能看出来吧，在views里就写了一个内视图，之前的所有版本都是写一个List和一个Detail视图的。 IdcListViewset继承了viewsets.GenericViewSet，其他的方法都是mixins的，跟第五版本一样，这些方法又封装好了相对应的如更新、删除、查询操作。真是便捷呀！ 值得一说的是viewsets.GenericViewSet继承了ViewSetMixin方法，从ViewSetMixin的源码里能看到可以改写as_view的信息，来达到定制路由的效果ViewSetMixin的源码部分如下：123456789class ViewSetMixin(object): @classonlymethod def as_view(cls, actions=None, **initkwargs): ······ if not actions: raise TypeError("The `actions` argument must be provided when " "calling `.as_view()` on a ViewSet. For example " "`.as_view(&#123;'get': 'list'&#125;)`") ······ 能看到在actions字段可以设置{‘get’: ‘list’}这样的路由规则，那且看路由规则的写法 2、路由规则：$ vim idc/urls.py1234567891011121314151617181920########################### 版本六 ############################### 定义了idc_list处理get和post两个路由请求idc_list = views.IdcListViewset.as_view(&#123; "get": "list", # 这里的“list”对应IdcListViewset里继承的mixins.ListModelMixin，而且post和下面的put等方法，也是需要一一对应 "post": "create"&#125;)# 定义了idc_detail处理get和put和delete请求idc_detail = views.IdcListViewset.as_view(&#123; "get": "retrieve", "put": "update", "delete": "destroy"&#125;)urlpatterns = [ url("^$",views.api_root), url('^idcs/$', idc_list, name='idc_list'), #&lt;==这里使用上面的定义即可 url('^idcs/(?P&lt;pk&gt;[0-9]+)/$', idc_detail, name='idc_detail')]urlpatterns = format_suffix_patterns(urlpatterns) 版本七 （终极大法：写项目选用此法）1、撰写视图：$ vim idcs/views.py123456########################### 版本七 ##############################from rest_framework import viewsetsclass IdcViewset_V7(viewsets.ModelViewSet): queryset = Idc.objects.all() serializer_class = IdcSerializer 根据版本五和版本六的继承套路，版本七直接继承了一个巨无霸（ModelViewSet），这个巨无霸将所有的功能都封装到一块。相当于把我们从第一版到第六版写的所有事情都干了，按照老规矩，我们来看看它的源码：1234567891011class ModelViewSet(mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, mixins.ListModelMixin, GenericViewSet): &quot;&quot;&quot; A viewset that provides default `create()`, `retrieve()`, `update()`, `partial_update()`, `destroy()` and `list()` actions. &quot;&quot;&quot; pass 从源码中能看出，ModelViewSet所继承的视图类，我们在前面几个版本中都重点继承并介绍过。所以，你知道它的原理了吧！ 2、路由规则：$ vim idc/urls.py12345678########################### 版本七 ##############################from rest_framework.routers import DefaultRouterroute = DefaultRouter()route.register("idcs", views.IdcViewset_V7)urlpatterns = [ url(r'^', include(route.urls))] 最终版的规则使用了drf的DefaultRouter函数，通过实例化DefaultRouter得到route对象，使用route.register()你的app路由，有多个注册多个即可，使用也很简单。 本文由“Rock”布道，本站学习和整理发布。 如果你在阅读或使用文章中遇到问题，欢迎加入QQ群：32330026，我们是一群爱学习的人，期待与你一起学习进步。]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python系列之《Django-DRF-序列化模型（一）》]]></title>
    <url>%2F2018%2F06%2F15%2FPython%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ADjango-DRF-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%80%EF%BC%89%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：本文带领大家了解django-rest-framework “序列化模型”的一些使用及知识点，通过实战演示进一步让大家能简单看懂，并且上手操作，了解DRF的强大。 一、App创建与管理在讲解本文主题“序列化模型”之前，我们需要创建一个app来进行实际的演示，因此你想更深入的了解学习本文知识点的话，建议你跟着一起操作。 首先创建一个app：1$ python manage.py startapp idcs 随着项目越来越庞大，项目里的app越来越多，因此需要将所有app进行管理起来，管理方法如下 创建管理app的目录：12$ mkdir apps$ mv idcs apps 需要知道的是，创建的app也是一个python的模块，我们后面也可以将一些非app的模块放到这里面，目的是便于管理。 因为目录结构有了变化，Django找不到创建的app了，所以需要我们配置一下，告诉Django在哪里能找到我们创建的app。方法如下Django加载apps目录：123456789101112$ vim ops/settings.pyimport osimport sys # 先导入sys模块# Build paths inside the project like this: os.path.join(BASE_DIR, ...)BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, &apos;apps&apos;)) # 增加这一行INSTALLED_APPS = [······ &apos;idcs.apps.IdcsConfig&apos;, #在apps列表里添加刚刚创建的idcs app] 需要在Django的配置文件里加载它，这里使用sys模块将apps所在的路径插入到sys.path中，这样Django在寻找app的时候就知道去哪找了。 idcs创建urls.py： 上面通过startapp创建好app后，你会发现app里没有urls.py。嘿嘿嘿，如果没有这个文件，你启动Django项目试试？它保证不会任性不出错（讽刺）。。。 因此，我们需要自己来创建这个文件，并写下urlpatterns字段。123$ vim apps/idcs/urls.pyurlpatterns = [] 配置路由：app的导入和url问题都搞定了，接下来需要在全局urls.py文件里配置idcs这个app的路由关系12345$ vim ops/urls.pyurlpatterns = [ url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;^idcs/&apos;, include(&quot;idcs.urls&quot;)), # 默认无法识别，Pycharm工具配置自动识别方法：右击“apps”目录 --&gt;Mark --&gt;Source Root ] 安装DRF：配置好app后，我们顺便把DRF也安装和配置一下，非常简单快速。 1、安装1$ pip install django-rest-framework 2、配置Django加载DRFDjango使用drf比较简单，直接在apps列表里写入如下模块即可完成，后面就等着使用了。123456$ vim ops/settings.pyINSTALLED_APPS = [······ &apos;rest_framework&apos;,] 好，完事。 二、序列化模型2.1、同步Idc模型在models模型文件里创建字段如下，并使用下面给出的命令快速同步到数据库中123456789101112131415161718$ vim idcs/models.pyclass Idc(models.Model): name = models.CharField(&quot;机房名称&quot;,max_length=32) address = models.CharField(&quot;机房地址&quot;,max_length=256) phone = models.CharField(&quot;联系人&quot;,max_length=15) email = models.EmailField(&quot;邮件地址&quot;,default=&quot;null&quot;) letter = models.CharField(&quot;IDC简称&quot;,max_length=5) def __str__(self): return self.name class Meta: db_table = &apos;resource_idc&apos;$ python manage.py showmigrations$ python manage.py migrate$ python manage.py makemigrations idcs$ python manage.py migrate idcs 2.2、定义序列化类在idc app目录下新建一个文件serializers.py，因为是做序列化，而序列化是针对模型的，所以需要跟模型文件models.py在同一个目录下123456789101112131415$ vim idcs/serializers.pyfrom rest_framework import serializers # 导入这个模块，来使用序列化# 定义并编写序列化类class IdcSerializer(serializers.Serializer): &quot;&quot;&quot; Idc, 序列化类 &quot;&quot;&quot; id = serializers.IntegerField(read_only=True) # 处理只读 name = serializers.CharField(required=True, max_length=32) # 字段必须传, 最大限制32个字符 address = serializers.CharField(required=True, max_length=256) phone = serializers.CharField(required=True, max_length=15) email = serializers.CharField(required=True) letter = serializers.CharField(required=True, max_length=5) 2.3、使用序列化通过python mamage.py shell往IDC模型里添加两条记录 12345678910111213141516In [1]: from idcs.models import IdcIn [2]: idc = Idc()In [3]: idc.name = &apos;亦庄机房&apos;In [4]: idc.address = &apos;北京亦庄机房&apos;In [5]: idc.phone = &apos;12312341234&apos;In [6]: idc.email = &apos;nick@qq.com&apos;In [7]: idc.letter = &apos;yz&apos;In [8]: idc.save()In [9]:In [9]: idc.id = NoneIn [10]: idc.name = &apos;兆维机房&apos;In [11]: idc.address = &apos;兆维工业园&apos;In [12]: idc.phone = &apos;18512341234&apos;In [13]: idc.email = &apos;zw@qq.com&apos;In [14]: idc.letter = &apos;zw&apos;In [15]: idc.save() 2.3.1、正向序列化正向序列化的定义：从“数据库”里获取数据并序列化后返回标准JSON类型的数据给前端 具体使用序列化的操作如下：1234567891011121314151617181920212223242526In [17]: from idcs.serializers import IdcSerializer # 导入序列化类In [18]: idc = Idc.objects.get(pk=1) # 获取一条IDC信息，等待序列化In [19]: idcOut[19]: &lt;Idc: 亦庄机房&gt;In [20]: serializers = IdcSerializer(idc) # 将idc对象传给序列化类 进行序列化操作后存放到一个变量里，序列化完成。In [21]: serializers # 输出结果，会把序列化里的字段给打印出来Out[21]:IdcSerializer(&lt;Idc: 亦庄机房&gt;): id = IntegerField() name = CharField() address = CharField() phone = CharField() email = CharField() letter = CharField()In [23]: serializers.data # 通过.data能获取到所有的数据Out[23]: &#123;&apos;id&apos;: 1, &apos;name&apos;: &apos;亦庄机房&apos;, &apos;address&apos;: &apos;北京亦庄机房&apos;, &apos;phone&apos;: &apos;12312341234&apos;, &apos;email&apos;: &apos;nick@qq.com&apos;, &apos;letter&apos;: &apos;yz&apos;&#125;In [24]: a = serializers.dataIn [25]: type(a)Out[25]: rest_framework.utils.serializer_helpers.ReturnDict 能看到serializers.data的类型是一个ReturnDict，而我们最终需求是需要转换成Json类型返回给前端的，那如何转换呢？ 使用drf内置模块将结果转换成标准的json数据：1234567891011121314In [26]: from rest_framework.renderers import JSONRendererIn [27]: ?JSONRendererInit signature: JSONRenderer()Docstring: Renderer which serializes to JSON.File: ~/Projects/Python/v3/VirtualSource/venv/lib/python3.6/site-packages/rest_framework/renderers.pyType: typeIn [28]: jr = JSONRenderer() # 实例化In [29]: jr.render(serializers.data) # 使用render方法将结果转换成标准的Json数据Out[29]: b&apos;&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;\xe4\xba\xa6\xe5\xba\x84\xe6\x9c\xba\xe6\x88\xbf&quot;,&quot;address&quot;:&quot;\xe5\x8c\x97\xe4\xba\xac\xe4\xba\xa6\xe5\xba\x84\xe6\x9c\xba\xe6\x88\xbf&quot;,&quot;phone&quot;:&quot;12312341234&quot;,&quot;email&quot;:&quot;nick@qq.com&quot;,&quot;letter&quot;:&quot;yz&quot;&#125;&apos;In [30]: content = jr.render(serializers.data) # 拿到这个数据，就可以直接返回给前端了。 正向序列化多条记录：1234567891011121314151617181920212223242526In [1]: from idcs.models import IdcIn [2]: from idcs.serializers import IdcSerializerIn [3]: Idc.objects.all()Out[3]: &lt;QuerySet [&lt;Idc: 亦庄机房&gt;, &lt;Idc: 兆维机房&gt;]&gt;In [4]: data = IdcSerializer(Idc.objects.all(), many=True) # 关键在于这一步，使用many=True声明传入的是多个object对象In [5]: dataOut[15]: IdcSerializer(&lt;QuerySet [&lt;Idc: 亦庄机房&gt;, &lt;Idc: 兆维机房&gt;]&gt;, many=True): id = IntegerField() name = CharField() address = CharField() phone = CharField() email = CharField() letter = CharField()In [6]: from rest_framework.renderers import JSONRendererIn [6]: content = JSONRenderer().render(data.data)In [7]: contentOut[7]: b&apos;[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;\xe4\xba\xa6\xe5\xba\x84\xe6\x9c\xba\xe6\x88\xbf&quot;,&quot;address&quot;:&quot;\xe5\x8c\x97\xe4\xba\xac\xe4\xba\xa6\xe5\xba\x84\xe6\x9c\xba\xe6\x88\xbf&quot;,&quot;phone&quot;:&quot;12312341234&quot;,&quot;email&quot;:&quot;nick@qq.com&quot;,&quot;letter&quot;:&quot;yz&quot;&#125;,&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;\xe5\x85\x86\xe7\xbb\xb4\xe6\x9c\xba\xe6\x88\xbf&quot;,&quot;address&quot;:&quot;\xe5\x85\x86\xe7\xbb\xb4\xe5\xb7\xa5\xe4\xb8\x9a\xe5\x9b\xad&quot;,&quot;phone&quot;:&quot;18512341234&quot;,&quot;email&quot;:&quot;zw@qq.com&quot;,&quot;letter&quot;:&quot;zw&quot;&#125;]&apos; 小结：序列化的过程如下 mysql获取数据 queryset = Idc.objects.all() #获取单个或所有idc对象 content = JSORNRenderer().render(queryset) # 转换成标准Json数据 HttpResponse(content) # 返回给前端 重点：上面的小结里，能看到序列化的过程，那序列化还能做些什么事情呢？它可以验证前端传入过来的数据，并且添加、更新这些数据。 2.3.2、反向序列化反向序列化的定义：从“前端接口”接收添加的数据并序列化后返回一个object对象给后端，而且可以进行数据验证以及添加到数据库的操作。 编写序列化验证、创建、更新、保存规则：123456789101112131415161718192021222324252627282930$ vim idcs/serializers.pyfrom rest_framework import serializersfrom .models import Idc # 需要导入Idc模型class IdcSerializer(serializers.Serializer): &quot;&quot;&quot; Idc, 序列化类 &quot;&quot;&quot; id = serializers.IntegerField(read_only=True) # 处理只读 name = serializers.CharField(required=True, max_length=32) # 字段必须传, 最大限制32个字符 address = serializers.CharField(required=True, max_length=256) phone = serializers.CharField(required=True, max_length=15) email = serializers.CharField(required=True) letter = serializers.CharField(required=True, max_length=5) def create(self, validated_data): return Idc.objects.create(**validated_data) # 调用Idc模型进行create操作 def update(self, instance, validated_data): # 参数介绍：instance是当前的对象，validated_data是处理过的干净数据 &apos;&apos;&apos; update方法可以允许修改什么字段，如果有不需要修改的字段，不写即可 &apos;&apos;&apos; instance.name = validated_data.get(&quot;name&quot;,instance.name) instance.address = validated_data.get(&quot;address&quot;,instance.name) instance.phone = validated_data.get(&quot;phone&quot;,instance.name) instance.email = validated_data.get(&quot;email&quot;,instance.name) instance.save() return instance 注意：在“正向序列化”的时候，CharField字段里的参数都没有作用，只有在“反向序列化”是才会有作用。 反向序列化的操作过程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849In [1]: from idcs.serializers import IdcSerializerIn [2]: data = &#123;&apos;id&apos;: 1,&apos;name&apos;: &apos;数北机房&apos;,&apos;address&apos;: &apos;北京数北机房&apos;,&apos;phone&apos;: &apos;12312341234&apos;,&apos;email&apos;: &apos;aaa@123.com&apos;,&apos;letter&apos;: &apos;yz&apos;&#125;In [2]: data = &#123;&apos;id&apos;: 1, ...: &apos;name&apos;: &apos;亦庄机房&apos;, ...: &apos;address&apos;: &apos;北京亦庄机房&apos;, ...: &apos;phone&apos;: &apos;12345678&apos;, ...: &apos;email&apos;: &apos;aaa@123.com&apos;, ...: &apos;letter&apos;: &apos;yz&apos;&#125; # 这里的data数据模拟前端接口传进来的数据哈In [3]: serializer = IdcSerializer(data=data) # 将Json格式的数据传入IdcSerializer进行序列化In [4]: serializer # 得到序列化后的结果Out[4]: IdcSerializer(data=&#123;&apos;id&apos;: 1, &apos;name&apos;: &apos;亦庄机房&apos;, &apos;address&apos;: &apos;北京亦庄机房&apos;, &apos;phone&apos;: &apos;12345678&apos;, &apos;email&apos;: &apos;rock@51reboot.com&apos;, &apos;letter&apos;: &apos;yz&apos;&#125;): id = IntegerField(read_only=True) name = CharField(max_length=32, required=True) address = CharField(max_length=256, required=True) phone = CharField(max_length=15, required=True) email = EmailField(required=True) letter = CharField(max_length=5, required=True)In [5]: serializer.is_valid() # 查看验证是否通过，调用的是(required=True, max_length=32)这些条件参数哈Out[5]: TrueIn [6]: serializer.validated_data # 通过validated_data获取数据Out[6]: OrderedDict([(&apos;name&apos;, &apos;亦庄机房&apos;), (&apos;address&apos;, &apos;北京亦庄机房&apos;), (&apos;phone&apos;, &apos;12345678&apos;), (&apos;email&apos;, &apos;aaa@123.com&apos;), (&apos;letter&apos;, &apos;yz&apos;)])In [7]: del data[&quot;id&quot;] # 删除id数据后方便添加到数据库中In [8]: dataOut[8]: &#123;&apos;name&apos;: &apos;亦庄机房&apos;,&apos;address&apos;: &apos;北京亦庄机房&apos;,&apos;phone&apos;: &apos;12345678&apos;,&apos;email&apos;: &apos;aaa@123.com&apos;,&apos;letter&apos;: &apos;yz&apos;&#125;In [9]: serializer = IdcSerializer(data=data) # 重新执行验证In [10]: serializer.is_valid()Out[10]: TrueIn [11]: serializer.save() # 这里的save调用了IdcSerializer类里我们写的create方法Out[11]: &lt;Idc: 亦庄机房&gt; 上面在编写IdcSerializer序列化类的时候，写了create和update方法，那么这两个方法有什么用呢？又是如何使用的呢？ 重点概念：Django能自动判断你提交的请求是需要增加，判断的标准是基于ID，如果传入的数据里有ID的话，那么认为你是需要进行修改，没有ID则认为是需要进行创建 在上面操作细节中，在执行serializer.save() 方法时，事实上是调用了IdcSerializer类的create方法，就是基于Django智能判断来实现的。 2.4、序列化总结1、正向序列化1234561. 拿到quertset2. 将quertset给序列化类 serializer = IdcSerializer(idc) serializer = IdcSerializer(Idc.objects.all(), many=True)3. 转JSON JSONRenderer().render(serializer.data) 2、反向序列化1234data = JSONRenderer().parse(content)serializer = IdcSerializer(data=data)serializer.is_valid()serializer.save() 正向序列化：从数据库里拿出来数据，然后返回Json给前端反向序列化：从前端接口拿到数据（data），将数据转换成数据流，然后序列化验证后，保存到数据库中。 本文由“Rock”布道，本站学习和整理发布。]]></content>
      <categories>
        <category>Python开发</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>DRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《自动化发现主机并添加监控》]]></title>
    <url>%2F2018%2F06%2F14%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8F%91%E7%8E%B0%E4%B8%BB%E6%9C%BA%E5%B9%B6%E6%B7%BB%E5%8A%A0%E7%9B%91%E6%8E%A7%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言，本文将介绍zabbix强大功能之一，自动化发现主机并将其添加到zabbix监控平台，思考一下，如果贵公司上线20台物理机或云主机，需要将这些主机添加到监控平台，如果一台一台添加的话那基础设施运维就有活干了！那本文就是介绍zabbix是如何解决这个问题的。 创建思路首先说下自动发现强大的功能，到底可以完成什么工作： 快速发现并添加主机; 简单的管理; 随着环境的改变而快速搭建监控系统; 自动发现基于网络发现功能，而网络发现又基于以下信息： IP地址段; 基于服务的FTP、SSH、Web、POP3、IMAP、TCP等; 从Zabbix-agent端接收的信息; 从SNMP agent端接受的信息; 可以理解创建自动发现的过程为，zabbix-server会扫描添加的IP地址段。比如需要添加IP地址段为192.168.80.100到192.168.80.199这个区间的机器，设定好网络区间。当zabbix-server扫描到已经启动的些机器时，下一步需要触发动作，什么类型的机器进行什么操作。比如linux机器添加linux模板，并且添加到KVM虚拟机的分组当中。当完成了这些操作，zabbix主机的添加也就已经完成了。下面先进行动作的设置。 创建动作上面简单介绍了下创建思路，有了简单的了解，那么下面来看看是如何创建的 依次点击以下位置： 配置 动作 自动发现 创建动作 填写动作位置名称，这里设置的是自动发现，当然可以设置多个动作，就像上边说的不同主机不同动作。 点击动作旁边的条件选项，下拉选择主机地址，并设置符合条件的IP地址区间段。 选择新的触发条件选择下拉框中的服务类型，在选择Zabbix客户端 点击操作 选择操作类型为：添加到主机群组 选择要添加的主机群组 先点击里面的添加 再点击外边的添加 （4和5这个步骤是zabbix的老毛病了，注意好顺序。） 此处就是外边的那个添加。 添加操作为主机选择添加主机。 添加关联模板 添加完成 创建发现规则上面的创建动作完成后，紧接着就需要创建发现规则，因为有了这个规则，才能完成一系列自动发现、自动添加监控、触发告警等动作。 点击自动发现→创建发现规则 点击主机 自动发现： 填写名称 由agent代理程序自动发现 IP范围：填写发现范围 延迟：此处按秒计算 添加检查：此处添加zabbix的uname 设备唯一性准则：按IP地址区分 最后点击启用，添加。 自动发现就添加完成了，点击到首页仪表板，在agent配置没问题的情况下，应该添加到主机了。 已经有机器被发现并自动添加到zabbix监控平台。 本文来源于作者：“祁成” 投稿，由本站整理发布，感谢作者辛勤付出。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>自动发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《WEB场景监控》]]></title>
    <url>%2F2018%2F06%2F14%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8AWEB%E5%9C%BA%E6%99%AF%E7%9B%91%E6%8E%A7%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：本文将带你了解Zabbix在监控日常web业务上会有哪些优异的表现。 回想一下你曾今登录JD Shopping的时候，都经历了哪些步骤呢？ 因此我进行了分析，步骤大致如下： 打开页面–&gt;登录–&gt;登录验证–&gt;退出。 在工作中，有些业务是需要监控web平台的登录是否正常，有些平台的登录认证也比较复杂，那如果做简单的监控室无法实现的。因此，我们通过下面的例子来进行模拟，监控的是本身的zabbix网站，因为zabbix本身是有用户认证功能的。 检测流程 打开网站：如果http code为200，并且响应的html中包含Zabbix SIA表示打开成功（zabbix页面有这个标示） 登陆后台：post用户名和密码到index.php，如果响应200，那表示post成功。并且通过正则表达式从响应的html中匹配sid，这个sid也就是一个宏变量，退出可以使用到 验证登陆：打开首页，检索html中是否包含Profile（只有登陆成功，才会有Profile出现，它在成功页面的右上角，一个按钮） 退出账号：传递参数sid给index.php即可退出，响应200即表示退出成功。可以使用item key来获取每个step的速度以及响应时间或者说最新的一个错误消息，自己去研究吧，不难。 创建Web scenarios 在这里填写登录zabbix的用户名密码，设定为变量。 打开首页 登录 登录验证 退出登录 保存配置全部填写完成之后记得保存 查看结果monitorning-&gt;web-&gt;筛选出你的主机-&gt;查看“zabbix性能监控”，结果如下图各个阶段的响应时间、速度、返回状态码以及总的响应时间 创建触发器系统为每个step创建了3个item，分别是DownloadSpeed页面下载速度/ResponseCode响应代码/ResponseTime响应时间，为整个Web Scenario创建了一个web.test.fail的item和一个web.test.error的item，可以分别为其创建trigger。 创建一个监控Zabbix登陆失败的触发器 创建一个监控整个web scenario所有step运行是否成功的触发器 返回值为0表示整个web scenario的所有step都执行成功了，第几步的step执行失败就返回数字几，且后续的step都不会继续执行下去。 本文来源于作者：“祁成” 投稿，由本站整理发布，感谢作者辛勤付出。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker系列之《善用DockerHub管理自己的镜像》]]></title>
    <url>%2F2018%2F06%2F12%2FDocker%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E5%96%84%E7%94%A8DockerHub%E7%AE%A1%E7%90%86%E8%87%AA%E5%B7%B1%E7%9A%84%E9%95%9C%E5%83%8F%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：在上一篇文章里我们简单介绍了Docker Hub，这篇文章带着大家深入学习如何使用Docker开源的镜像托管仓库。对于个人制作的镜像而言，托管在Docker Hub镜像仓库上是一个非常不错的选择。 Docker Hub的官网是：https://hub.docker.com/它与同源代码托管服务的Github类似，不同的是Docker Hub提供的是镜像托管服务。利用Docker Hbu，我们可以搜索、创建、分享和管理镜像，还可以利用其提供的自动化构建技术直接在集群云服务器上构建镜像。 Docker Hub为用户提供不限数目的公开镜像托管服务，但仅提供一个私有镜像托管服务。如果需要更多的私有镜像托管，需要额外付费。 镜像的分发想要将本机创建的镜像分发到互联网提供其他用户使用，最便捷的方式就是使用Docker Hub。首先登录Docker Hub官网注册 可以使用GitHub帐号登录，请自行探索。注册成功后，在命令行登录我们刚注册的帐号： 1234567891011121314151617# docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: nicksorsPassword:Login Succeeded# cat /root/.docker/config.json # 这里是登录后的配置文件&#123; "auths": &#123; "https://index.docker.io/v1/": &#123; "auth": "bmlja3NvcnM6YWJjMTIzISE=" &#125; &#125;, "HttpHeaders": &#123; "User-Agent": "Docker-Client/18.03.1-ce (linux)" &#125;&#125; 用户名和密码通过哈希运算之后保存在auth字段，这样可以保证密码的安全性。当然，我们也可以在首次上传镜像时由Docker主动提示我们输入密码。 登录成功后，使用push命令上传镜像，如果不指定镜像TAG，指定的仓库在本地的所有镜像上都会上传到Docker Hub。下面的push将imported镜像上传到Docker Hub： 12345# docker images importedREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEimported container a5d0a399ea83 38 minutes ago 144.7 MBimported v1 a5d0a399ea83 38 minutes ago 144.7 MB# docker push nicksors/imported # 将本地的imported推送到docker hub上的nicksors仓库里 遇到的错误1：在上传时，必须在镜像名字前面加用户ID ，否则会出现错误信息：1Error response from daemon: You cannot push a &quot;root&quot; repository. Please rename your repository to &lt;user&gt;/&lt;repo&gt; (ex: nicksors/imported) 解决方法：12# docker tag imported:v1 nicksors/imported:v1# docker push nicksors/imported:v1 #再执行就可以了 可以看到镜像正在上传，输出信息如下：123456# docker push nicksors/imported:v1Do you really want to push to public registry? [y/n]: yThe push refers to a repository [docker.io/nicksors/imported] (len: 1)a5d0a399ea83: Pushed v1: digest: sha256:01e7412fcf2bc44191d874a6be336988cb266ebbc0b4928fc63fdbd1dc5f9733 size: 1197 上传成功后可以在Docker Hub上看到提交的镜像： 上传到公网镜像仓库的一些重要注意事项 要上传到公网镜像仓库时，要先给本地镜像打一个标签tag； 本地镜像名中要包含公网的网址和账号； 1234 $ docker tag imported:v1 docker.io/nicksors/imported:v1 然后再执行上传就可以了，$ docker push docker.io/nicksors/imported:v1 个人理解： Linux拷贝命令 cp 为例，它含两个参数，一个是源文件、另一个是目标文件，cp 源文件 目标文件 docker push 只有一个参数，要想把本地的文件上传到公网或私有仓库中，只有用docker tag 命令，给本地镜像多打一个标签，使得本地镜像包含公网或私有仓库的域名或IP地址信息，这样执行docker push 时docker 就知道将进行上传到哪里了。 怎么样，你学会如何使用Docker Hub了吗！]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>DockerHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker系列之《企业级私有仓库解决方案》]]></title>
    <url>%2F2018%2F06%2F11%2FDocker%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%81%E4%B8%9A%E7%BA%A7%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言： Docker Hub作为Docker默认官方公共镜像，如果自己想搭建私有镜像仓库，官网也提供registry镜像，使得搭建私有仓库非常简单。 一、搭建私有镜像仓库:registry1.1、下载registry镜像并启动12$ docker pull registry$ docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --restart=always --name registry registry 1.2、测试，查看镜像仓库中所有镜像12$ curl http://127.0.0.1:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125; 1.3、私有镜像仓库管理1）配置私有仓库可信任1234567891011$ vim /etc/docker/daemon.json&#123;"insecure-registries":["127.0.0.1:5000"]&#125;systemctl restart docker这里需要注意一点，如果daemon.json里已经有配置，"需要在原有配置后面加逗号，不然失效" 譬如：$ cat /etc/docker/daemon.json&#123; "registry-mirrors": [ "https://registry.docker-cn.com"], "insecure-registries": ["172.16.194.130:5000"]&#125; 2）打标签1$ docker tag centos:6 127.0.0.1:5000/centos:7 3）上传1$ docker push 127.0.0.1:5000/centos:7 4）下载1$ docker pull 127.0.0.1:5000/centos:7 5）列出镜像标签12$ curl http://127.0.0.1:5000/v2/centos/tags/list&#123;"name":"centos","tags":["7"]&#125; 注意：127.0.0.1可以换成你网卡的地址 二、Docker Hub公共镜像仓库使用因为dockerHub是国外的服务器，push和pull操作都比较慢，甚至有连接超时的情况，个人研究玩玩还是可以的。https://cloud.docker.com/swarm/nicksors 1）注册帐号https://hub.docker.com 2）登录Docker Hub12345$ docker login或$ docker login --username=nicksors --password=xxxWARNING! Using --password via the CLI is insecure. Use --password-stdin.Login Succeeded 3）镜像打标签1$ docker tag nginx:1.12 nicksors/nginx:v2 4）上传1$ docker push nicksors/nginx:v2 5）下载1$ docker pull nicksors/nginx:v2 三、基于Harbor搭建Docker私有镜像仓库（推荐:很多企业都用这个）3.1、什么是Harbor？Harbor是VMware开源的又一个Docker Registry企业级私有仓库，其项目地址为https://github.com/vmware/harbor；相比Docker公司自己提供的Registry私有镜像仓库而言，Harbor提供了更多的功能，如下： 基于角色的访问控制 - 用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 镜像复制 - 镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。 图形化用户界面 - 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。 AD/LDAP 支持 - Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。 审计管理 - 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。 国际化 - 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。 RESTful API - RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。 部署简单 - 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。 以上来自官网介绍：https://vmware.github.io/harbor/cn/ 3.2、准备环境 自己创建的虚拟机：CentOS7.2、配置是2G2C； Docker版本：Docker version 18.03.0-ce Docker-compose：docker-compose version 1.20.1 Harbor版本：harbor-offline-installer-v1.4.0.tgz 3.3、安装Harbor在安装Harbor之前，必须保证你的环境已经安装好docker和docker-compose了,这两个安装方法在Docker官网都有：12安装Docker方法：https://docs.docker.com/install/linux/docker-ce/centos安装Docker-Compose方法：https://docs.docker.com/compose/install/#install-compose 你可以在 Harbor版本https://github.com/vmware/harbor/releases 地址下载你想要装的版本，这里我选择最新的1.4.0，当然你看到的时候已经不是最新版本了。123# 选择离线安装版本$ wget https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-offline-installer-v1.4.0.tgz（如果下载慢的话，你可以使用迅雷下载，有的网友就这么干，会快很多） 解压下载的包，进入解压后的harbor目录，里面有个harbor.cfg就是配置文件啦，简单说下：这里面可以配置LDAP，数据库，邮件信息，ssl证书等。12345$ vim harbor.cfg# 我配置了两个地方，主机名和Harbor admin的密码，其他默认hostname = 172.16.194.130harbor_admin_password = abc123!! 下面奉上一份harbor.cfg的关键参数说明：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145## Configuration file of Harbor#hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhosthostname = 172.16.194.130 #这里我使用本机IP# 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置onui_url_protocol = http#Maximum number of job workers in job servicemax_job_workers = 3#Determine whether or not to generate certificate for the registry's token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer's cert.customize_crt = on# 指定的证书文件，生产环境一定要使用ssl证书ssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.key# 存放证书的路径,这个路径会挂载到宿主机的/data/目录下secretkey_path = /data#Admiral's url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.log_rotate_count = 50#Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.#If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G#are all valid.log_rotate_size = 200M#************************BEGIN INITIAL PROPERTIES************************# 配置邮件server信息email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = falseemail_insecure = false# 启动Harbor后，管理员UI登录的密码，默认是Harbor12345harbor_admin_password = admin123# 认证方式，这里支持多种认证方式，如LADP、本次存储、数据库认证。默认是db_auth，mysql数据库认证auth_mode = db_auth# ldap配置ldap_url = ldaps://ldap.mydomain.com#A user's DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/ADldap_uid = uid#the scope to search for users, 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREEldap_scope = 2#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5#Verify certificate from LDAP serverldap_verify_cert = true#Turn on or off the self-registration featureself_registration = on#The expiration time (in minute) of token created by token service, default is 30 minutestoken_expiration = 30# 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）project_creation_restriction = everyone#************************END INITIAL PROPERTIES************************#######Harbor DB configuration section########The address of the Harbor database. Only need to change when using external db.db_host = mysql#The password for the root user of Harbor DB. Change this before any production use.db_password = root123#The port of Harbor database hostdb_port = 3306#The user name of Harbor databasedb_user = root##### End of Harbor DB configuration########The redis server address. Only needed in HA installation.redis_url =##########Clair DB configuration#############Clair DB host address. Only change it when using an exteral DB.clair_db_host = postgres#The password of the Clair's postgres database. Only effective when Harbor is deployed with Clair.#Please update it before deployment. Subsequent update will cause Clair's API server and Harbor unable to access Clair's database.clair_db_password = password#Clair DB connect portclair_db_port = 5432#Clair DB usernameclair_db_username = postgres#Clair default databaseclair_db = postgres##########End of Clair DB configuration#############The following attributes only need to be set when auth mode is uaa_authuaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pem### Docker Registry setting ####registry_storage_provider can be: filesystem, s3, gcs, azure, etc.registry_storage_provider_name = filesystem#registry_storage_provider_config is a comma separated "key: value" pairs, e.g. "key1: value, key2: value2".#Refer to https://docs.docker.com/registry/configuration/#storage for all available configuration.registry_storage_provider_config = 3.4、启动Harbor修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当期目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动各个服务。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293$ ./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.03.0Note: docker-compose version: 1.20.1[Step 1]: loading Harbor images ...Loaded image: vmware/notary-server-photon:v0.5.1-v1.4.0Loaded image: vmware/notary-signer-photon:v0.5.1-v1.4.0Loaded image: vmware/harbor-db:v1.4.0Loaded image: vmware/clair-photon:v2.0.1-v1.4.0Loaded image: vmware/postgresql-photon:v1.4.0Loaded image: vmware/harbor-adminserver:v1.4.0Loaded image: vmware/harbor-ui:v1.4.0Loaded image: vmware/harbor-log:v1.4.0Loaded image: vmware/harbor-jobservice:v1.4.0Loaded image: vmware/nginx-photon:v1.4.0Loaded image: vmware/registry-photon:v2.6.2-v1.4.0Loaded image: vmware/photon:1.0Loaded image: vmware/mariadb-photon:v1.4.0Loaded image: vmware/harbor-db-migrator:1.4[Step 2]: preparing environment ...Clearing the configuration file: ./common/config/adminserver/envClearing the configuration file: ./common/config/ui/envClearing the configuration file: ./common/config/ui/app.confClearing the configuration file: ./common/config/ui/private_key.pemClearing the configuration file: ./common/config/db/envClearing the configuration file: ./common/config/jobservice/envClearing the configuration file: ./common/config/jobservice/app.confClearing the configuration file: ./common/config/registry/config.ymlClearing the configuration file: ./common/config/registry/root.crtClearing the configuration file: ./common/config/nginx/nginx.confClearing the configuration file: ./common/config/log/logrotate.confloaded secret from file: /data/secretkeyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/adminserver/envGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/log/logrotate.confGenerated configuration file: ./common/config/jobservice/app.confGenerated configuration file: ./common/config/ui/app.confGenerated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ...Creating harbor-log ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-adminserver ... doneCreating harbor-ui ... doneCreating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://172.16.194.130.For more details, please visit https://github.com/vmware/harbor .# 这时候你可以通过docker-compose 或docker ps来查看Harbor依赖运行的一些容器# 当然你也可以通过docker-compose来管理这些容器$ docker-compose ps Name Command State Ports-------------------------------------------------------------------------------------------------------------------------------------harbor-adminserver /harbor/start.sh Up (healthy)harbor-db /usr/local/bin/docker-entr ... Up (healthy) 3306/tcpharbor-jobservice /harbor/start.sh Up (healthy)harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcpharbor-ui /harbor/start.sh Up (healthy)nginx nginx -g daemon off; Up 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpregistry /entrypoint.sh serve /etc/ ... Up (healthy) 5000/tcp$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbaf9d3e586f8 vmware/harbor-jobservice:v1.4.0 "/harbor/start.sh" About an hour ago Up About an hour (healthy) harbor-jobservice484d5c4fca4b vmware/nginx-photon:v1.4.0 "nginx -g 'daemon of…" About an hour ago Up About an hour 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp nginxdd7c62b45af1 vmware/harbor-ui:v1.4.0 "/harbor/start.sh" About an hour ago Up About an hour (healthy) harbor-uie5494bd12f64 vmware/registry-photon:v2.6.2-v1.4.0 "/entrypoint.sh serv…" About an hour ago Up About an hour (healthy) 5000/tcp registry915b753623b7 vmware/harbor-adminserver:v1.4.0 "/harbor/start.sh" About an hour ago Up About an hour (healthy) harbor-adminserver55ca16b86243 vmware/harbor-db:v1.4.0 "/usr/local/bin/dock…" About an hour ago Up About an hour (healthy) 3306/tcp harbor-db30ca0cb76dd0 vmware/harbor-log:v1.4.0 "/bin/sh -c /usr/loc…" About an hour ago Up About an hour (healthy) 127.0.0.1:1514-&gt;10514/tcp 3.5、登录Harbor启动完成后，会提示你Harbor的访问地址：http://172.16.194.130 登录界面 输入账号和我们预先设定的密码：admin/admin123 我们可以看到系统各个模块如下： 项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等 日志：仓库各个镜像create、push、pull等操作日志 系统管理 用户管理：新增/删除用户、设置管理员等 复制管理：新增/删除从库目标、新建/删除/启停复制规则等 配置管理：认证模式、复制、邮箱设置、系统设置等 其他设置 用户设置：修改用户名、邮箱、名称信息 修改密码：修改用户密码 注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。 3.6、向Harbor仓库中心提交私有镜像我们要尝试下能不能把自己 Docker 里面的镜像 push 到 Harbor 的 library 里来（默认这个 library 项目是公开的，所有人都可以有读的权限，都不需要 docker login 进来，就可以拉取里面的镜像）。 3.6.1、配置Docker registry仓库地址在/etc/docker/daemon.json里添加配置如下：123&#123; &quot;insecure-registries&quot;: [&quot;172.16.194.130&quot;]&#125; 配置好后，别忘了重启systemctl restart docker 3.6.2、Docker 登录Harbor为什么要登录呢？跟Docker Hub一样，你得登录才能表明你是合法用户，才能push；1234$ docker login 172.16.194.130Username: adminPassword: (这里输入harbor平台设置的admin密码)Login Succeeded 3.6.3、本地私有镜像打tag，提交到Harbor1234567891011$ docker tag tale:base 172.16.194.130/library/tale:base$ docker push 172.16.194.130/library/taleThe push refers to repository [172.16.194.130/library/tale]a3ece4722ead: Pusheded61150eb02c: Pushed0f9f3d37a459: Pushed8ed018b01f91: Pushedb17185091796: Pushedb03095563b79: Pushedbase: digest: sha256:f82f2e175479d6d232efab45f81a4495cc4ad0a48135fd839dc27fdee8c13c77 size: 1574 提交成功，我们来看看Harbor仓库里的信息 能看到已经提交到libary公共仓库中。 同理，你也可以测试下从 Harbor pull 镜像到你的 Docker 中去，操作如下：123456789$ docker rmi 172.16.194.130/library/tale:base$ docker pull 172.16.194.130/library/tale:basebase: Pulling from library/taleDigest: sha256:f82f2e175479d6d232efab45f81a4495cc4ad0a48135fd839dc27fdee8c13c77Status: Downloaded newer image for 172.16.194.130/library/tale:base$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE172.16.194.130/library/tale base ab8e3ca33cd0 5 days ago 372MB 镜像在被我删除后，从Harbor里成功pull了回来。 3.7、Harbor配置ssl认证3.7.1、创建证书1$ cd /data/cert/ 1、创建 CA 根证书1$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt -subj &quot;/C=CN/L=xian/O=nova/CN=harbor-registry&quot; 2、生成一个证书签名, 设置访问域名为harbor.moxiu.cn1$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout harbor.moxiu.cn.key -out server.csr -subj &quot;/C=CN/L=xian/O=nova/CN=harbor.moxiu.cn&quot; 3、生成主机的证书1$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out harbor.moxiu.cn.crt 3.7.2、配置harbor以https方式访问12345$ vim harbor.cfghostname = harbor.moxiu.cn:443ui_url_protocol = httpsssl_cert = /data/cert/harbor.moxiu.cn.crtssl_cert_key = /data/cert/harbor.moxiu.cn.key 3.7.3、配置Docker registry仓库地址在/etc/docker/daemon.json里添加配置如下：123&#123; "insecure-registries": ["harbor.moxiu.cn"]&#125; 然后，重启docker服务生效 3.7.4、登录验证1、验证admin登录方法11234$ docker login harbor.moxiu.cnUsername (admin): adminPassword: Login Succeeded 2、验证admin登录方法212$ docker login -u admin -p abc123!! harbor.moxiu.cnLogin Succeeded 3、Web页面登录验证http://harbor.moxiu.cn/harbor/sign-in用户名/密码：admin/abc123!! 因为不是有效机构颁发的证书，所有浏览器会提示不安全。如果企业需要使用，那需要买商用的证书更换即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>harbor</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《监控TCP连接状态》]]></title>
    <url>%2F2018%2F06%2F08%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E7%9B%91%E6%8E%A7TCP%E8%BF%9E%E6%8E%A5%E7%8A%B6%E6%80%81%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：在运维工作中，服务器的TCP连接情况一致是关注的范畴，TCP活跃连接突然增高？是什么原因导致？机器被黑了？web业务流量上来了？谁知道呢… 今天为大家介绍zabbix监控服务器的TCP连接状态，看完本文你将学到如下知识： 撰写zabbix自定义监控key和如何使用自定义脚本； 学会自己创建模板，并在模板里创建一个图表； 效果展示 服务器的TCP连接通过zabbix监控并用上面的图表呈现出来，可以横向与历史数据进行对比，一眼看出当前值是否正常。这个图表是自己创建的，下面有创建的方法。 那么我们接下来就开始学习之旅~ 客户端撰写自定义脚本我使用shell写了一个脚本，便于获取TCP的各种状态的值12345678[root@adminset ~]# sh tcp.sh total91[root@adminset ~]# sh tcp.sh ESTABLISHED41[root@adminset ~]# sh tcp.sh TIME_WAIT51[root@adminset ~]# sh tcp.sh CLOSE_WAIT0 脚本内容如下：vim tcp.sh123456789101112131415#!/bin/bash#获取各种状态连接数if [[ "$1" = "" || "$1" = "total" ]]then netstat -n |grep 'tcp'|wc -l exit 0;fistr=`netstat -n | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;'|grep $1`if [[ "$str" = "" ]]then echo 0;else echo $str|awk '&#123;print $2&#125;'fi 把此文件存放到/usr/lib/zabbix/externalscripts/里(没有的话创建)，然后给与755权限，并修改用户与组为zabbix，同时允许zabbix用户无密码运行netstat12echo "zabbix ALL=(root) NOPASSWD:/bin/netstat"&gt;&gt;/etc/sudoerssed -i 's/^Defaults.*.requiretty/#Defaults requiretty/' /etc/sudoers #不关闭的话，会无法获取数据，并且zabbix日志里报 创建自定义key在Zabbix系列之《安装Agent客户端并添加主机监控》一文里写到zabbix_agent.conf配置文件中，Include参数包含了/etc/zabbix/zabbix_agentd.d/*.conf这个目录下所有以.conf结尾的文件。 在/etc/zabbix/zabbix_agentd.d/目录里创建tcp.conf文件，写入下面内容：12# tcp连接数UserParameter=netstat.conn[*], /usr/lib/zabbix/externalscripts/tcp.sh $1 关于zabbix 自定义key的写法，这里没法展开说明，还请善用搜索框和官网。 测试在测试前，需要重启客户端程序，加载刚添加的配置1systemctl restart zabbix-agent 我的测试结果：123456[root@adminset ~]# zabbix_get -s 172.16.194.128 -p 10050 -k "netstat.conn[total]"83[root@adminset ~]# zabbix_get -s 172.16.194.128 -p 10050 -k "netstat.conn[ESTABLISHED]"44[root@adminset ~]# zabbix_get -s 172.16.194.128 -p 10050 -k "netstat.conn[CLOSE_WAIT]"0 如果你能通过zabbix_get命令获取到值，说明你的客户端配置完全没问题了，接下来就是服务端添加监控。什么？你没有zabbix_get命令？天啦！请查看：解决方法 服务端创建模板1、创建一个名为“Template OS Netstat”的模板点击 【Configuration】–&gt;【Templates】–&gt;【Create templateImport】点击Add添加。 2、创建一个名为“Netstat”的Applications点击刚创建的模板，【Applications】–&gt;【Create application】 3、创建ITEMS(监控项)创建一个item 创建好的items上面所有的items都是一个一个加上去的，如果你要自己制作模板，就得这么干！除此之外别无他法。 4、创建Graphs（图表）点击 【Graphs】–&gt;【Create graph】 到这里，整个创建模板的过程就算完成了。 模板导入如果你嫌上面的步骤麻烦，只是想使用这个模板的话，这里提供了一种便捷的方法：我将上面制作的模板导出，并提供你下载。 请点击下载：Template OS Netstat模板，下载后在你的zabbix里导入该模板即可。 主机关联模板把需要监控的主机添加模板关联即可监控 本文讲解了如何自定义脚本和key，并且通过案例演示了创建模板的过程，以及提供创建的模板给大家，今后在工作中自己也可以尝试着做模板，一方面为了自己学习，另一方面自己做的模板给他人使用，你会有成就感。 好了，本文就到这，如果你在阅读或使用文章中遇到问题，欢迎加入QQ群：32330026，我们是一群爱学习的人，期待与你一起学习进步。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>netstat</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《安装Agent客户端并添加主机监控》]]></title>
    <url>%2F2018%2F06%2F07%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E5%AE%89%E8%A3%85Agent%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B9%B6%E6%B7%BB%E5%8A%A0%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：上一片文章讲解了如何快速安装zabbix，本文将介绍安装zabbix和增加一个主机监控的细节，不足之处还望指出。 安装ZabbixAgent使用yum快速安装1yum install zabbix-agent 修改配置文件123find / -name &apos;*zabbix_agentd.conf*&apos;# /etc/zabbix/zabbix_agentd.confcp /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak 修改相关具体项123456789# vim /etc/zabbix/zabbix_agentd.confPidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=zabbix_server_IP zabbix服务器ip地址ServerActive=zabbix_server_IP 主动向zabbix server发送监控内容Hostname=Monitor_host agent节点的host主机名，在添加监控的时候要与这个名称一致UnsafeUserParameters=1 是否启用自定义key,zabbix监控mysql、tomcat等数据时需要自定义keyInclude=/etc/zabbix/zabbix_agentd.d/*.conf 启动客户端1# systemctl start zabbix-agent 开机自启动1# systemctl enable zabbix-agent 服务器添加被监控主机截至目前，已经安装好了zabbix server和zabbix agent，那接下来添加一台监控主机作为演示，帮助大家认识zabbix是如何监控服务器的。 登陆 http://zabbix_server_ip/zabbix, 点击 【Configuration】-&gt;【Hosts】-&gt;【Create host】 添加模板 模板添加完成后，回到Host页，然后点击页面下方的“add”按钮，即添加完成。 监控列表如下，你添加的所有主机监控，都将会在下方显示：绿色的“ZBX”字样表示已成功通过zabbix agent对其进行监控，当然还有其他方式进行监控，如SNMP方式等。 整个Dashboard监控页面显示如下显示有一个监控异常 查看图表点击【Monitoring】-&gt;【Graphs】-&gt;【Group】-&gt;【Host】-&gt; 【Graph】（可以选择你关心的查看） 到这里，我们完成了如何安装zabbix，以及部署、添加监控等信息。也有了详细的示例演示，希望能帮助你对zabbix有个初步的认识，后续文章，敬请持续关注。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix系列之《CentOS7快速安装Zabbix3-4》]]></title>
    <url>%2F2018%2F06%2F03%2FZabbix%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8ACentOS7%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85Zabbix3-4%E3%80%8B.html</url>
    <content type="text"><![CDATA[前言：Zabbix系列文章是实战操作为主的文章，这个系列文章设计从入门实战到深度使用，以及后面呈现一些高阶的玩法，带领运维同学搞定企业监控。 这篇文章适合刚入行的运维同学，在运维领域学习技术最好的方式就是：先快速将它搭建起来，然后根据每个组件或知识点横向深入学习，这也是最佳实践，本文就是带领运维童鞋们快速安装和掌握监控领域的利器，zabbix！ 系统环境准备在安装zabbix之前，你需要对系统做一个简单的初始化工作，这是zabbix能否正常运行的必备条件。 关闭selinux永久关闭123# vi /etc/selinux/configSELINUX=disabledSELINUXTYPE=targeted 临时关闭1setenforce 0 关闭防火墙永久关闭12systemctl stop firewalld.service #停止firewallsystemctl disable firewalld.service #禁止firewall开机启动 临时生效1iptables -F 配置系统时间同步设置每隔20分钟同步一次1*/20 * * * * ntpdate -u asia.pool.ntp.org &gt;/dev/null 2&gt;&amp;1 选择你需要的版本进行安装你打开zabbix官网会发现，zabbix安装页面提供了非常详细的条件供你选择，能适应主流Linux发行版的需求。 如题，我使用的是CentOS7.2版本，那么我选择的最佳实践就是通过yum安装，如果你没有那么高的定制需求，建议使用此方法。当然如果有特定需求或规范，你也可以选择源码编译安装。 如上，是我选择的版本信息。 安装和初始化安装zabbix1、CentOS7的yum源里默认不能安装zabbix，这使得zabbix自己提供了一个repo源，我们需要安装下：1# rpm -i http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-2.el7.noarch.rpm 2、安装zabbix-server以及必要的一些包1# yum install zabbix-server-mysql zabbix-web-mysql 安装数据库1、CentOS7默认安装MariaDB，值得说一下的是，MariaDB与MySQL在使用上没有区别，咱们正常使用即可。 快速安装数据库：123456789101112131415161718# yum -y install mariadb mariadb-server# systemctl start mariadb# systemctl enable mariadb# 初始化数据库# mysql_secure_installation &lt;== 会有很多提示，一路回车即可# 登录数据库[root@nicksors ~]# mysqlWelcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 412Server version: 5.5.56-MariaDB MariaDB ServerCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; 2、创建初始数据库12345# mysql &lt;==进入数据库，并执行下面几条SQL语句mysql&gt; create database zabbix character set utf8 collate utf8_bin; &lt;==创建一个数据库，名称为zabbixmysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by 'password'; &lt;==创建用户，并设置权限和密码mysql&gt; quit; 导入初始化数据库1# zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix 给zabbix-server做一些配置1、首先你得告诉zabbix_server，你的数据库密码是什么？编辑 /etc/zabbix/zabbix_server.conf文件，修改如下：1DBPassword=123123 （我设置的密码是123123，你自己设置的多少，填写到这里） 2、配置php的时间区域编辑/etc/httpd/conf.d/zabbix.conf文件，取消下面这一行的注释1# php_value date.timezone Asia/Shanghai &lt;==如果你是中国大陆用户，请设置时间区域为“亚洲/上海” 启动zabbix_server 和zabbix_agent进程1、启动12# systemctl restart zabbix-server httpd# systemctl enable zabbix-server httpd 2、启动后保持检查的好习惯：1234[root@nicksors ~]# netstat -lntup|egrep "zabbix|http"tcp 0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 23022/zabbix_servertcp6 0 0 :::80 :::* LISTEN 23205/httpdtcp6 0 0 :::10051 :::* LISTEN 23022/zabbix_server zabbix_server端口默认为10051 zabbix_agent端口默认为10050 3、访问地址为：http://server_ip_or_name/zabbix请将server_ip_or_name换成你的主机IP地址 Web页面配置zabbix点击Next step 这一步如果有“红叉”的，你需要满足，自行百度可以解决。点击Next step 默认会选择MySQL，port填写3306，Password填写你设置的密码即可。点击Next step 设置Zabbix Server信息，默认即可，Name写不写随你。点击Next step 你的配置总览。点击Next step 告诉你已经配置成功，配置文件在/etc/zabbix/web/zabbix.conf.php文件里，今后有变动需要更改配置，在这个文件更改就行。点击Finish跳转至登录页面 默认用户名密码：Admin/zabbix Zabbix Dashboard 到这里就完成了zabbix的安装部署啦，后续文章，敬请持续关注。]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
</search>
